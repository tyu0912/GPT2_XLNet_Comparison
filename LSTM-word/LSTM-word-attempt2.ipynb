{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Reference: https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation\n",
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Result embedding shape: (45, 100)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4578ed90a741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mword_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1. '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"thirteen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2. '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"thirteen\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-4578ed90a741>\u001b[0m in \u001b[0;36mword2idx\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mword_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0midx2word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mword_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'thirteen'"
     ],
     "ename": "KeyError",
     "evalue": "'thirteen'",
     "output_type": "error"
    }
   ],
   "source": [
    "# import string\n",
    "\n",
    "max_sentence_len = 40\n",
    "documents = [\"Human machine interface for lab abc computer applications . \",\n",
    "             \"A survey of user opinion of computer system response time , \",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey . maybe\"]\n",
    "sentences = [[word for word in doc.lower().split()[:max_sentence_len]] for doc in documents]\n",
    "sentences\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]\n",
    "\n",
    "print('1. ', word2idx(\"thirteen\"))\n",
    "print('2. ', word_model.wv.vocab[\"thirteen\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "total number of unique words:  6508\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# dir_split = f\"../1.DataPreparationResults/obama\"\n",
    "# file_train = open(f\"{dir_split}/train.txt\", 'r').read()\n",
    "# file_val = open(f\"{dir_split}/val.txt\", 'r').read()\n",
    "# file_test = open(f\"{dir_split}/test.txt\", 'r').read()\n",
    "\n",
    "# Windows ONLY\n",
    "dir_split = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\1.DataPreparationResults\\obama\"\n",
    "file_train = open(f\"{dir_split}\\\\train.txt\", 'r').read()\n",
    "file_val = open(f\"{dir_split}\\\\val.txt\", 'r').read()\n",
    "file_test = open(f\"{dir_split}\\\\test.txt\", 'r').read()\n",
    "\n",
    "# Add spaces around <speech_sep>\n",
    "# Create a set of all words in train.txt but remove <speech_sep>\n",
    "word_train = set(file_train.replace(\"<speech_sep>\", \" <speech_sep> \").split())\n",
    "word_train.remove(\"<speech_sep>\")\n",
    "\n",
    "print(\"total number of unique words: \",len(word_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_len = 30\n",
    "x_step = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\nResult embedding shape: (3000000, 300)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def file_to_sentences(file):\n",
    "    sentences = []\n",
    "    sentences2 = []\n",
    "    next_words = []\n",
    "    list_words = []\n",
    "    \n",
    "    for speech in file.split(\"<speech_sep>\"):\n",
    "        list_words = speech.split()\n",
    "        # I noticed the last speech has zero word \n",
    "        # because <speech_sep> is the last character\n",
    "        if len(list_words) == 0:\n",
    "            break\n",
    "        \n",
    "        # each row should have x_len + 1 (both input and target)\n",
    "        for i in range(0,len(list_words)-x_len-1, x_step):\n",
    "            sentences2 = [word for word in list_words[i: i + x_len + 1]]\n",
    "            sentences.append(sentences2)\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "train_sentences = file_to_sentences(file_train)\n",
    "\n",
    "# print('sentences rows = ', len(train_sentences))\n",
    "# print('sentences columns = ', len(train_sentences[0]))\n",
    "# print(train_sentences[0])\n",
    "\n",
    "# Train word2vec model\n",
    "# window: The maximum distance between the current and predicted word within a sentence\n",
    "# Reference: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "# word_model = gensim.models.Word2Vec(train_sentences, size=100, min_count=1, window=x_len+1, iter=5)\n",
    "# pretrained_weights = word_model.wv.vectors\n",
    "# print(type(pretrained_weights))\n",
    "\n",
    "# Google pre-trained word2vec model\n",
    "# Reference: https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "google_word_model = gensim.models.KeyedVectors.load_word2vec_format(r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "pretrained_weights = google_word_model.wv.vectors\n",
    "print(type(pretrained_weights))\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def word2idx(word):\n",
    "    return google_word_model.wv.vocab[word].index\n",
    "# def word2idx(word):\n",
    "#     if word in google_word_model.wv.vocab:\n",
    "#         return google_word_model.wv.vocab[word].index\n",
    "#     # Capitalization matters in Google trained word2vec\n",
    "#     # \"united\" is not equal to \"United\"\n",
    "#     elif type(word[0]) == str:\n",
    "#         word = word[0].lower() + word[1:]\n",
    "#         if word in google_word_model.wv.vocab:\n",
    "#             return google_word_model.wv.vocab[word].index\n",
    "#     else:\n",
    "#         return -1\n",
    "def idx2word(idx):\n",
    "    return google_word_model.wv.index2word[idx]\n",
    "\n",
    "vocab = google_word_model.wv.vocab\n",
    "# Confirm that word_model works\n",
    "# print(word_model.wv.vocab[\"doubts\"].index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "'IWO' in vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "{'1961', 'Tywanza', '77', '79', 'NASAnd', 'NASfrom', 'Iyou', '.', 'DePayne', '16', 'nothe', 'doughnut', '900', '22', '273', '2020', '10th', '2015', '59', '11', '50', '2005', 'Iwhen', '53', '23', '1099', '106', 'copouts', 'overspilled', 'NASour', '200', '750', '15', '1950s', '26', '48', '80', 'CASo', 'iuml', '000', '1990s', '60', ',', '44', '2000', 'REa', 'Clementa', 'to', '288', '75', '400', '46', '1929', 'SNAbecause', '25', '2004', '2001', '1997', '90', 'eacute', 'grey', '267', '1979', 'a', '21st', '2014', 'of135', '250', '2030s', '19', 'travelling', '18', 'and', '2009', 'NASbut', '15th', '10', '1960s', '47', 'of', '20th', '41', 'Ithe', '1999', 'NASWe', '100', '30', '1776', '99', '500', '2012', '78', '1948', 'G20', '40', '12', '52', '2007', 'UCThank', '160', '70s', 'travelled', '20', '47s', '2025', '19th', 'Ritterby', '600', '150', 'NASand', '2008', '189', '14', '98', '13', '2011', '55', '95'}\ntrain_X shape: (79982, 30)\ntrain_Y shape: (79982,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def sentences_to_2darray(sentences):\n",
    "    \n",
    "    missing_words = set()\n",
    "    \n",
    "    x = np.zeros([len(sentences), x_len], dtype=np.int32)\n",
    "    y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, word in enumerate(sentence[:-1]):\n",
    "            if word in vocab:\n",
    "                x[i, t] = word2idx(word)\n",
    "            # elif type(word[0]) == str:\n",
    "            #     word = word[0].lower() + word[1:]\n",
    "            #     if word in vocab:\n",
    "            #         x[i, t] = word2idx(word)\n",
    "            else:\n",
    "                x[i, t] = -1\n",
    "                missing_words.add(word)\n",
    "        if sentence[-1] in vocab:\n",
    "            y[i] = word2idx(sentence[-1])\n",
    "        else:\n",
    "            y[i] = -1\n",
    "    print(missing_words) \n",
    "        \n",
    "    return x, y\n",
    "\n",
    "train_X, train_Y = sentences_to_2darray(train_sentences)\n",
    "print('train_X shape:', train_X.shape)\n",
    "print('train_Y shape:', train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "{'102', '1965', '90', '2014', '31st', '61', 'salaamu', '30', '700', '17', '43', '50th', 'Vnothing', '1990s', 'CIto', 'spires', 'Cannot', 'MyRIt', 'iuml', '20th', '68', '93', '180', '1967', '19', '200', 'cuentapropistas', 'worshippers', '40', '300', '900', '1943', ',', '2007', 'ISIWe', '400', 'tranquillity', '2013', 'whyI', 'ISIover', 'sayyes', '140', '2015', '750', '15', '800', '1999', 'ISIeven', 'ISITake', 'Desiline', '150', '1796', '65th', '250', '000', '1994', '00', '60', '10', '2001', 'Cachita', '100', '401ks', 'AIDThat', 'G20', 'NASand', 'TPChina', '13', '18', 'futurode', '70', '1812', '26', 'inreconcilable', 'a', 'MyRA', 'IFewer', 'pretence', '20', 'IBstudents', 'oughtness', '2008', '77', 'UPand', 'Jobses', 'Nelba', '11', '154', '1983', '17th', 'DNAmerica', '.', '42', '33', '40s', 'to', '450', 'initiativeto', '95', '2016', '34', '1935', '90s', '14', '12', 'Cultivo', '1995', '500', 'inventa', 'PBand', '250th', 'and', '2010', '75', '2012', 'NOAand', '1970s', '80', '2011', 'BIn', '24', '1959', '50s', '21st', '46', 'Borlaugs', '99', '50', 'Estiven', 'Iwhen', 'of'}\n(83044, 30)\n(83044,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "val_X, val_Y = sentences_to_2darray(file_to_sentences(file_val))\n",
    "print(val_X.shape)\n",
    "print(val_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(emdedding_size, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(emdedding_size, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b52d2725f84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1280\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3000000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node embedding/embedding_lookup}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ],
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3000000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node embedding/embedding_lookup}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=2, batch_size=1280)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 80001 samples, validate on 83061 samples\n",
      "WARNING:tensorflow:From /home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 294s 4ms/sample - loss: 6.6451 - val_loss: 5.9541\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 285s 4ms/sample - loss: 6.4338 - val_loss: 5.8888\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['rocket', 'attacks', 'from', 'Gaza', ',', 'and', 'we', 'have', 'stood', 'up', 'for', 'Israel', 's', 'right', 'to', 'defend', 'itself', '.', 'And', 'that', 'is', 'why', 'Israel', 'has', 'a', 'right', 'to', 'expect', 'Hamas', 'to'] \"\n",
      "\n",
      "rocket attacks from Gaza , and we have stood up for Israel s right to defend itself . And that is why Israel has a right to expect Hamas to\n",
      " . . , that . . , , that . . . , , to , that the , . , . . . . . to . . , the . the , . , . the the , . . , . . . , . , .\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['rocket', 'attacks', 'from', 'Gaza', ',', 'and', 'we', 'have', 'stood', 'up', 'for', 'Israel', 's', 'right', 'to', 'defend', 'itself', '.', 'And', 'that', 'is', 'why', 'Israel', 'has', 'a', 'right', 'to', 'expect', 'Hamas', 'to'] \"\n",
      "\n",
      "rocket attacks from Gaza , and we have stood up for Israel s right to defend itself . And that is why Israel has a right to expect Hamas to\n",
      " the the . the that . . and s that the the . I that . , not and . to a . to it the those of , the our . you has . to , , , our We from America . of to of , of .\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" ['rocket', 'attacks', 'from', 'Gaza', ',', 'and', 'we', 'have', 'stood', 'up', 'for', 'Israel', 's', 'right', 'to', 'defend', 'itself', '.', 'And', 'that', 'is', 'why', 'Israel', 'has', 'a', 'right', 'to', 'expect', 'Hamas', 'to'] \"\n",
      "\n",
      "rocket attacks from Gaza , and we have stood up for Israel s right to defend itself . And that is why Israel has a right to expect Hamas to\n",
      " of legally it groups , have think the to in , force not in since politics office law the economic in a public s absolutely how for to for not even . and of . to is . graduate our have , will rights America her better not the came\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" ['rocket', 'attacks', 'from', 'Gaza', ',', 'and', 'we', 'have', 'stood', 'up', 'for', 'Israel', 's', 'right', 'to', 'defend', 'itself', '.', 'And', 'that', 'is', 'why', 'Israel', 'has', 'a', 'right', 'to', 'expect', 'Hamas', 'to'] \"\n",
      "\n",
      "rocket attacks from Gaza , and we have stood up for Israel s right to defend itself . And that is why Israel has a right to expect Hamas to\n",
      " from it One tax nation a and dreams dictator do More Right country doing the that to work roads training that forgotten administration , our . costs , them care practical be dynamic offer make . century our does The people of for where out trillions these this pride portion\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 80001 samples, validate on 83061 samples\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 6.3842 - val_loss: 5.8672\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 6.3252 - val_loss: 5.7456\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['I', 'think', 'that', 'on', 'energy', 'there', 'should', 'be', 'a', 'bipartisan', 'agreement', 'that', 'we', 'have', 'to', 'take', 'a', 'both', 'and', 'approach', 'rather', 'than', 'an', 'either', 'or', 'approach', '.', 'What', 'do', 'I'] \"\n",
      "\n",
      "I think that on energy there should be a bipartisan agreement that we have to take a both and approach rather than an either or approach . What do I\n",
      " the . of we the the . , , , the the , of . , . the . . . , , the the , . the the of . . to . our that , the the . . . . , . the a . of .\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['I', 'think', 'that', 'on', 'energy', 'there', 'should', 'be', 'a', 'bipartisan', 'agreement', 'that', 'we', 'have', 'to', 'take', 'a', 'both', 'and', 'approach', 'rather', 'than', 'an', 'either', 'or', 'approach', '.', 'What', 'do', 'I'] \"\n",
      "\n",
      "I think that on energy there should be a bipartisan agreement that we have to take a both and approach rather than an either or approach . What do I\n",
      " us and , , to the to of of a to . that , the the and that . the the is , of the , , , the take . know the , of to the to that of , , as I in . . we , We\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" ['I', 'think', 'that', 'on', 'energy', 'there', 'should', 'be', 'a', 'bipartisan', 'agreement', 'that', 'we', 'have', 'to', 'take', 'a', 'both', 'and', 'approach', 'rather', 'than', 'an', 'either', 'or', 'approach', '.', 'What', 'do', 'I'] \"\n",
      "\n",
      "I think that on energy there should be a bipartisan agreement that we have to take a both and approach rather than an either or approach . What do I\n",
      " folks . a Let President long is Golden needs her just with or From forward at Against again taking to immigration today to onto and 98 , to been the . , would change that Iran the Kansas is grace the 100 persuasive acts had willing to neither . it\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" ['I', 'think', 'that', 'on', 'energy', 'there', 'should', 'be', 'a', 'bipartisan', 'agreement', 'that', 'we', 'have', 'to', 'take', 'a', 'both', 'and', 'approach', 'rather', 'than', 'an', 'either', 'or', 'approach', '.', 'What', 'do', 'I'] \"\n",
      "\n",
      "I think that on energy there should be a bipartisan agreement that we have to take a both and approach rather than an either or approach . What do I\n",
      " Smart us resonated . poison say do the brothers what are discipline than in Forces our 6 , nations And a decide world are trends the discord s 20th let . crisis that After to term two and , was in Qaeda the chamber slows Detroit duty life 1 Bill\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 80001 samples, validate on 83061 samples\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 6.1820 - val_loss: 5.6788\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 6.0579 - val_loss: 5.5641\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['.', 'Imagine', 'imagine', 'for', 'a', 'moment', ',', 'here', 'was', 'a', 'young', 'girl', 'who', 'was', 'just', 'becoming', 'aware', 'of', 'our', 'democracy', 'just', 'beginning', 'to', 'understand', 'the', 'obligations', 'of', 'citizenship', 'just', 'starting'] \"\n",
      "\n",
      ". Imagine imagine for a moment , here was a young girl who was just becoming aware of our democracy just beginning to understand the obligations of citizenship just starting\n",
      " is the And . the a people . the the of , . the the that . And . the the a . the the the of . the the the the the the the the the the the the . . the the the the the the the our\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['.', 'Imagine', 'imagine', 'for', 'a', 'moment', ',', 'here', 'was', 'a', 'young', 'girl', 'who', 'was', 'just', 'becoming', 'aware', 'of', 'our', 'democracy', 'just', 'beginning', 'to', 'understand', 'the', 'obligations', 'of', 'citizenship', 'just', 'starting'] \"\n",
      "\n",
      ". Imagine imagine for a moment , here was a young girl who was just becoming aware of our democracy just beginning to understand the obligations of citizenship just starting\n",
      " , the And , the And that I a a the economy of the be of and do . the United want in the do and the God of the Americans , . the And . the The States . I . So for our the want . And to\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" ['.', 'Imagine', 'imagine', 'for', 'a', 'moment', ',', 'here', 'was', 'a', 'young', 'girl', 'who', 'was', 'just', 'becoming', 'aware', 'of', 'our', 'democracy', 'just', 'beginning', 'to', 'understand', 'the', 'obligations', 'of', 'citizenship', 'just', 'starting'] \"\n",
      "\n",
      ". Imagine imagine for a moment , here was a young girl who was just becoming aware of our democracy just beginning to understand the obligations of citizenship just starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that just but to But on escape are was class shut family all need and their chance . housing an earth for hospital I families all but at insurance that Air the use , she jobs and insurance have must argument at Yet we with a the destiny who plainly\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" ['.', 'Imagine', 'imagine', 'for', 'a', 'moment', ',', 'here', 'was', 'a', 'young', 'girl', 'who', 'was', 'just', 'becoming', 'aware', 'of', 'our', 'democracy', 'just', 'beginning', 'to', 'understand', 'the', 'obligations', 'of', 'citizenship', 'just', 'starting'] \"\n",
      "\n",
      ". Imagine imagine for a moment , here was a young girl who was just becoming aware of our democracy just beginning to understand the obligations of citizenship just starting\n",
      " to rebuild actually Scranton their steal while percent to freedom the himself to can stems got could between even makes now proposal cannot push the stop United this to feed flag is economy portion we that I loopholes crisis many made together to patriotism Tom us in things as talking\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 80001 samples, validate on 83061 samples\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 5.9289 - val_loss: 5.5228\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 286s 4ms/sample - loss: 5.7972 - val_loss: 5.4289\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['a', 'sense', 'that', 'if', 'a', 'white', 'male', 'teen', 'was', 'involved', 'in', 'the', 'same', 'kind', 'of', 'scenario', ',', 'that', ',', 'from', 'top', 'to', 'bottom', ',', 'both', 'the', 'outcome', 'and', 'the', 'aftermath'] \"\n",
      "\n",
      "a sense that if a white male teen was involved in the same kind of scenario , that , from top to bottom , both the outcome and the aftermath\n",
      " of the world , , we the a time of the United States . the the States . the , , we the be . the the States . the the time . the the people . the the people . the the a . the the . the the\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['a', 'sense', 'that', 'if', 'a', 'white', 'male', 'teen', 'was', 'involved', 'in', 'the', 'same', 'kind', 'of', 'scenario', ',', 'that', ',', 'from', 'top', 'to', 'bottom', ',', 'both', 'the', 'outcome', 'and', 'the', 'aftermath'] \"\n",
      "\n",
      "a sense that if a white male teen was involved in the same kind of scenario , that , from top to bottom , both the outcome and the aftermath\n",
      " that and the economy . We . We , . the as that that the The offering . We have should the sure , the top by the people , this are a church to as we , and our children , have the nation is our the care .\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" ['a', 'sense', 'that', 'if', 'a', 'white', 'male', 'teen', 'was', 'involved', 'in', 'the', 'same', 'kind', 'of', 'scenario', ',', 'that', ',', 'from', 'top', 'to', 'bottom', ',', 'both', 'the', 'outcome', 'and', 'the', 'aftermath'] \"\n",
      "\n",
      "a sense that if a white male teen was involved in the same kind of scenario , that , from top to bottom , both the outcome and the aftermath\n",
      " , . And this to is it but because had you when legally we Because . When was last landscape , . But they there is from sure that should want foreign other roll to continent the values of tomorrow to he know is never have the government of the\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" ['a', 'sense', 'that', 'if', 'a', 'white', 'male', 'teen', 'was', 'involved', 'in', 'the', 'same', 'kind', 'of', 'scenario', ',', 'that', ',', 'from', 'top', 'to', 'bottom', ',', 'both', 'the', 'outcome', 'and', 'the', 'aftermath'] \"\n",
      "\n",
      "a sense that if a white male teen was involved in the same kind of scenario , that , from top to bottom , both the outcome and the aftermath\n",
      " on . take should and that Bush . setbacks will tonight does and also a deficits of That means war effort once blood from rising came . And by not be quarter it to the anybody ahead to Now you are corporate them built I like going , be essential\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('GoTweights'):\n",
    "    model.load_weights('GoTweights')\n",
    "\n",
    "def sample(a, temperature=1.0):\n",
    "    # helper function to randomly select a word based on probability distribution\n",
    "#     a = np.log(a) / temperature\n",
    "#     a = np.exp(a) / np.sum(np.exp(a))\n",
    "#     return np.argmax(np.random.multinomial(1, a, 1))\n",
    "    \n",
    "    # Reference: https://github.com/llSourcell/How-to-Generate-Music-Demo/issues/4\n",
    "    a = np.log(a) / temperature \n",
    "    dist = np.exp(a)/np.sum(np.exp(a)) \n",
    "    choices = range(len(a)) \n",
    "    return np.random.choice(choices, p=dist)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 5):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(train_X, train_Y, batch_size=1280, epochs=2, validation_data=(val_X,val_Y))\n",
    "    model.save_weights('GoTweights',overwrite=True)\n",
    "\n",
    "    # Select a speech from the test file\n",
    "    # randint(a,b) selects from all integers between a and b (inclusive)\n",
    "    # The last speech has zero word, so instead of -1 use -2\n",
    "    gen_speech_index = random.randint(0, len(file_test.split(\"<speech_sep>\"))-2)\n",
    "    # A list of words in the speech\n",
    "    list_words = file_test.split(\"<speech_sep>\")[gen_speech_index].split()\n",
    "    # Select a starting point for the context\n",
    "    start_index = random.randint(0, len(list_words) - x_len - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = list_words[start_index: start_index + x_len]\n",
    "        generated += ' '.join(sentence)\n",
    "        print('----- Generating with seed: \"' , sentence , '\"')\n",
    "        print()\n",
    "        sys.stdout.write(generated)\n",
    "        print()\n",
    "\n",
    "        for i in range(50):\n",
    "            x = np.zeros((1, x_len, len(word_train)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                if word in word_train:\n",
    "                    x[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            # model.predict(x, verbose=0).shape = (1, x_len)\n",
    "            # model.predict(x, verbose=0)[0].shape = (x_len, )\n",
    "            preds = model.predict(x, verbose=0)[0]            \n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            generated += next_word\n",
    "            del sentence[0]\n",
    "            sentence.append(next_word)\n",
    "            sys.stdout.write(' ')\n",
    "            sys.stdout.write(next_word)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x, verbose=0)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}