{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_split = f\"../1.DataPreparationResults/obama\"\n",
    "\n",
    "file_train = open(f\"{dir_split}/train.txt\", 'r').read()\n",
    "file_val = open(f\"{dir_split}/val.txt\", 'r').read()\n",
    "file_test = open(f\"{dir_split}/test.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words:  10774\n"
     ]
    }
   ],
   "source": [
    "# This essentially is to produce a unique words list from all the text before splitting into sentences\n",
    "\n",
    "def google_preprocess(file):\n",
    "    file2 = re.sub('\\d', '#', file)\n",
    "    file2 = re.sub(' a ', ' A ', file2)\n",
    "    file2 = re.sub(' and ', ' And ', file2)\n",
    "    file2 = re.sub(' of ', ' Of ', file2)\n",
    "    file2 = re.sub(' to ', ' To ', file2)\n",
    "    # Add spaces around <speech_sep>\n",
    "    # Create a set of all words in file.txt but remove <speech_sep>\n",
    "    unique_words = set(file2.replace(\"<speech_sep>\", \" <speech_sep> \").split())\n",
    "    unique_words.remove(\"<speech_sep>\")\n",
    "    return file2, unique_words\n",
    "\n",
    "file_train_google, unique_words_train = google_preprocess(file_train)\n",
    "file_val_google, unique_words_val = google_preprocess(file_val)\n",
    "file_test_google, unique_words_test = google_preprocess(file_test)\n",
    "\n",
    "unique_words_all = unique_words_train.union(unique_words_val.union(unique_words_test))\n",
    "print(\"total number of unique words: \",len(unique_words_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = 30\n",
    "x_step = 1\n",
    "\n",
    "def file_to_rolling_sentences(file):\n",
    "    sentences = []\n",
    "    sentences2 = []\n",
    "    next_words = []\n",
    "    list_words = []\n",
    "    \n",
    "    for speech in file.split(\"<speech_sep>\"):\n",
    "        list_words = speech.split()\n",
    "        # I noticed the last speech has zero word \n",
    "        # because <speech_sep> is the last character\n",
    "        if len(list_words) == 0:\n",
    "            break\n",
    "        \n",
    "        # each row should have x_len + 1 (both input and target)\n",
    "        for i in range(0,len(list_words)-x_len-1, x_step):\n",
    "            sentences2 = [word for word in list_words[i: i + x_len + 1]]\n",
    "            sentences.append(sentences2)\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "# train_sentences = file_to_sentences(file_train)\n",
    "train_sentences = file_to_rolling_sentences(file_train_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_each_sentence(file):\n",
    "    \n",
    "    sentence_all = []\n",
    "    \n",
    "    for speech in file.split(\"<speech_sep>\"):\n",
    "        sentences = speech.split('.')\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_all.append(sentence.strip() + '.')\n",
    "    \n",
    "    return sentence_all\n",
    "\n",
    "train_sentences = file_to_each_sentence(file_train_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "number_words = 25\n",
    "maxlength = 50\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=number_words)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "tokenized_sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5940 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(tokenized_sequences, maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data[:, :-1]\n",
    "train_y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word_model = gensim.models.KeyedVectors.load_word2vec_format('../../test/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the vectors\n",
    "#pretrained_weights = google_word_model.wv.vectors\n",
    "\n",
    "#word2idx\n",
    "google_word_model.wv.vocab[\"Porn\"].index\n",
    "\n",
    "#idx2word\n",
    "#google_word_model.wv.index2word[idx]\n",
    "\n",
    "#get vector of word\n",
    "google_word_model[\"Porn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = google_word_model[word] if word in google_word_model else None\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlength-1, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(EMBEDDING_DIM))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(len(word_index), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 3620 samples\n",
      "Epoch 1/20\n",
      "3620/3620 [==============================] - 6s 2ms/sample - loss: 5.1757\n",
      "Epoch 2/20\n",
      "3620/3620 [==============================] - 3s 755us/sample - loss: 2.9708\n",
      "Epoch 3/20\n",
      "3620/3620 [==============================] - 3s 754us/sample - loss: 2.9862\n",
      "Epoch 4/20\n",
      "3620/3620 [==============================] - 3s 749us/sample - loss: 2.9746\n",
      "Epoch 5/20\n",
      "3620/3620 [==============================] - 3s 755us/sample - loss: 2.9679\n",
      "Epoch 6/20\n",
      "3620/3620 [==============================] - 3s 754us/sample - loss: 2.9675\n",
      "Epoch 7/20\n",
      "3620/3620 [==============================] - 3s 753us/sample - loss: 2.9666\n",
      "Epoch 8/20\n",
      "3620/3620 [==============================] - 3s 749us/sample - loss: 2.9659\n",
      "Epoch 9/20\n",
      "3620/3620 [==============================] - 3s 748us/sample - loss: 2.9577\n",
      "Epoch 10/20\n",
      "3620/3620 [==============================] - 3s 745us/sample - loss: 2.9546\n",
      "Epoch 11/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9427\n",
      "Epoch 12/20\n",
      "3620/3620 [==============================] - 3s 745us/sample - loss: 2.9455\n",
      "Epoch 13/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9483\n",
      "Epoch 14/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9338\n",
      "Epoch 15/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9379\n",
      "Epoch 16/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9343\n",
      "Epoch 17/20\n",
      "3620/3620 [==============================] - 3s 748us/sample - loss: 2.9196\n",
      "Epoch 18/20\n",
      "3620/3620 [==============================] - 3s 751us/sample - loss: 2.9196\n",
      "Epoch 19/20\n",
      "3620/3620 [==============================] - 3s 754us/sample - loss: 2.9172\n",
      "Epoch 20/20\n",
      "3620/3620 [==============================] - 3s 756us/sample - loss: 2.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fecc0099310>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=20, batch_size=128) #callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-lstm",
   "language": "python",
   "name": "w266-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
