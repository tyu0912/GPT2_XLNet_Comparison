{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pydot\n",
    "import os\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation, Dropout, CuDNNLSTM, Input, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "#from AttentionWeightedAverage import AttentionWeightedAverage\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = ['hoover', 'fdroosevelt', 'truman', 'hayes', 'adams', 'carter', 'madison', 'ford', 'cleveland', 'obama', 'harding', 'wilson', 'taylor', 'monroe', 'lincoln', '.ipynb_checkpoints', 'jefferson', 'vanburen', 'jackson', 'washington', 'polk', 'bush', 'gwbush', 'pierce', 'reagan', 'garfield', 'mckinley', 'coolidge', 'roosevelt', 'fillmore', 'johnson', 'harrison', 'taft', 'lbjohnson', 'jqadams', 'tyler', 'clinton', 'kennedy', 'eisenhower', 'nixon', 'arthur', 'grant', 'buchanan', 'bharrison']\n",
    "\n",
    "file_train = ''\n",
    "file_val = ''\n",
    "file_test = ''\n",
    "\n",
    "for root, dirs, files in os.walk(\"../1.DataPreparationResults\", topdown=False):\n",
    "\n",
    "    for name in files:\n",
    "        file = os.path.join(root, name)\n",
    "        \n",
    "        if root.split('/')[2] in pres:\n",
    "            text = open(file).read()\n",
    "            \n",
    "            if 'train' in file:\n",
    "                file_train += text\n",
    "            elif 'val' in file:\n",
    "                file_val += text\n",
    "            elif 'test' in file:\n",
    "                file_test += text \n",
    "\n",
    "# Write to the 2.GPT2 XLNET folder\n",
    "dir_split = \"../1.DataPreparationResults/combined_data\"\n",
    "open(f\"{dir_split}/train.txt\", 'w').write(file_train)\n",
    "open(f\"{dir_split}/val.txt\", 'w').write(file_val)\n",
    "open(f\"{dir_split}/test.txt\", 'w').write(file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This essentially is to produce a unique words list from all the text before splitting into sentences\n",
    "\n",
    "def google_preprocess(file):\n",
    "    file2 = re.sub('\\d', '#', file)\n",
    "    file2 = re.sub(' a ', ' A ', file2)\n",
    "    file2 = re.sub(' and ', ' And ', file2)\n",
    "    file2 = re.sub(' of ', ' Of ', file2)\n",
    "    file2 = re.sub(' to ', ' To ', file2)\n",
    "    # Add spaces around <speech_sep>\n",
    "    # Create a set of all words in file.txt but remove <speech_sep>\n",
    "    unique_words = set(file2.replace(\"<speech_sep>\", \" <speech_sep> \").split())\n",
    "    unique_words.remove(\"<speech_sep>\")\n",
    "    return file2, unique_words\n",
    "\n",
    "file_train_google, unique_words_train = google_preprocess(file_train)\n",
    "file_val_google, unique_words_val = google_preprocess(file_val)\n",
    "file_test_google, unique_words_test = google_preprocess(file_test)\n",
    "\n",
    "unique_words_all = unique_words_train.union(unique_words_val.union(unique_words_test))\n",
    "print(\"total number of unique words: \",len(unique_words_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = 30\n",
    "x_step = 1\n",
    "\n",
    "def file_to_rolling_sentences(file):\n",
    "    sentences = []\n",
    "    sentences2 = []\n",
    "    next_words = []\n",
    "    list_words = []\n",
    "    \n",
    "    for speech in file.split(\"<speech_sep>\"):\n",
    "        list_words = speech.split()\n",
    "        # I noticed the last speech has zero word \n",
    "        # because <speech_sep> is the last character\n",
    "        if len(list_words) == 0:\n",
    "            break\n",
    "        \n",
    "        # each row should have x_len + 1 (both input and target)\n",
    "        for i in range(0,len(list_words)-x_len-1, x_step):\n",
    "            sentences2 = [word for word in list_words[i: i + x_len + 1]]\n",
    "            sentences.append(sentences2)\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "# train_sentences = file_to_sentences(file_train)\n",
    "train_sentences = file_to_rolling_sentences(file_train_google)\n",
    "val_sentences = file_to_rolling_sentences(file_val_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_each_sentence(file):\n",
    "    \n",
    "    sentence_all = []\n",
    "    \n",
    "    for speech in file.split(\"<speech_sep>\"):\n",
    "        sentences = speech.split('.')\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_all.append(sentence.strip() + '.')\n",
    "    \n",
    "    return sentence_all\n",
    "\n",
    "train_sentences = file_to_each_sentence(file_train_google)\n",
    "val_sentences = file_to_each_sentence(file_val_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words = 25\n",
    "maxlength = 64\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=number_words)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "tokenized_sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = new_dict = dict([(value, key) for key, value in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(tokenized_sequences, maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:, :-1]\n",
    "train_y = train_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val = tokenizer.texts_to_sequences(val_sentences)\n",
    "val_data = pad_sequences(tokenized_val, maxlen=maxlength)\n",
    "val_x = val_data[:, :-1]\n",
    "val_y = val_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word_model = gensim.models.KeyedVectors.load_word2vec_format('../../test/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the vectors\n",
    "#pretrained_weights = google_word_model.wv.vectors\n",
    "\n",
    "#word2idx\n",
    "def word2idx(word):\n",
    "    return google_word_model.wv.vocab[word].index\n",
    "    \n",
    "\n",
    "#idx2word\n",
    "def idx2word(idx):\n",
    "    return google_word_model.wv.index2word[idx]\n",
    "\n",
    "#get vector of word\n",
    "#google_word_model[\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = google_word_model[word] if word in google_word_model else None\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "\n",
    "# inputs = Input(shape=(49,))\n",
    "\n",
    "# embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlength-1, trainable=False)(inputs)\n",
    "# lstm1 = LSTM(units = 300, return_sequences = True)(embedding_layer)\n",
    "# lstm2 = LSTM(units = 300, return_sequences = True)(lstm1)\n",
    "# predictions = Dense(len(word_index), activation='softmax')(lstm2)\n",
    "\n",
    "\n",
    "# #test = Concatenate(axis=1)([embedding_layer, lstm1, lstm2])\n",
    "# #custom_layer = AttentionWeightedAverage(name='attention')(test)\n",
    "\n",
    "# model = Model(inputs=inputs,outputs=predictions)\n",
    "\n",
    "# # model.add(embedding_layer)\n",
    "# # model.add(LSTM(units = 128, return_sequences = True))\n",
    "# # model.add(LSTM(units = 128, return_sequences = True))\n",
    "# # model.add(Dense(len(word_index), activation='softmax'))\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# #print(model.summary())\n",
    "# #plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlength-1, trainable=False)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(units = 300, return_sequences=True))\n",
    "model.add(LSTM(units = 300))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(len(word_index), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, epochs=10, batch_size = 64, validation_data=(val_x, val_y)) #, callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_next(text, num_generated=50):\n",
    "    tokenized_sequences = [word_index[word] for word in text.lower().split() if word in word_index.keys()]\n",
    "    #tokenized_sequences = [word2idx(word) for word in text.lower().split() if word in google_word_model]\n",
    "    \n",
    "    #maxlen = maxlength, was 33 before.\n",
    "    print(maxlength)\n",
    "    padded_sequences = pad_sequences([tokenized_sequences], maxlen=maxlength)[:, 1:]\n",
    "    \n",
    "    #print(padded_sequences)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for i in range(num_generated):\n",
    "        #print(padded_sequences.shape)\n",
    "        prediction = model.predict(x=padded_sequences)\n",
    "        idx = sample(prediction[-1], temperature=0.9)\n",
    "        \n",
    "        generated_tokens.append(idx)\n",
    "        \n",
    "        tokenized_sequences = np.append(tokenized_sequences, idx)\n",
    "        padded_sequences = pad_sequences([tokenized_sequences], maxlen=maxlength)[:, 1:]\n",
    "        \n",
    "        \n",
    "    return ' '.join([index_word[idx] if idx != 0 else \"</unk>\" for idx in generated_tokens])\n",
    "    \n",
    "    #return ' '.join([idx2word(idx) for idx in padded_sequences[0] if idx != 0])\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    #print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    \n",
    "    texts = [\"There are two ways to love you\"]\n",
    "    \n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s' % (text, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_next(\"Hello How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_history_sentence_style_google_w2v_embedtrain_F_LSTM2x.json', 'w') as f:\n",
    "    json.dump(str(history.history), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('train_history_sentence_style_google_w2v_embedtrain_F_LSTM2x.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tennisonyu/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 64, 300)           7125900   \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 64, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 23752)             7149352   \n",
      "=================================================================\n",
      "Total params: 15,717,652\n",
      "Trainable params: 8,591,752\n",
      "Non-trainable params: 7,125,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('train_history_sentence_style_google_w2v_embedtrain_F_LSTM2x.h5')\n",
    "\n",
    "# Check its architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_14_input to have shape (64,) but got array with shape (63,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-92f3a4351801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello How are you\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-b7de68b813ec>\u001b[0m in \u001b[0;36mgenerate_next\u001b[0;34m(text, num_generated)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(padded_sequences.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m--> 716\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m    717\u001b[0m     return predict_loop(\n\u001b[1;32m    718\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2471\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266-lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    570\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    573\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_14_input to have shape (64,) but got array with shape (63,)"
     ]
    }
   ],
   "source": [
    "generate_next(\"Hello How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"LSTM-test-file.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"LSTM-fixed-window-result.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(fh):\n",
    "    print(\"###############################\")\n",
    "    prediction = generate_next(line)\n",
    "    actual = fh[i+1]\n",
    "    \n",
    "    output.write(line + '|' + actual + '|' + prediction)\n",
    "    print(prediction)\n",
    "    print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-lstm",
   "language": "python",
   "name": "w266-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
