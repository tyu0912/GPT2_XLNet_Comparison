mkdir: cannot create directory â€˜/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_1_wd_-0.5â€™: File exists
To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
2019-11-08 08:49:16.842023: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-08 08:49:16.848996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-11-08 08:49:16.849337: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b02d916a0 executing computations on platform Host. Devices:
2019-11-08 08:49:16.849364: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
11/08/2019 08:49:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/08/2019 08:49:19 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/tennisonyu/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
11/08/2019 08:49:19 - INFO - transformers.configuration_utils -   Model config {
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

11/08/2019 08:49:19 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/tennisonyu/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
11/08/2019 08:49:19 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/tennisonyu/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/08/2019 08:49:19 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/tennisonyu/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
Traceback (most recent call last):
  File "run_lm_finetuning.py", line 545, in <module>
    main()
  File "run_lm_finetuning.py", line 480, in main
    model.to(args.device)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
KeyboardInterrupt
mkdir: cannot create directory â€˜/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_1_wd_0â€™: File exists
Traceback (most recent call last):
  File "run_lm_finetuning.py", line 45, in <module>
    from transformers import (WEIGHTS_NAME, AdamW, WarmupLinearSchedule,
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/__init__.py", line 20, in <module>
    from .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/file_utils.py", line 29, in <module>
    import tensorflow as tf
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow/__init__.py", line 98, in <module>
    from tensorflow_core import *
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/__init__.py", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 959, in _find_and_load_unlocked
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow/__init__.py", line 50, in __getattr__
    module = self._load()
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow/__init__.py", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/python/__init__.py", line 84, in <module>
    from tensorflow.python.feature_column import feature_column_lib as feature_column
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_lib.py", line 24, in <module>
    from tensorflow.python.feature_column.dense_features_v2 import *
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/python/feature_column/dense_features_v2.py", line 21, in <module>
    from tensorflow.python.feature_column import dense_features
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/python/feature_column/dense_features.py", line 23, in <module>
    from tensorflow.python.feature_column import feature_column_v2 as fc
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py", line 3333, in <module>
    ('categorical_column', 'shared_embedding_column_creator', 'combiner',
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/collections/__init__.py", line 397, in namedtuple
    exec(s, namespace)
  File "<string>", line 1, in <module>
KeyboardInterrupt
mkdir: cannot create directory â€˜/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_1_wd_0.5â€™: File exists
To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
2019-11-08 08:49:31.155915: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-08 08:49:31.162719: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-11-08 08:49:31.162987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559773fd0ae0 executing computations on platform Host. Devices:
2019-11-08 08:49:31.163023: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
11/08/2019 08:49:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/08/2019 08:49:33 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/tennisonyu/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
11/08/2019 08:49:33 - INFO - transformers.configuration_utils -   Model config {
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

11/08/2019 08:49:33 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/tennisonyu/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
11/08/2019 08:49:33 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/tennisonyu/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/08/2019 08:49:33 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/tennisonyu/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
Traceback (most recent call last):
  File "run_lm_finetuning.py", line 545, in <module>
    main()
  File "run_lm_finetuning.py", line 479, in main
    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/modeling_utils.py", line 342, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/modeling_gpt2.py", line 516, in __init__
    self.init_weights()
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/modeling_utils.py", line 167, in init_weights
    self.apply(self._init_weights)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 293, in apply
    module.apply(fn)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 293, in apply
    module.apply(fn)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 293, in apply
    module.apply(fn)
  [Previous line repeated 2 more times]
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/torch/nn/modules/module.py", line 294, in apply
    fn(self)
  File "/home/tennisonyu/miniconda3/envs/w266-hugging/lib/python3.7/site-packages/transformers/modeling_gpt2.py", line 260, in _init_weights
    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
KeyboardInterrupt
mkdir: cannot create directory â€˜/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5â€™: File exists
To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
2019-11-08 08:49:39.232619: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-08 08:49:39.239198: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-11-08 08:49:39.239654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5640c81d1990 executing computations on platform Host. Devices:
2019-11-08 08:49:39.239695: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
11/08/2019 08:49:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/08/2019 08:49:41 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/tennisonyu/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
11/08/2019 08:49:41 - INFO - transformers.configuration_utils -   Model config {
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

11/08/2019 08:49:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/tennisonyu/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
11/08/2019 08:49:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/tennisonyu/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/08/2019 08:49:41 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/tennisonyu/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
11/08/2019 08:49:49 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir='', config_name='', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=True, eval_data_file='/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/val.txt', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, learning_rate=0.0001, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=10.0, output_dir='/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', tokenizer_name='', train_data_file='/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/train.txt', warmup_steps=0, weight_decay=-0.5)
11/08/2019 08:49:49 - INFO - __main__ -   Loading features from cached file /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/cached_lm_1024_train.txt
11/08/2019 08:49:49 - INFO - __main__ -   ***** Running training *****
11/08/2019 08:49:49 - INFO - __main__ -     Num examples = 80
11/08/2019 08:49:49 - INFO - __main__ -     Num Epochs = 10
11/08/2019 08:49:49 - INFO - __main__ -     Instantaneous batch size per GPU = 2
11/08/2019 08:49:49 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4
11/08/2019 08:49:49 - INFO - __main__ -     Gradient Accumulation steps = 2
11/08/2019 08:49:49 - INFO - __main__ -     Total optimization steps = 200
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:54,  1.39s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:53,  1.40s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:51,  1.39s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:50,  1.40s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.40s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:47,  1.40s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:46,  1.40s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.40s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:43,  1.40s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.40s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.40s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:39,  1.40s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:18<00:37,  1.40s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:36,  1.40s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.39s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.40s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:32,  1.39s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:25<00:30,  1.39s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:29,  1.39s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.40s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:29<00:26,  1.39s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:25,  1.40s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:32<00:23,  1.39s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.39s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.38s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:36<00:19,  1.39s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:18,  1.39s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:39<00:16,  1.39s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.39s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:43<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.38s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:47<00:08,  1.39s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.39s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:50<00:05,  1.39s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.39s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.39s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:54<00:01,  1.39s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it]
Epoch:  10%|â–ˆ         | 1/10 [00:55<08:21, 55.68s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.37s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.38s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:51,  1.38s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.39s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:47,  1.39s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.39s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.39s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:18<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.38s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:28,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.39s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:29<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.38s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:31<00:23,  1.38s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.38s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.38s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:35<00:19,  1.38s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:17,  1.38s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:38<00:16,  1.38s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.38s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:42<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.39s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:47<00:08,  1.39s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:49<00:05,  1.38s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.38s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.38s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:53<00:01,  1.38s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.38s/it]
Epoch:  20%|â–ˆâ–ˆ        | 2/10 [01:51<07:24, 55.57s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:54,  1.39s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.39s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:51,  1.39s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:50,  1.39s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:47,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.39s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:34,  1.45s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:32,  1.43s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:25<00:31,  1.41s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:29,  1.40s/it][A11/08/2019 08:52:08 - INFO - __main__ -   Creating features from dataset file at /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   Token indices sequence length is longer than the specified maximum sequence length for this model (85126 > 1024). Running this sequence through the model will result in indexing errors
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.
11/08/2019 08:52:09 - INFO - __main__ -   Saving features into cached file /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/cached_lm_1024_val.txt
11/08/2019 08:52:09 - INFO - __main__ -   ***** Running evaluation  *****
11/08/2019 08:52:09 - INFO - __main__ -     Num examples = 83
11/08/2019 08:52:09 - INFO - __main__ -     Batch size = 2


Evaluating:   0%|          | 0/42 [00:00<?, ?it/s][A[A

Evaluating:   2%|â–         | 1/42 [00:00<00:18,  2.25it/s][A[A

Evaluating:   5%|â–         | 2/42 [00:00<00:18,  2.20it/s][A[A

Evaluating:   7%|â–‹         | 3/42 [00:01<00:17,  2.18it/s][A[A

Evaluating:  10%|â–‰         | 4/42 [00:01<00:17,  2.15it/s][A[A

Evaluating:  12%|â–ˆâ–        | 5/42 [00:02<00:17,  2.14it/s][A[A

Evaluating:  14%|â–ˆâ–        | 6/42 [00:02<00:16,  2.13it/s][A[A

Evaluating:  17%|â–ˆâ–‹        | 7/42 [00:03<00:16,  2.12it/s][A[A

Evaluating:  19%|â–ˆâ–‰        | 8/42 [00:03<00:16,  2.11it/s][A[A

Evaluating:  21%|â–ˆâ–ˆâ–       | 9/42 [00:04<00:15,  2.12it/s][A[A

Evaluating:  24%|â–ˆâ–ˆâ–       | 10/42 [00:04<00:15,  2.11it/s][A[A

Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 11/42 [00:05<00:14,  2.11it/s][A[A

Evaluating:  29%|â–ˆâ–ˆâ–Š       | 12/42 [00:05<00:14,  2.11it/s][A[A

Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 13/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17/42 [00:08<00:11,  2.11it/s][A[A

Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18/42 [00:08<00:11,  2.11it/s][A[A

Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19/42 [00:08<00:10,  2.11it/s][A[A

Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20/42 [00:09<00:10,  2.11it/s][A[A

Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/42 [00:09<00:09,  2.10it/s][A[A

Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/42 [00:10<00:09,  2.10it/s][A[A

Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23/42 [00:10<00:09,  2.10it/s][A[A

Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24/42 [00:11<00:08,  2.10it/s][A[A

Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25/42 [00:11<00:08,  2.11it/s][A[A

Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/42 [00:12<00:07,  2.11it/s][A[A

Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27/42 [00:12<00:07,  2.11it/s][A[A

Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 28/42 [00:13<00:06,  2.11it/s][A[A

Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 29/42 [00:13<00:06,  2.11it/s][A[A

Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 34/42 [00:16<00:03,  2.11it/s][A[A

Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 35/42 [00:16<00:03,  2.11it/s][A[A

Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/42 [00:17<00:02,  2.12it/s][A[A

Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 37/42 [00:17<00:02,  2.11it/s][A[A

Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 38/42 [00:17<00:01,  2.11it/s][A[A

Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 39/42 [00:18<00:01,  2.12it/s][A[A

Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 40/42 [00:18<00:00,  2.11it/s][A[A

Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 41/42 [00:19<00:00,  2.11it/s][A[A

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:19<00:00,  2.46it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:19<00:00,  2.14it/s]
11/08/2019 08:52:28 - INFO - __main__ -   ***** Eval results  *****
11/08/2019 08:52:28 - INFO - __main__ -     perplexity = tensor(20.0024)
11/08/2019 08:52:28 - INFO - transformers.configuration_utils -   Configuration saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-50/config.json
11/08/2019 08:52:29 - INFO - transformers.modeling_utils -   Model weights saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-50/pytorch_model.bin
11/08/2019 08:52:29 - INFO - __main__ -   Saving model checkpoint to /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-50

Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:48<02:33,  7.68s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:50<01:49,  5.78s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:51<01:20,  4.46s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:52<01:00,  3.53s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:54<00:46,  2.89s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:55<00:36,  2.44s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:57<00:29,  2.12s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:58<00:24,  1.90s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:59<00:20,  1.75s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [01:01<00:17,  1.63s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [01:02<00:15,  1.56s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [01:03<00:13,  1.51s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [01:05<00:11,  1.47s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [01:06<00:10,  1.44s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [01:08<00:08,  1.43s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [01:09<00:07,  1.41s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [01:10<00:05,  1.41s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [01:12<00:04,  1.39s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [01:13<00:02,  1.39s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [01:15<00:01,  1.39s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:16<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:16<00:00,  1.91s/it]
Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [03:07<07:12, 61.83s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.37s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.38s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:50,  1.38s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.39s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:29,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.39s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:29<00:26,  1.39s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.39s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:31<00:23,  1.39s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.39s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.38s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:35<00:19,  1.38s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:17,  1.38s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:38<00:16,  1.38s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.38s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:42<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.38s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:46<00:08,  1.38s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:49<00:05,  1.38s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.37s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.38s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:53<00:01,  1.37s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.38s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.38s/it]
Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [04:02<05:59, 59.85s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.37s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.37s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:50,  1.38s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:39,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.38s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:28,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.38s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:28<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.38s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:31<00:23,  1.38s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.38s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.38s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:35<00:19,  1.38s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:17,  1.38s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:38<00:16,  1.38s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.38s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:42<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.38s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:46<00:08,  1.38s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:49<00:05,  1.38s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.38s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.39s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:53<00:01,  1.38s/it][A11/08/2019 08:54:47 - INFO - __main__ -   Loading features from cached file /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/cached_lm_1024_val.txt
11/08/2019 08:54:47 - INFO - __main__ -   ***** Running evaluation  *****
11/08/2019 08:54:47 - INFO - __main__ -     Num examples = 83
11/08/2019 08:54:47 - INFO - __main__ -     Batch size = 2


Evaluating:   0%|          | 0/42 [00:00<?, ?it/s][A[A

Evaluating:   2%|â–         | 1/42 [00:00<00:20,  2.00it/s][A[A

Evaluating:   5%|â–         | 2/42 [00:00<00:19,  2.02it/s][A[A

Evaluating:   7%|â–‹         | 3/42 [00:01<00:19,  2.04it/s][A[A

Evaluating:  10%|â–‰         | 4/42 [00:01<00:18,  2.06it/s][A[A

Evaluating:  12%|â–ˆâ–        | 5/42 [00:02<00:17,  2.07it/s][A[A

Evaluating:  14%|â–ˆâ–        | 6/42 [00:02<00:17,  2.08it/s][A[A

Evaluating:  17%|â–ˆâ–‹        | 7/42 [00:03<00:16,  2.08it/s][A[A

Evaluating:  19%|â–ˆâ–‰        | 8/42 [00:03<00:16,  2.08it/s][A[A

Evaluating:  21%|â–ˆâ–ˆâ–       | 9/42 [00:04<00:15,  2.09it/s][A[A

Evaluating:  24%|â–ˆâ–ˆâ–       | 10/42 [00:04<00:15,  2.10it/s][A[A

Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 11/42 [00:05<00:14,  2.10it/s][A[A

Evaluating:  29%|â–ˆâ–ˆâ–Š       | 12/42 [00:05<00:14,  2.10it/s][A[A

Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 13/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17/42 [00:08<00:11,  2.11it/s][A[A

Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18/42 [00:08<00:11,  2.11it/s][A[A

Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19/42 [00:09<00:10,  2.10it/s][A[A

Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20/42 [00:09<00:10,  2.10it/s][A[A

Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/42 [00:10<00:09,  2.10it/s][A[A

Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/42 [00:10<00:09,  2.11it/s][A[A

Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23/42 [00:10<00:09,  2.10it/s][A[A

Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24/42 [00:11<00:08,  2.10it/s][A[A

Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25/42 [00:11<00:08,  2.11it/s][A[A

Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/42 [00:12<00:07,  2.11it/s][A[A

Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27/42 [00:12<00:07,  2.11it/s][A[A

Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 28/42 [00:13<00:06,  2.11it/s][A[A

Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 29/42 [00:13<00:06,  2.11it/s][A[A

Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 34/42 [00:16<00:03,  2.10it/s][A[A

Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 35/42 [00:16<00:03,  2.11it/s][A[A

Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/42 [00:17<00:02,  2.10it/s][A[A

Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 37/42 [00:17<00:02,  2.10it/s][A[A

Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 38/42 [00:18<00:01,  2.10it/s][A[A

Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 39/42 [00:18<00:01,  2.10it/s][A[A

Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 40/42 [00:19<00:00,  2.10it/s][A[A

Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 41/42 [00:19<00:00,  2.10it/s][A[A

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:19<00:00,  2.46it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:19<00:00,  2.13it/s]
11/08/2019 08:55:07 - INFO - __main__ -   ***** Eval results  *****
11/08/2019 08:55:07 - INFO - __main__ -     perplexity = tensor(20.8902)
11/08/2019 08:55:07 - INFO - transformers.configuration_utils -   Configuration saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-100/config.json
11/08/2019 08:55:07 - INFO - transformers.modeling_utils -   Model weights saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-100/pytorch_model.bin
11/08/2019 08:55:07 - INFO - __main__ -   Saving model checkpoint to /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-100

Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:15<00:00,  7.49s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:15<00:00,  1.89s/it]
Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [05:18<05:22, 64.58s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:51,  1.33s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:51,  1.35s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:50,  1.35s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.36s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:47,  1.37s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.37s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.37s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:10<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:39,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.38s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:28,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.38s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:28<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.38s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:33<00:23,  1.38s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:34<00:30,  1.88s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:36<00:25,  1.73s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:37<00:22,  1.62s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:38<00:20,  1.55s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:40<00:17,  1.50s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:41<00:16,  1.46s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:43<00:14,  1.44s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:44<00:12,  1.42s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:45<00:11,  1.41s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:47<00:09,  1.40s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:48<00:08,  1.39s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:49<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:51<00:05,  1.38s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:52<00:04,  1.38s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:54<00:02,  1.38s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:55<00:01,  1.38s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:56<00:00,  1.38s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:56<00:00,  1.42s/it]
Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [06:15<04:08, 62.25s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.37s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.38s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:50,  1.38s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:47,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.39s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.39s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.39s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.39s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:18<00:37,  1.39s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:36,  1.39s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.38s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:29,  1.39s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.38s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:29<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.39s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:31<00:23,  1.38s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.39s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.38s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:35<00:19,  1.39s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:18,  1.39s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:38<00:16,  1.39s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.39s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:42<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.38s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:47<00:08,  1.39s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:49<00:05,  1.39s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.39s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.39s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:54<00:01,  1.39s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it]
Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [07:10<03:00, 60.19s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.38s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:52,  1.39s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:51,  1.38s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:13<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:14<01:00,  1.96s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:15<00:53,  1.79s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:17<00:48,  1.66s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:18<00:44,  1.58s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:19<00:41,  1.52s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:21<00:38,  1.48s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:22<00:36,  1.45s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:24<00:34,  1.43s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:25<00:32,  1.42s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:26<00:30,  1.41s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:28<00:29,  1.40s/it][A11/08/2019 08:57:29 - INFO - __main__ -   Loading features from cached file /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/cached_lm_1024_val.txt
11/08/2019 08:57:29 - INFO - __main__ -   ***** Running evaluation  *****
11/08/2019 08:57:29 - INFO - __main__ -     Num examples = 83
11/08/2019 08:57:29 - INFO - __main__ -     Batch size = 2


Evaluating:   0%|          | 0/42 [00:00<?, ?it/s][A[A

Evaluating:   2%|â–         | 1/42 [00:00<00:20,  2.02it/s][A[A

Evaluating:   5%|â–         | 2/42 [00:00<00:19,  2.03it/s][A[A

Evaluating:   7%|â–‹         | 3/42 [00:01<00:18,  2.06it/s][A[A

Evaluating:  10%|â–‰         | 4/42 [00:01<00:18,  2.07it/s][A[A

Evaluating:  12%|â–ˆâ–        | 5/42 [00:02<00:17,  2.09it/s][A[A

Evaluating:  14%|â–ˆâ–        | 6/42 [00:02<00:17,  2.09it/s][A[A

Evaluating:  17%|â–ˆâ–‹        | 7/42 [00:03<00:16,  2.10it/s][A[A

Evaluating:  19%|â–ˆâ–‰        | 8/42 [00:03<00:16,  2.10it/s][A[A

Evaluating:  21%|â–ˆâ–ˆâ–       | 9/42 [00:04<00:15,  2.10it/s][A[A

Evaluating:  24%|â–ˆâ–ˆâ–       | 10/42 [00:04<00:15,  2.10it/s][A[A

Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 11/42 [00:05<00:14,  2.11it/s][A[A

Evaluating:  29%|â–ˆâ–ˆâ–Š       | 12/42 [00:05<00:14,  2.12it/s][A[A

Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 13/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/42 [00:06<00:13,  2.11it/s][A[A

Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16/42 [00:07<00:12,  2.11it/s][A[A

Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17/42 [00:08<00:11,  2.12it/s][A[A

Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18/42 [00:08<00:11,  2.12it/s][A[A

Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19/42 [00:09<00:10,  2.12it/s][A[A

Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20/42 [00:09<00:10,  2.12it/s][A[A

Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/42 [00:09<00:09,  2.12it/s][A[A

Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/42 [00:10<00:09,  2.13it/s][A[A

Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23/42 [00:10<00:08,  2.12it/s][A[A

Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24/42 [00:11<00:08,  2.12it/s][A[A

Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25/42 [00:11<00:08,  2.11it/s][A[A

Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/42 [00:12<00:07,  2.10it/s][A[A

Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27/42 [00:12<00:07,  2.09it/s][A[A

Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 28/42 [00:13<00:06,  2.10it/s][A[A

Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 29/42 [00:13<00:06,  2.11it/s][A[A

Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31/42 [00:14<00:05,  2.11it/s][A[A

Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33/42 [00:15<00:04,  2.11it/s][A[A

Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 34/42 [00:16<00:03,  2.11it/s][A[A

Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 35/42 [00:18<00:03,  2.11it/s][A[A

Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/42 [00:18<00:05,  1.07it/s][A[A

Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 37/42 [00:19<00:03,  1.25it/s][A[A

Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 38/42 [00:19<00:02,  1.43it/s][A[A

Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 39/42 [00:20<00:01,  1.58it/s][A[A

Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 40/42 [00:20<00:01,  1.71it/s][A[A

Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 41/42 [00:20<00:00,  1.81it/s][A[A

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:21<00:00,  2.17it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:21<00:00,  1.98it/s]
11/08/2019 08:57:50 - INFO - __main__ -   ***** Eval results  *****
11/08/2019 08:57:50 - INFO - __main__ -     perplexity = tensor(22.2288)
11/08/2019 08:57:50 - INFO - transformers.configuration_utils -   Configuration saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-150/config.json
11/08/2019 08:57:51 - INFO - transformers.modeling_utils -   Model weights saved in /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-150/pytorch_model.bin
11/08/2019 08:57:51 - INFO - __main__ -   Saving model checkpoint to /home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/lr_1e-4_gas_2_wd_-0.5/checkpoint-150

Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:51<02:39,  7.95s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:52<01:53,  5.96s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:54<01:22,  4.59s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:55<01:01,  3.62s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:56<00:47,  2.95s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:58<00:37,  2.48s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:59<00:30,  2.15s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [01:01<00:24,  1.92s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [01:02<00:21,  1.76s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [01:03<00:18,  1.65s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [01:05<00:15,  1.57s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [01:06<00:13,  1.51s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [01:07<00:11,  1.48s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [01:09<00:10,  1.45s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [01:10<00:08,  1.43s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [01:12<00:07,  1.41s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [01:13<00:05,  1.41s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [01:14<00:04,  1.40s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [01:16<00:02,  1.40s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [01:17<00:01,  1.39s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:20<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:20<00:00,  2.02s/it]
Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [08:31<02:12, 66.35s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:51,  1.32s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:50,  1.34s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:49,  1.35s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:48,  1.36s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:47,  1.36s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.37s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.37s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:10<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:39,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.38s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.39s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.39s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:29,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.39s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:28<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.39s/it][A
Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:31<00:23,  1.39s/it][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:33<00:22,  1.39s/it][A
Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:34<00:20,  1.39s/it][A
Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:35<00:19,  1.39s/it][A
Iteration:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:37<00:17,  1.38s/it][A
Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:38<00:16,  1.39s/it][A
Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:40<00:15,  1.38s/it][A
Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:41<00:13,  1.38s/it][A
Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:42<00:12,  1.38s/it][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:44<00:11,  1.38s/it][A
Iteration:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:45<00:09,  1.38s/it][A
Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:46<00:08,  1.38s/it][A
Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:48<00:06,  1.38s/it][A
Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:49<00:05,  1.38s/it][A
Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:51<00:04,  1.38s/it][A
Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:52<00:02,  1.38s/it][A
Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:53<00:01,  1.38s/it][A
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.39s/it][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:55<00:00,  1.38s/it]
Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [09:26<01:03, 63.02s/it]
Iteration:   0%|          | 0/40 [00:00<?, ?it/s][A
Iteration:   2%|â–Ž         | 1/40 [00:01<00:53,  1.36s/it][A
Iteration:   5%|â–Œ         | 2/40 [00:02<00:51,  1.37s/it][A
Iteration:   8%|â–Š         | 3/40 [00:04<00:50,  1.37s/it][A
Iteration:  10%|â–ˆ         | 4/40 [00:05<00:49,  1.38s/it][A
Iteration:  12%|â–ˆâ–Ž        | 5/40 [00:06<00:48,  1.38s/it][A
Iteration:  15%|â–ˆâ–Œ        | 6/40 [00:08<00:46,  1.38s/it][A
Iteration:  18%|â–ˆâ–Š        | 7/40 [00:09<00:45,  1.38s/it][A
Iteration:  20%|â–ˆâ–ˆ        | 8/40 [00:11<00:44,  1.38s/it][A
Iteration:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:42,  1.38s/it][A
Iteration:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:13<00:41,  1.38s/it][A
Iteration:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:15<00:40,  1.38s/it][A
Iteration:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:16<00:38,  1.39s/it][A
Iteration:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:17<00:37,  1.38s/it][A
Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:19<00:35,  1.38s/it][A
Iteration:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:20<00:34,  1.38s/it][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:22<00:33,  1.38s/it][A
Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:23<00:31,  1.38s/it][A
Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:24<00:30,  1.39s/it][A
Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:26<00:28,  1.38s/it][A
Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:27<00:27,  1.38s/it][A
Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:29<00:26,  1.38s/it][A
Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:30<00:24,  1.38s/it][A