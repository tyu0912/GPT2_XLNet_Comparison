GPT2 Train Commands:
python run_lm_finetuning.py --num_train_epochs 10 --per_gpu_train_batch_size 2 --per_gpu_eval_batch_size 2 --overwrite_output_dir --train_data_file "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/train.txt" --output_dir "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama" --eval_data_file "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/val.txt" --model_type gpt2 --model_name_or_path gpt2 --evaluate_during_training --do_train --do_eval --eval_all_checkpoints


python run_lm_finetuning.py \
	--num_train_epochs 10 \
	--per_gpu_train_batch_size 2 \
	--per_gpu_eval_batch_size 2 \
	--overwrite_output_dir \
	--train_data_file "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/train.txt" \
	--output_dir "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama" \
	--eval_data_file "/home/tennisonyu/w266_project/2.ModelingGPT2andXLNet/processed_data/speech_level_train/obama/val.txt" \
	--model_type gpt2 \
	--model_name_or_path gpt2 \
	--evaluate_during_training \
	--do_train \
	--do_eval \
	--eval_all_checkpoints \
	--learning_rate 5e-5 \
	--gradient_accumulation_steps 1 \
	--weight_decay 0

python generate_text.py --model_type gpt2 --model_name_or_path testing_data_run/obama/model/lr_1e-4_gas_1_wd_0.75/ --text_file testing_data_run/obama/test.txt --stop_token <eod> --length 75 --top_k 3
