{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "# from keras.utils import np_utils\n",
    "# from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Angela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lling086/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-9405d16d83c1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9405d16d83c1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import tensorflow-gpu\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"CorpusOfPresidentialSpeeches/obama/obama_speeches_000.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Create input data to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Questions**_:\n",
    "* Why convert all to lower case and removing special characters?  \n",
    "* Why remove stop words?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    # Remove the title and date (the first two row)\n",
    "    startChar = [word.end() for word in re.finditer(\"\\n\",file)][1]\n",
    "    input2 = input[startChar:]\n",
    "    \n",
    "    # lowercase everything to standardize it\n",
    "    input2 = input2.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input2)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyone still doubts america place things possible still wonders dream founders alive time still questions power democracy tonight answer answer told lines stretched around schools churches numbers nation never seen people waited three hours four hours many first time lives believed time must different voice could difference answer spoken young old rich poor democrat republican black white latino asian native american gay straight disabled disabled americans sent message world never collection red states blue states always united states america answer led told long many cynical fearful doubtful achieve put hands arc history bend toward hope better day long time coming tonight day election defining moment change come america received gracious call senator mccain fought long hard campaign fought even longer harder country loves endured sacrifices america us cannot begin imagine better service rendered brave selfless leader congratulate governor palin achieved look forward working renew nation promise months ahead want thank partner journey man campaigned heart spoke men women grew streets scranton rode train home delaware vice president elect united states joe biden would standing tonight without unyielding support best friend last sixteen years rock family love life nation next first lady michelle obama sasha malia love much earned new puppy coming us white house longer us know grandmother watching along family made miss tonight know debt beyond measure campaign manager david plouffe chief strategist david axelrod best campaign team ever assembled history politics made happen forever grateful sacrificed get done never forget victory truly belongs belongs never likeliest candidate office start much money many endorsements campaign hatched halls washington began backyards des moines living rooms concord front porches charleston built working men women dug little savings give five dollars ten dollars twenty dollars cause grew strength young people rejected myth generation apathy left homes families jobs offered little pay less sleep young people braved bitter cold scorching heat knock doors perfect strangers millions americans volunteered organized proved two centuries later government people people people perished earth victory know win election know understand enormity task lies ahead even celebrate tonight know challenges tomorrow bring greatest lifetime two wars planet peril worst financial crisis century even stand tonight know brave americans waking deserts iraq mountains afghanistan risk lives us mothers fathers lie awake children fall asleep wonder make mortgage pay doctor bills save enough college new energy harness new jobs created new schools build threats meet alliances repair road ahead long climb steep may get one year even one term america never hopeful tonight get promise people get setbacks false starts many agree every decision policy make president know government solve every problem always honest challenges face listen especially disagree ask join work remaking nation way done america two hundred twenty one years block block brick brick calloused hand calloused hand began twenty one months ago depths winter must end autumn night victory alone change seek chance us make change cannot happen go back way things cannot happen without let us summon new spirit patriotism service responsibility us resolves pitch work harder look let us remember financial crisis taught us anything cannot thriving wall street main street suffers country rise fall one nation one people let us resist temptation fall back partisanship pettiness immaturity poisoned politics long let us remember man state first carried banner republican party white house party founded values self reliance individual liberty national unity values share democratic party great victory tonight measure humility determination heal divides held back progress lincoln said nation far divided enemies friends though passion may strained must break bonds affection americans whose support yet earn may vote hear voices need help president watching tonight beyond shores parliaments palaces huddled around radios forgotten corners world stories singular destiny shared new dawn american leadership hand would tear world defeat seek peace security support wondered america beacon still burns bright tonight proved true strength nation comes might arms scale wealth enduring power ideals democracy liberty opportunity unyielding hope true genius america america change union perfected already achieved gives us hope must achieve tomorrow election many firsts many stories told generations one mind tonight woman cast ballot atlanta lot like millions others stood line make voice heard election except one thing ann nixon cooper 106 years old born generation past slavery time cars road planes sky someone like vote two reasons woman color skin tonight think seen throughout century america heartache hope struggle progress times told people pressed american creed yes time women voices silenced hopes dismissed lived see stand speak reach ballot yes despair dust bowl depression across land saw nation conquer fear new deal new jobs new sense common purpose yes bombs fell harbor tyranny threatened world witness generation rise greatness democracy saved yes buses montgomery hoses birmingham bridge selma preacher atlanta told people shall overcome yes man touched moon wall came berlin world connected science imagination year election touched finger screen cast vote 106 years america best times darkest hours knows america change yes america come far seen much much tonight let us ask children live see next century daughters lucky live long ann nixon cooper change see progress made chance answer call moment time put people back work open doors opportunity kids restore prosperity promote cause peace reclaim american dream reaffirm fundamental truth many one breathe hope met cynicism doubt tell us respond timeless creed sums spirit people yes thank god bless may god bless united states america'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**_:\n",
    "* Shall we add all other numbers to the dictionary below?  \n",
    "\n",
    "_**Note**_:\n",
    "* Need to find paper to justify choosing character-level generation over word-level generation. OR we can try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '0': 1,\n",
       " '1': 2,\n",
       " '6': 3,\n",
       " 'a': 4,\n",
       " 'b': 5,\n",
       " 'c': 6,\n",
       " 'd': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'h': 11,\n",
       " 'i': 12,\n",
       " 'j': 13,\n",
       " 'k': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'q': 20,\n",
       " 'r': 21,\n",
       " 's': 22,\n",
       " 't': 23,\n",
       " 'u': 24,\n",
       " 'v': 25,\n",
       " 'w': 26,\n",
       " 'x': 27,\n",
       " 'y': 28,\n",
       " 'z': 29}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the characters in our input to number\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 6029\n",
      "Total vocab: 30\n"
     ]
    }
   ],
   "source": [
    "# We need the total length of our inputs and total length of our set of characters \n",
    "# for later data prep, so we'll store these in a variable.\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the data\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go through the entire list of inputs and convert the characters to numbers\n",
    "\n",
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll doubts america place things possible still wonders dream founders alive time still questions powe\n",
      "-----\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "# Check my understanding\n",
    "i = 10\n",
    "in_seq = processed_inputs[i:i + seq_length]\n",
    "out_seq = processed_inputs[i + seq_length]\n",
    "print(in_seq)\n",
    "print('-----')\n",
    "print(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 5929\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**_:\n",
    "* I don't understand the logic of converting `X` to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert our input sequences into a processed numpy array that our network can use\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "# convert the numpy array values into floats so that the sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode our label data\n",
    "y = keras.utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56666667],\n",
       "       [0.93333333],\n",
       "       [0.6       ],\n",
       "       [0.56666667],\n",
       "       [0.26666667],\n",
       "       [0.        ],\n",
       "       [0.73333333],\n",
       "       [0.76666667],\n",
       "       [0.4       ],\n",
       "       [0.5       ],\n",
       "       [0.5       ],\n",
       "       [0.        ],\n",
       "       [0.23333333],\n",
       "       [0.6       ],\n",
       "       [0.8       ],\n",
       "       [0.16666667],\n",
       "       [0.76666667],\n",
       "       [0.73333333],\n",
       "       [0.        ],\n",
       "       [0.13333333],\n",
       "       [0.53333333],\n",
       "       [0.26666667],\n",
       "       [0.7       ],\n",
       "       [0.4       ],\n",
       "       [0.2       ],\n",
       "       [0.13333333],\n",
       "       [0.        ],\n",
       "       [0.63333333],\n",
       "       [0.5       ],\n",
       "       [0.13333333],\n",
       "       [0.2       ],\n",
       "       [0.26666667],\n",
       "       [0.        ],\n",
       "       [0.76666667],\n",
       "       [0.36666667],\n",
       "       [0.4       ],\n",
       "       [0.56666667],\n",
       "       [0.33333333],\n",
       "       [0.73333333],\n",
       "       [0.        ],\n",
       "       [0.63333333],\n",
       "       [0.6       ],\n",
       "       [0.73333333],\n",
       "       [0.73333333],\n",
       "       [0.4       ],\n",
       "       [0.16666667],\n",
       "       [0.5       ],\n",
       "       [0.26666667],\n",
       "       [0.        ],\n",
       "       [0.73333333],\n",
       "       [0.76666667],\n",
       "       [0.4       ],\n",
       "       [0.5       ],\n",
       "       [0.5       ],\n",
       "       [0.        ],\n",
       "       [0.86666667],\n",
       "       [0.6       ],\n",
       "       [0.56666667],\n",
       "       [0.23333333],\n",
       "       [0.26666667],\n",
       "       [0.7       ],\n",
       "       [0.73333333],\n",
       "       [0.        ],\n",
       "       [0.23333333],\n",
       "       [0.7       ],\n",
       "       [0.26666667],\n",
       "       [0.13333333],\n",
       "       [0.53333333],\n",
       "       [0.        ],\n",
       "       [0.3       ],\n",
       "       [0.6       ],\n",
       "       [0.8       ],\n",
       "       [0.56666667],\n",
       "       [0.23333333],\n",
       "       [0.26666667],\n",
       "       [0.7       ],\n",
       "       [0.73333333],\n",
       "       [0.        ],\n",
       "       [0.13333333],\n",
       "       [0.5       ],\n",
       "       [0.4       ],\n",
       "       [0.83333333],\n",
       "       [0.26666667],\n",
       "       [0.        ],\n",
       "       [0.76666667],\n",
       "       [0.4       ],\n",
       "       [0.53333333],\n",
       "       [0.26666667],\n",
       "       [0.        ],\n",
       "       [0.73333333],\n",
       "       [0.76666667],\n",
       "       [0.4       ],\n",
       "       [0.5       ],\n",
       "       [0.5       ],\n",
       "       [0.        ],\n",
       "       [0.66666667],\n",
       "       [0.8       ],\n",
       "       [0.26666667],\n",
       "       [0.73333333],\n",
       "       [0.76666667]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default learning rate for adam optimizer is 0.001.  \n",
    "(Reference: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)  \n",
    "To change the learning rate, see https://keras.io/optimizers/)  \n",
    "\n",
    "_**Note**_: maybe research on the optimizer to use??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference on `keras.callbacks.ModelCheckpoint`:\n",
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"model_weights_LSTM_character_Angela_attemp1.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5929/5929 [==============================] - 68s 11ms/step - loss: 3.0678\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.06778, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/40\n",
      "5929/5929 [==============================] - 96s 16ms/step - loss: 2.9381\n",
      "\n",
      "Epoch 00002: loss improved from 3.06778 to 2.93814, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/40\n",
      "5929/5929 [==============================] - 112s 19ms/step - loss: 2.9362\n",
      "\n",
      "Epoch 00003: loss improved from 2.93814 to 2.93622, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/40\n",
      "5929/5929 [==============================] - 121s 20ms/step - loss: 2.9209\n",
      "\n",
      "Epoch 00004: loss improved from 2.93622 to 2.92090, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/40\n",
      "5929/5929 [==============================] - 131s 22ms/step - loss: 2.9158\n",
      "\n",
      "Epoch 00005: loss improved from 2.92090 to 2.91584, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/40\n",
      "5929/5929 [==============================] - 134s 23ms/step - loss: 2.9190\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.91584\n",
      "Epoch 7/40\n",
      "5929/5929 [==============================] - 139s 23ms/step - loss: 2.9187\n",
      "\n",
      "Epoch 00007: loss did not improve from 2.91584\n",
      "Epoch 8/40\n",
      "5929/5929 [==============================] - 140s 24ms/step - loss: 2.9120\n",
      "\n",
      "Epoch 00008: loss improved from 2.91584 to 2.91199, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/40\n",
      "5929/5929 [==============================] - 145s 24ms/step - loss: 2.9161\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.91199\n",
      "Epoch 10/40\n",
      "5929/5929 [==============================] - 146s 25ms/step - loss: 2.9110\n",
      "\n",
      "Epoch 00010: loss improved from 2.91199 to 2.91104, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 2.9128\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.91104\n",
      "Epoch 12/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.9100\n",
      "\n",
      "Epoch 00012: loss improved from 2.91104 to 2.91004, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 2.9097\n",
      "\n",
      "Epoch 00013: loss improved from 2.91004 to 2.90974, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 2.9113\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.90974\n",
      "Epoch 15/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.9069\n",
      "\n",
      "Epoch 00015: loss improved from 2.90974 to 2.90691, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.8986\n",
      "\n",
      "Epoch 00016: loss improved from 2.90691 to 2.89856, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/40\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.8880\n",
      "\n",
      "Epoch 00017: loss improved from 2.89856 to 2.88796, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/40\n",
      "5929/5929 [==============================] - 155s 26ms/step - loss: 2.8677\n",
      "\n",
      "Epoch 00018: loss improved from 2.88796 to 2.86770, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.8444\n",
      "\n",
      "Epoch 00019: loss improved from 2.86770 to 2.84443, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.8195\n",
      "\n",
      "Epoch 00020: loss improved from 2.84443 to 2.81954, saving model to model_weights_saved.hdf5\n",
      "Epoch 21/40\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 2.8036\n",
      "\n",
      "Epoch 00021: loss improved from 2.81954 to 2.80363, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.7727\n",
      "\n",
      "Epoch 00022: loss improved from 2.80363 to 2.77268, saving model to model_weights_saved.hdf5\n",
      "Epoch 23/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 2.7364\n",
      "\n",
      "Epoch 00023: loss improved from 2.77268 to 2.73641, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/40\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.7075\n",
      "\n",
      "Epoch 00024: loss improved from 2.73641 to 2.70752, saving model to model_weights_saved.hdf5\n",
      "Epoch 25/40\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 2.6888\n",
      "\n",
      "Epoch 00025: loss improved from 2.70752 to 2.68877, saving model to model_weights_saved.hdf5\n",
      "Epoch 26/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.6706\n",
      "\n",
      "Epoch 00026: loss improved from 2.68877 to 2.67064, saving model to model_weights_saved.hdf5\n",
      "Epoch 27/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.6616\n",
      "\n",
      "Epoch 00027: loss improved from 2.67064 to 2.66156, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 2.6493\n",
      "\n",
      "Epoch 00028: loss improved from 2.66156 to 2.64927, saving model to model_weights_saved.hdf5\n",
      "Epoch 29/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 2.6355\n",
      "\n",
      "Epoch 00029: loss improved from 2.64927 to 2.63554, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.6321\n",
      "\n",
      "Epoch 00030: loss improved from 2.63554 to 2.63211, saving model to model_weights_saved.hdf5\n",
      "Epoch 31/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.6197\n",
      "\n",
      "Epoch 00031: loss improved from 2.63211 to 2.61966, saving model to model_weights_saved.hdf5\n",
      "Epoch 32/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.6071\n",
      "\n",
      "Epoch 00032: loss improved from 2.61966 to 2.60711, saving model to model_weights_saved.hdf5\n",
      "Epoch 33/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.5882\n",
      "\n",
      "Epoch 00033: loss improved from 2.60711 to 2.58820, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.5806\n",
      "\n",
      "Epoch 00034: loss improved from 2.58820 to 2.58057, saving model to model_weights_saved.hdf5\n",
      "Epoch 35/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.5630\n",
      "\n",
      "Epoch 00035: loss improved from 2.58057 to 2.56301, saving model to model_weights_saved.hdf5\n",
      "Epoch 36/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.5553\n",
      "\n",
      "Epoch 00036: loss improved from 2.56301 to 2.55528, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.5239\n",
      "\n",
      "Epoch 00037: loss improved from 2.55528 to 2.52388, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.5025\n",
      "\n",
      "Epoch 00038: loss improved from 2.52388 to 2.50252, saving model to model_weights_saved.hdf5\n",
      "Epoch 39/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.4835\n",
      "\n",
      "Epoch 00039: loss improved from 2.50252 to 2.48346, saving model to model_weights_saved.hdf5\n",
      "Epoch 40/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.4553\n",
      "\n",
      "Epoch 00040: loss improved from 2.48346 to 2.45532, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f56d00048>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the same session, I can continue to train using another `fit`. If the session was restarted or interrupted, see  \n",
    "* https://stackoverflow.com/questions/45393429/keras-how-to-save-model-and-continue-training  \n",
    "* https://www.mikulskibartosz.name/save-and-restore-a-tensorflow-model-using-keras-for-continuous-model-training/  \n",
    "(Haven't implemented it yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.4368\n",
      "\n",
      "Epoch 00001: loss improved from 2.45532 to 2.43682, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.4065\n",
      "\n",
      "Epoch 00002: loss improved from 2.43682 to 2.40654, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.3761\n",
      "\n",
      "Epoch 00003: loss improved from 2.40654 to 2.37607, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.3542\n",
      "\n",
      "Epoch 00004: loss improved from 2.37607 to 2.35417, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/20\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 2.3160\n",
      "\n",
      "Epoch 00005: loss improved from 2.35417 to 2.31597, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.2786\n",
      "\n",
      "Epoch 00006: loss improved from 2.31597 to 2.27857, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.2432\n",
      "\n",
      "Epoch 00007: loss improved from 2.27857 to 2.24322, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "5929/5929 [==============================] - 155s 26ms/step - loss: 2.2074\n",
      "\n",
      "Epoch 00008: loss improved from 2.24322 to 2.20737, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 2.1708\n",
      "\n",
      "Epoch 00009: loss improved from 2.20737 to 2.17077, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 2.1280\n",
      "\n",
      "Epoch 00010: loss improved from 2.17077 to 2.12795, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 2.1005\n",
      "\n",
      "Epoch 00011: loss improved from 2.12795 to 2.10054, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.0497\n",
      "\n",
      "Epoch 00012: loss improved from 2.10054 to 2.04972, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 2.0118\n",
      "\n",
      "Epoch 00013: loss improved from 2.04972 to 2.01179, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 1.9692\n",
      "\n",
      "Epoch 00014: loss improved from 2.01179 to 1.96918, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.9003\n",
      "\n",
      "Epoch 00015: loss improved from 1.96918 to 1.90035, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 1.8625\n",
      "\n",
      "Epoch 00016: loss improved from 1.90035 to 1.86251, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/20\n",
      "5929/5929 [==============================] - 146s 25ms/step - loss: 1.8237\n",
      "\n",
      "Epoch 00017: loss improved from 1.86251 to 1.82370, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.7783\n",
      "\n",
      "Epoch 00018: loss improved from 1.82370 to 1.77832, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/20\n",
      "5929/5929 [==============================] - 158s 27ms/step - loss: 1.7272\n",
      "\n",
      "Epoch 00019: loss improved from 1.77832 to 1.72723, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.6870\n",
      "\n",
      "Epoch 00020: loss improved from 1.72723 to 1.68699, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f56d00080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.6292\n",
      "\n",
      "Epoch 00001: loss improved from 1.68699 to 1.62925, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.5841\n",
      "\n",
      "Epoch 00002: loss improved from 1.62925 to 1.58411, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.5526\n",
      "\n",
      "Epoch 00003: loss improved from 1.58411 to 1.55259, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.4836\n",
      "\n",
      "Epoch 00004: loss improved from 1.55259 to 1.48358, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.4374\n",
      "\n",
      "Epoch 00005: loss improved from 1.48358 to 1.43740, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.4047\n",
      "\n",
      "Epoch 00006: loss improved from 1.43740 to 1.40468, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.3731\n",
      "\n",
      "Epoch 00007: loss improved from 1.40468 to 1.37305, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 1.3045\n",
      "\n",
      "Epoch 00008: loss improved from 1.37305 to 1.30448, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.2809\n",
      "\n",
      "Epoch 00009: loss improved from 1.30448 to 1.28089, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.2314\n",
      "\n",
      "Epoch 00010: loss improved from 1.28089 to 1.23145, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1794\n",
      "\n",
      "Epoch 00011: loss improved from 1.23145 to 1.17941, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1421\n",
      "\n",
      "Epoch 00012: loss improved from 1.17941 to 1.14205, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1276\n",
      "\n",
      "Epoch 00013: loss improved from 1.14205 to 1.12757, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.0674\n",
      "\n",
      "Epoch 00014: loss improved from 1.12757 to 1.06736, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 1.0508\n",
      "\n",
      "Epoch 00015: loss improved from 1.06736 to 1.05076, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 1.0113\n",
      "\n",
      "Epoch 00016: loss improved from 1.05076 to 1.01128, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 0.9626\n",
      "\n",
      "Epoch 00017: loss improved from 1.01128 to 0.96265, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.9304\n",
      "\n",
      "Epoch 00018: loss improved from 0.96265 to 0.93042, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/20\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.9037\n",
      "\n",
      "Epoch 00019: loss improved from 0.93042 to 0.90374, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.8637\n",
      "\n",
      "Epoch 00020: loss improved from 0.90374 to 0.86370, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f02f97978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.8390\n",
      "\n",
      "Epoch 00001: loss improved from 0.86370 to 0.83903, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.8051\n",
      "\n",
      "Epoch 00002: loss improved from 0.83903 to 0.80507, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.7663\n",
      "\n",
      "Epoch 00003: loss improved from 0.80507 to 0.76631, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.7368\n",
      "\n",
      "Epoch 00004: loss improved from 0.76631 to 0.73677, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.7190\n",
      "\n",
      "Epoch 00005: loss improved from 0.73677 to 0.71900, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.6889\n",
      "\n",
      "Epoch 00006: loss improved from 0.71900 to 0.68887, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.6558\n",
      "\n",
      "Epoch 00007: loss improved from 0.68887 to 0.65578, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/40\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 0.6278\n",
      "\n",
      "Epoch 00008: loss improved from 0.65578 to 0.62784, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.6190\n",
      "\n",
      "Epoch 00009: loss improved from 0.62784 to 0.61905, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.6004\n",
      "\n",
      "Epoch 00010: loss improved from 0.61905 to 0.60044, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.5642\n",
      "\n",
      "Epoch 00011: loss improved from 0.60044 to 0.56416, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.5591\n",
      "\n",
      "Epoch 00012: loss improved from 0.56416 to 0.55909, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.5407\n",
      "\n",
      "Epoch 00013: loss improved from 0.55909 to 0.54067, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.5241\n",
      "\n",
      "Epoch 00014: loss improved from 0.54067 to 0.52412, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.5024\n",
      "\n",
      "Epoch 00015: loss improved from 0.52412 to 0.50243, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4667\n",
      "\n",
      "Epoch 00016: loss improved from 0.50243 to 0.46669, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4619\n",
      "\n",
      "Epoch 00017: loss improved from 0.46669 to 0.46194, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4424\n",
      "\n",
      "Epoch 00018: loss improved from 0.46194 to 0.44238, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4270\n",
      "\n",
      "Epoch 00019: loss improved from 0.44238 to 0.42700, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4117\n",
      "\n",
      "Epoch 00020: loss improved from 0.42700 to 0.41169, saving model to model_weights_saved.hdf5\n",
      "Epoch 21/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3887\n",
      "\n",
      "Epoch 00021: loss improved from 0.41169 to 0.38866, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3865\n",
      "\n",
      "Epoch 00022: loss improved from 0.38866 to 0.38649, saving model to model_weights_saved.hdf5\n",
      "Epoch 23/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3602\n",
      "\n",
      "Epoch 00023: loss improved from 0.38649 to 0.36019, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3662\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.36019\n",
      "Epoch 25/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3583\n",
      "\n",
      "Epoch 00025: loss improved from 0.36019 to 0.35831, saving model to model_weights_saved.hdf5\n",
      "Epoch 26/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.3340\n",
      "\n",
      "Epoch 00026: loss improved from 0.35831 to 0.33399, saving model to model_weights_saved.hdf5\n",
      "Epoch 27/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3295\n",
      "\n",
      "Epoch 00027: loss improved from 0.33399 to 0.32953, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.3188\n",
      "\n",
      "Epoch 00028: loss improved from 0.32953 to 0.31880, saving model to model_weights_saved.hdf5\n",
      "Epoch 29/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2929\n",
      "\n",
      "Epoch 00029: loss improved from 0.31880 to 0.29288, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2943\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.29288\n",
      "Epoch 31/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2797\n",
      "\n",
      "Epoch 00031: loss improved from 0.29288 to 0.27972, saving model to model_weights_saved.hdf5\n",
      "Epoch 32/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2739\n",
      "\n",
      "Epoch 00032: loss improved from 0.27972 to 0.27387, saving model to model_weights_saved.hdf5\n",
      "Epoch 33/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2640\n",
      "\n",
      "Epoch 00033: loss improved from 0.27387 to 0.26402, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2527\n",
      "\n",
      "Epoch 00034: loss improved from 0.26402 to 0.25269, saving model to model_weights_saved.hdf5\n",
      "Epoch 35/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2519\n",
      "\n",
      "Epoch 00035: loss improved from 0.25269 to 0.25195, saving model to model_weights_saved.hdf5\n",
      "Epoch 36/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2395\n",
      "\n",
      "Epoch 00036: loss improved from 0.25195 to 0.23954, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2222\n",
      "\n",
      "Epoch 00037: loss improved from 0.23954 to 0.22224, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/40\n",
      "5929/5929 [==============================] - 14619s 2s/step - loss: 0.2338\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.22224\n",
      "Epoch 39/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 0.2230\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.22224\n",
      "Epoch 40/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.2229\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.22224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f02f97668>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.2186\n",
      "\n",
      "Epoch 00001: loss improved from 0.22224 to 0.21859, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2072\n",
      "\n",
      "Epoch 00002: loss improved from 0.21859 to 0.20716, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1941\n",
      "\n",
      "Epoch 00003: loss improved from 0.20716 to 0.19415, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1915\n",
      "\n",
      "Epoch 00004: loss improved from 0.19415 to 0.19152, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1760\n",
      "\n",
      "Epoch 00005: loss improved from 0.19152 to 0.17600, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1755\n",
      "\n",
      "Epoch 00006: loss improved from 0.17600 to 0.17551, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1813\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.17551\n",
      "Epoch 8/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1717\n",
      "\n",
      "Epoch 00008: loss improved from 0.17551 to 0.17171, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1591\n",
      "\n",
      "Epoch 00009: loss improved from 0.17171 to 0.15912, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1613\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.15912\n",
      "Epoch 11/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1530\n",
      "\n",
      "Epoch 00011: loss improved from 0.15912 to 0.15303, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1523\n",
      "\n",
      "Epoch 00012: loss improved from 0.15303 to 0.15228, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1612\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.15228\n",
      "Epoch 14/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1522\n",
      "\n",
      "Epoch 00014: loss improved from 0.15228 to 0.15219, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1488\n",
      "\n",
      "Epoch 00015: loss improved from 0.15219 to 0.14879, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1396\n",
      "\n",
      "Epoch 00016: loss improved from 0.14879 to 0.13955, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1374\n",
      "\n",
      "Epoch 00017: loss improved from 0.13955 to 0.13741, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1355\n",
      "\n",
      "Epoch 00018: loss improved from 0.13741 to 0.13550, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1236\n",
      "\n",
      "Epoch 00019: loss improved from 0.13550 to 0.12365, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1261\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.12365\n",
      "Epoch 21/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1189\n",
      "\n",
      "Epoch 00021: loss improved from 0.12365 to 0.11894, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1203\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.11894\n",
      "Epoch 23/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1175\n",
      "\n",
      "Epoch 00023: loss improved from 0.11894 to 0.11748, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1197\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.11748\n",
      "Epoch 25/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1226\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.11748\n",
      "Epoch 26/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1199\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.11748\n",
      "Epoch 27/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1045\n",
      "\n",
      "Epoch 00027: loss improved from 0.11748 to 0.10452, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1073\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.10452\n",
      "Epoch 29/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0982\n",
      "\n",
      "Epoch 00029: loss improved from 0.10452 to 0.09816, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1018\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.09816\n",
      "Epoch 31/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1016\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.09816\n",
      "Epoch 32/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1007\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.09816\n",
      "Epoch 33/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0953\n",
      "\n",
      "Epoch 00033: loss improved from 0.09816 to 0.09534, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0996\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.09534\n",
      "Epoch 35/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0958\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.09534\n",
      "Epoch 36/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0852\n",
      "\n",
      "Epoch 00036: loss improved from 0.09534 to 0.08516, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0820\n",
      "\n",
      "Epoch 00037: loss improved from 0.08516 to 0.08198, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1000\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.08198\n",
      "Epoch 39/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1047\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.08198\n",
      "Epoch 40/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0872\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.08198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f5b6074a8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture loss history, see  \n",
    "* https://stackoverflow.com/questions/38445982/how-to-log-keras-loss-output-to-a-file\n",
    "* https://forums.fast.ai/t/passing-multiple-callbacks-in-keras-early-stopping-modelcheckpoint-lrratescheduler/5477  \n",
    "(Haven't implemented it yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"model_weights_LSTM_character_Angela_attemp1.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\"  defeat seek peace security support wondered america beacon still burns bright tonight proved true s \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trength nation comes might arms scale wealth enduring power ideals democracy liberty opportunity unyielding hope true genius america america change union perfected already achieved gives us hope must achieve tomorrow election many firsts many stories told generations one mind tonight woman cast ballot atlanta lot like millions others stood line make voice heard election except one thing ann nixon cooper 106 years old born generation past slavery time cars road planes sky someone like vote two reasons woman color skin tonight think seen throughout century america heartache hope struggle progress times told people pressed american creed yes time women voices silenced hopes dismissed lived see stand speak reach ballot yes despair dust bowl depression across land saw nation conquer fear new deal new jobs new sense common purpose yes bombs fell harbor tyranny threatened world witness generation rise greatness democracy saved yes buses montgomery hoses birmingham bridge selma preacher atlant"
     ]
    }
   ],
   "source": [
    "# Below are the generated after 160 epoches\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
