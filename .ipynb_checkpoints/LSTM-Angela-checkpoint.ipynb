{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "# from keras.utils import np_utils\n",
    "# from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angela/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"CorpusOfPresidentialSpeeches/obama/obama_speeches_000.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Create input data to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Why convert all to lower case and removing special characters?  \n",
    "* Why remove stop words?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    # Remove the title and date (the first two row)\n",
    "    startChar = [word.end() for word in re.finditer(\"\\n\",file)][1]\n",
    "    input2 = input[startChar:]\n",
    "    \n",
    "    # lowercase everything to standardize it\n",
    "    input2 = input2.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input2)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyone still doubts america place things possible still wonders dream founders alive time still questions power democracy tonight answer answer told lines stretched around schools churches numbers nation never seen people waited three hours four hours many first time lives believed time must different voice could difference answer spoken young old rich poor democrat republican black white latino asian native american gay straight disabled disabled americans sent message world never collection red states blue states always united states america answer led told long many cynical fearful doubtful achieve put hands arc history bend toward hope better day long time coming tonight day election defining moment change come america received gracious call senator mccain fought long hard campaign fought even longer harder country loves endured sacrifices america us cannot begin imagine better service rendered brave selfless leader congratulate governor palin achieved look forward working renew nation promise months ahead want thank partner journey man campaigned heart spoke men women grew streets scranton rode train home delaware vice president elect united states joe biden would standing tonight without unyielding support best friend last sixteen years rock family love life nation next first lady michelle obama sasha malia love much earned new puppy coming us white house longer us know grandmother watching along family made miss tonight know debt beyond measure campaign manager david plouffe chief strategist david axelrod best campaign team ever assembled history politics made happen forever grateful sacrificed get done never forget victory truly belongs belongs never likeliest candidate office start much money many endorsements campaign hatched halls washington began backyards des moines living rooms concord front porches charleston built working men women dug little savings give five dollars ten dollars twenty dollars cause grew strength young people rejected myth generation apathy left homes families jobs offered little pay less sleep young people braved bitter cold scorching heat knock doors perfect strangers millions americans volunteered organized proved two centuries later government people people people perished earth victory know win election know understand enormity task lies ahead even celebrate tonight know challenges tomorrow bring greatest lifetime two wars planet peril worst financial crisis century even stand tonight know brave americans waking deserts iraq mountains afghanistan risk lives us mothers fathers lie awake children fall asleep wonder make mortgage pay doctor bills save enough college new energy harness new jobs created new schools build threats meet alliances repair road ahead long climb steep may get one year even one term america never hopeful tonight get promise people get setbacks false starts many agree every decision policy make president know government solve every problem always honest challenges face listen especially disagree ask join work remaking nation way done america two hundred twenty one years block block brick brick calloused hand calloused hand began twenty one months ago depths winter must end autumn night victory alone change seek chance us make change cannot happen go back way things cannot happen without let us summon new spirit patriotism service responsibility us resolves pitch work harder look let us remember financial crisis taught us anything cannot thriving wall street main street suffers country rise fall one nation one people let us resist temptation fall back partisanship pettiness immaturity poisoned politics long let us remember man state first carried banner republican party white house party founded values self reliance individual liberty national unity values share democratic party great victory tonight measure humility determination heal divides held back progress lincoln said nation far divided enemies friends though passion may strained must break bonds affection americans whose support yet earn may vote hear voices need help president watching tonight beyond shores parliaments palaces huddled around radios forgotten corners world stories singular destiny shared new dawn american leadership hand would tear world defeat seek peace security support wondered america beacon still burns bright tonight proved true strength nation comes might arms scale wealth enduring power ideals democracy liberty opportunity unyielding hope true genius america america change union perfected already achieved gives us hope must achieve tomorrow election many firsts many stories told generations one mind tonight woman cast ballot atlanta lot like millions others stood line make voice heard election except one thing ann nixon cooper 106 years old born generation past slavery time cars road planes sky someone like vote two reasons woman color skin tonight think seen throughout century america heartache hope struggle progress times told people pressed american creed yes time women voices silenced hopes dismissed lived see stand speak reach ballot yes despair dust bowl depression across land saw nation conquer fear new deal new jobs new sense common purpose yes bombs fell harbor tyranny threatened world witness generation rise greatness democracy saved yes buses montgomery hoses birmingham bridge selma preacher atlanta told people shall overcome yes man touched moon wall came berlin world connected science imagination year election touched finger screen cast vote 106 years america best times darkest hours knows america change yes america come far seen much much tonight let us ask children live see next century daughters lucky live long ann nixon cooper change see progress made chance answer call moment time put people back work open doors opportunity kids restore prosperity promote cause peace reclaim american dream reaffirm fundamental truth many one breathe hope met cynicism doubt tell us respond timeless creed sums spirit people yes thank god bless may god bless united states america'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "* Shall we add all other numbers to the dictionary below?  \n",
    "\n",
    "Note:\n",
    "* Need to find paper to justify choosing character-level generation over word-level generation. OR we can try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '0': 1,\n",
       " '1': 2,\n",
       " '6': 3,\n",
       " 'a': 4,\n",
       " 'b': 5,\n",
       " 'c': 6,\n",
       " 'd': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'h': 11,\n",
       " 'i': 12,\n",
       " 'j': 13,\n",
       " 'k': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'q': 20,\n",
       " 'r': 21,\n",
       " 's': 22,\n",
       " 't': 23,\n",
       " 'u': 24,\n",
       " 'v': 25,\n",
       " 'w': 26,\n",
       " 'x': 27,\n",
       " 'y': 28,\n",
       " 'z': 29}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the characters in our input to number\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 6029\n",
      "Total vocab: 30\n"
     ]
    }
   ],
   "source": [
    "# We need the total length of our inputs and total length of our set of characters \n",
    "# for later data prep, so we'll store these in a variable.\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the entire list of inputs and convert the characters to numbers\n",
    "\n",
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll doubts america place things possible still wonders dream founders alive time still questions powe\n",
      "-----\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "# Check my understanding\n",
    "i = 10\n",
    "in_seq = processed_inputs[i:i + seq_length]\n",
    "out_seq = processed_inputs[i + seq_length]\n",
    "print(in_seq)\n",
    "print('-----')\n",
    "print(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 5929\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "* I don't understand the logic of converting `X` to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our input sequences into a processed numpy array that our network can use\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "# convert the numpy array values into floats so that the sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode our label data\n",
    "y = keras.utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/angela/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9223\n",
      "Epoch 00001: loss improved from 2.92567 to 2.92074, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 52s 9ms/sample - loss: 2.9207\n",
      "Epoch 2/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9162\n",
      "Epoch 00002: loss improved from 2.92074 to 2.91717, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 50s 8ms/sample - loss: 2.9172\n",
      "Epoch 3/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9186\n",
      "Epoch 00003: loss did not improve from 2.91717\n",
      "5929/5929 [==============================] - 50s 8ms/sample - loss: 2.9190\n",
      "Epoch 4/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9158\n",
      "Epoch 00004: loss improved from 2.91717 to 2.91675, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 51s 9ms/sample - loss: 2.9167\n",
      "Epoch 5/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9163\n",
      "Epoch 00005: loss improved from 2.91675 to 2.91649, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 50s 8ms/sample - loss: 2.9165\n",
      "Epoch 6/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9145\n",
      "Epoch 00006: loss improved from 2.91649 to 2.91306, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 51s 9ms/sample - loss: 2.9131\n",
      "Epoch 7/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9103\n",
      "Epoch 00007: loss improved from 2.91306 to 2.90960, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 50s 8ms/sample - loss: 2.9096\n",
      "Epoch 8/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9116\n",
      "Epoch 00008: loss did not improve from 2.90960\n",
      "5929/5929 [==============================] - 51s 9ms/sample - loss: 2.9115\n",
      "Epoch 9/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9109\n",
      "Epoch 00009: loss did not improve from 2.90960\n",
      "5929/5929 [==============================] - 51s 9ms/sample - loss: 2.9114\n",
      "Epoch 10/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9071\n",
      "Epoch 00010: loss improved from 2.90960 to 2.90760, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 52s 9ms/sample - loss: 2.9076\n",
      "Epoch 11/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9025\n",
      "Epoch 00011: loss improved from 2.90760 to 2.90222, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 53s 9ms/sample - loss: 2.9022\n",
      "Epoch 12/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.9046\n",
      "Epoch 00012: loss did not improve from 2.90222\n",
      "5929/5929 [==============================] - 52s 9ms/sample - loss: 2.9044\n",
      "Epoch 13/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.8963\n",
      "Epoch 00013: loss improved from 2.90222 to 2.89599, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 52s 9ms/sample - loss: 2.8960\n",
      "Epoch 14/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.8843\n",
      "Epoch 00014: loss improved from 2.89599 to 2.88507, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 50s 8ms/sample - loss: 2.8851\n",
      "Epoch 15/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.8668\n",
      "Epoch 00015: loss improved from 2.88507 to 2.86840, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 49s 8ms/sample - loss: 2.8684\n",
      "Epoch 16/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.8498\n",
      "Epoch 00016: loss improved from 2.86840 to 2.84925, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 49s 8ms/sample - loss: 2.8493\n",
      "Epoch 17/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.8276\n",
      "Epoch 00017: loss improved from 2.84925 to 2.82731, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 49s 8ms/sample - loss: 2.8273\n",
      "Epoch 18/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.7900\n",
      "Epoch 00018: loss improved from 2.82731 to 2.78965, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 49s 8ms/sample - loss: 2.7897\n",
      "Epoch 19/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.7486\n",
      "Epoch 00019: loss improved from 2.78965 to 2.74773, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 51s 9ms/sample - loss: 2.7477\n",
      "Epoch 20/20\n",
      "5888/5929 [============================>.] - ETA: 0s - loss: 2.7135\n",
      "Epoch 00020: loss improved from 2.74773 to 2.71298, saving model to model_weights_saved.hdf5\n",
      "5929/5929 [==============================] - 55s 9ms/sample - loss: 2.7130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3258c190>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\"  still wonders dream founders alive time still questions power democracy tonight answer answer told  \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
