{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts here are used for tokenization and preparing the train/dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, getcwd\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are titles, punctuations, and contractions that need to be replaced.\n",
    "\n",
    "replacing_titles = ['Dr.','Esq.','Hon.','Jr.','Mr.','Mrs.','Ms.','Messrs.','Mmes.','Msgr.','Prof.','Rev.','Rt. Hon.','Sr.','St.']\n",
    "punctuation_marks = ['\"', \"'\", \"!\", \"@\", \"#\", \"$\", \"%\", \"&\", \"*\", \"(\", \")\", \"-\", \"?\", \",\", \".\"]\n",
    "contractions_dict = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he had\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I had\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"iit will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she had\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that had\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they had\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "country_acronyms = {\"U.S\": \"United States\", \"U.S.A\": \"United States of America\", \"U.A.E\": \"United Arab Emirates\", \"U.S.S.R\": \"Union of Soviet Socialist Republics\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input_file):\n",
    "    \"\"\"\n",
    "    This function accomplishes four purposes:\n",
    "    1. Remove the title and date (the first two rows) from the input file\n",
    "    2. Remove all special characters except for . and ,\n",
    "    3. Convert all characters to lower case\n",
    "    4. Tokenize words\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): input file\n",
    "        \n",
    "    Returns:\n",
    "        output_file (str): tokenized strings separated by space\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    input_file = open(input_file, 'r').read()\n",
    "    \n",
    "    # Remove the title and date (the first two rows)\n",
    "    \n",
    "    startChar = [word.end() for word in re.finditer(\"\\n\", input_file)][1]\n",
    "    input2 = input_file[startChar:]\n",
    "    \n",
    "    # Remove things in angle quotes which are added to account for crowd reactions\n",
    "    input2 = re.sub(r\"\\<[^\\>]*\\>\", '', input2)\n",
    "    \n",
    "    # lowercase everything so that we have less tokens to predict. i.e., no need to distinguish a vs. A\n",
    "    #input2 = input2.lower()\n",
    "    \n",
    "    # Standardize contractions\n",
    "    for k, v in contractions_dict.items():\n",
    "        input2 = input2.replace(k, v) \n",
    "        \n",
    "        k_caps = k[:1].upper() + k[1:]\n",
    "        v_caps = v[:1].upper() + v[1:]\n",
    "        \n",
    "        input2 = input2.replace(k_caps, v_caps)\n",
    "        \n",
    "    # Replace country acronyms\n",
    "    for k, v in country_acronyms.items():\n",
    "        input2 = input2.replace(k, v)\n",
    "        \n",
    "    # Replace titles\n",
    "    for v in replacing_titles:\n",
    "        input2 = input2.replace(v, '')\n",
    "        \n",
    "    # Remove middle initial\n",
    "    input2 = re.sub(r\"([A-Z])\\W \", '', input2)\n",
    "    \n",
    "    # Keep all the words and digitis\n",
    "    # Keep only two special characters: . and ,\n",
    "    # If we want to keep carriage return, add |\\n\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[\\.\\,]')\n",
    "    tokens = tokenizer.tokenize(input2)\n",
    "    output_file = \" \".join(tokens)\n",
    "    output_file = output_file + \"<speech_sep>\"\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['hoover_speeches_022.txt' 'hoover_speeches_016.txt'\n",
      " 'hoover_speeches_012.txt' 'hoover_speeches_027.txt'\n",
      " 'hoover_speeches_009.txt' 'hoover_speeches_015.txt'\n",
      " 'hoover_speeches_025.txt' 'hoover_speeches_002.txt'\n",
      " 'hoover_speeches_021.txt' 'hoover_speeches_024.txt'\n",
      " 'hoover_speeches_013.txt' 'hoover_speeches_028.txt']\n",
      "Validation set:\n",
      "['hoover_speeches_010.txt' 'hoover_speeches_020.txt'\n",
      " 'hoover_speeches_011.txt' 'hoover_speeches_017.txt'\n",
      " 'hoover_speeches_001.txt' 'hoover_speeches_026.txt'\n",
      " 'hoover_speeches_019.txt' 'hoover_speeches_005.txt']\n",
      "Test set:\n",
      "['hoover_speeches_007.txt' 'hoover_speeches_004.txt'\n",
      " 'hoover_speeches_000.txt' 'hoover_speeches_023.txt'\n",
      " 'hoover_speeches_014.txt' 'hoover_speeches_003.txt'\n",
      " 'hoover_speeches_018.txt' 'hoover_speeches_008.txt'\n",
      " 'hoover_speeches_006.txt']\n",
      "Done\n",
      "Training set:\n",
      "['fdroosevelt_speeches_004.txt' 'fdroosevelt_speeches_018.txt'\n",
      " 'fdroosevelt_speeches_047.txt' 'fdroosevelt_speeches_007.txt'\n",
      " 'fdroosevelt_speeches_027.txt' 'fdroosevelt_speeches_017.txt'\n",
      " 'fdroosevelt_speeches_011.txt' 'fdroosevelt_speeches_008.txt'\n",
      " 'fdroosevelt_speeches_033.txt' 'fdroosevelt_speeches_034.txt'\n",
      " 'fdroosevelt_speeches_016.txt' 'fdroosevelt_speeches_048.txt'\n",
      " 'fdroosevelt_speeches_009.txt' 'fdroosevelt_speeches_024.txt'\n",
      " 'fdroosevelt_speeches_003.txt' 'fdroosevelt_speeches_002.txt'\n",
      " 'fdroosevelt_speeches_032.txt' 'fdroosevelt_speeches_005.txt'\n",
      " 'fdroosevelt_speeches_021.txt' 'fdroosevelt_speeches_031.txt']\n",
      "Validation set:\n",
      "['fdroosevelt_speeches_022.txt' 'fdroosevelt_speeches_000.txt'\n",
      " 'fdroosevelt_speeches_039.txt' 'fdroosevelt_speeches_010.txt'\n",
      " 'fdroosevelt_speeches_043.txt' 'fdroosevelt_speeches_028.txt'\n",
      " 'fdroosevelt_speeches_015.txt' 'fdroosevelt_speeches_001.txt'\n",
      " 'fdroosevelt_speeches_036.txt' 'fdroosevelt_speeches_030.txt'\n",
      " 'fdroosevelt_speeches_023.txt' 'fdroosevelt_speeches_044.txt'\n",
      " 'fdroosevelt_speeches_042.txt' 'fdroosevelt_speeches_035.txt']\n",
      "Test set:\n",
      "['fdroosevelt_speeches_029.txt' 'fdroosevelt_speeches_020.txt'\n",
      " 'fdroosevelt_speeches_041.txt' 'fdroosevelt_speeches_037.txt'\n",
      " 'fdroosevelt_speeches_045.txt' 'fdroosevelt_speeches_006.txt'\n",
      " 'fdroosevelt_speeches_012.txt' 'fdroosevelt_speeches_040.txt'\n",
      " 'fdroosevelt_speeches_014.txt' 'fdroosevelt_speeches_026.txt'\n",
      " 'fdroosevelt_speeches_013.txt' 'fdroosevelt_speeches_019.txt'\n",
      " 'fdroosevelt_speeches_046.txt' 'fdroosevelt_speeches_025.txt'\n",
      " 'fdroosevelt_speeches_038.txt']\n",
      "Done\n",
      "Training set:\n",
      "['truman_speeches_007.txt' 'truman_speeches_014.txt'\n",
      " 'truman_speeches_004.txt' 'truman_speeches_009.txt'\n",
      " 'truman_speeches_018.txt' 'truman_speeches_017.txt'\n",
      " 'truman_speeches_005.txt' 'truman_speeches_013.txt']\n",
      "Validation set:\n",
      "['truman_speeches_002.txt' 'truman_speeches_015.txt'\n",
      " 'truman_speeches_000.txt' 'truman_speeches_016.txt'\n",
      " 'truman_speeches_006.txt' 'truman_speeches_011.txt']\n",
      "Test set:\n",
      "['truman_speeches_001.txt' 'truman_speeches_010.txt'\n",
      " 'truman_speeches_008.txt' 'truman_speeches_012.txt'\n",
      " 'truman_speeches_003.txt']\n",
      "Done\n",
      "Training set:\n",
      "['hayes_speeches_005.txt' 'hayes_speeches_004.txt'\n",
      " 'hayes_speeches_015.txt' 'hayes_speeches_014.txt'\n",
      " 'hayes_speeches_013.txt' 'hayes_speeches_009.txt']\n",
      "Validation set:\n",
      "['hayes_speeches_010.txt' 'hayes_speeches_002.txt'\n",
      " 'hayes_speeches_007.txt' 'hayes_speeches_008.txt'\n",
      " 'hayes_speeches_011.txt']\n",
      "Test set:\n",
      "['hayes_speeches_003.txt' 'hayes_speeches_000.txt'\n",
      " 'hayes_speeches_012.txt' 'hayes_speeches_006.txt'\n",
      " 'hayes_speeches_001.txt']\n",
      "Done\n",
      "Training set:\n",
      "['adams_speeches_008.txt' 'adams_speeches_001.txt'\n",
      " 'adams_speeches_003.txt' 'adams_speeches_007.txt']\n",
      "Validation set:\n",
      "['adams_speeches_005.txt' 'adams_speeches_000.txt']\n",
      "Test set:\n",
      "['adams_speeches_004.txt' 'adams_speeches_002.txt'\n",
      " 'adams_speeches_006.txt']\n",
      "Done\n",
      "Training set:\n",
      "['carter_speeches_012.txt' 'carter_speeches_019.txt'\n",
      " 'carter_speeches_006.txt' 'carter_speeches_020.txt'\n",
      " 'carter_speeches_000.txt' 'carter_speeches_010.txt'\n",
      " 'carter_speeches_005.txt' 'carter_speeches_017.txt'\n",
      " 'carter_speeches_015.txt']\n",
      "Validation set:\n",
      "['carter_speeches_018.txt' 'carter_speeches_014.txt'\n",
      " 'carter_speeches_016.txt' 'carter_speeches_001.txt'\n",
      " 'carter_speeches_011.txt' 'carter_speeches_013.txt']\n",
      "Test set:\n",
      "['carter_speeches_009.txt' 'carter_speeches_003.txt'\n",
      " 'carter_speeches_007.txt' 'carter_speeches_021.txt'\n",
      " 'carter_speeches_008.txt' 'carter_speeches_002.txt'\n",
      " 'carter_speeches_004.txt']\n",
      "Done\n",
      "Training set:\n",
      "['madison_speeches_007.txt' 'madison_speeches_018.txt'\n",
      " 'madison_speeches_008.txt' 'madison_speeches_017.txt'\n",
      " 'madison_speeches_014.txt' 'madison_speeches_011.txt'\n",
      " 'madison_speeches_012.txt' 'madison_speeches_002.txt'\n",
      " 'madison_speeches_004.txt']\n",
      "Validation set:\n",
      "['madison_speeches_001.txt' 'madison_speeches_015.txt'\n",
      " 'madison_speeches_013.txt' 'madison_speeches_016.txt'\n",
      " 'madison_speeches_009.txt' 'madison_speeches_021.txt']\n",
      "Test set:\n",
      "['madison_speeches_005.txt' 'madison_speeches_000.txt'\n",
      " 'madison_speeches_003.txt' 'madison_speeches_006.txt'\n",
      " 'madison_speeches_019.txt' 'madison_speeches_020.txt'\n",
      " 'madison_speeches_010.txt']\n",
      "Done\n",
      "Training set:\n",
      "['ford_speeches_002.txt' 'ford_speeches_003.txt' 'ford_speeches_012.txt'\n",
      " 'ford_speeches_005.txt' 'ford_speeches_013.txt' 'ford_speeches_010.txt']\n",
      "Validation set:\n",
      "['ford_speeches_007.txt' 'ford_speeches_004.txt' 'ford_speeches_011.txt'\n",
      " 'ford_speeches_009.txt']\n",
      "Test set:\n",
      "['ford_speeches_000.txt' 'ford_speeches_001.txt' 'ford_speeches_008.txt'\n",
      " 'ford_speeches_006.txt']\n",
      "Done\n",
      "Training set:\n",
      "['cleveland_speeches_017.txt' 'cleveland_speeches_004.txt'\n",
      " 'cleveland_speeches_029.txt' 'cleveland_speeches_026.txt'\n",
      " 'cleveland_speeches_018.txt' 'cleveland_speeches_027.txt'\n",
      " 'cleveland_speeches_009.txt' 'cleveland_speeches_012.txt'\n",
      " 'cleveland_speeches_011.txt' 'cleveland_speeches_022.txt'\n",
      " 'cleveland_speeches_000.txt' 'cleveland_speeches_010.txt']\n",
      "Validation set:\n",
      "['cleveland_speeches_013.txt' 'cleveland_speeches_016.txt'\n",
      " 'cleveland_speeches_005.txt' 'cleveland_speeches_030.txt'\n",
      " 'cleveland_speeches_020.txt' 'cleveland_speeches_021.txt'\n",
      " 'cleveland_speeches_023.txt' 'cleveland_speeches_025.txt'\n",
      " 'cleveland_speeches_002.txt' 'cleveland_speeches_007.txt']\n",
      "Test set:\n",
      "['cleveland_speeches_014.txt' 'cleveland_speeches_003.txt'\n",
      " 'cleveland_speeches_024.txt' 'cleveland_speeches_008.txt'\n",
      " 'cleveland_speeches_015.txt' 'cleveland_speeches_001.txt'\n",
      " 'cleveland_speeches_019.txt' 'cleveland_speeches_028.txt'\n",
      " 'cleveland_speeches_006.txt']\n",
      "Done\n",
      "Training set:\n",
      "['obama_speeches_033.txt' 'obama_speeches_005.txt'\n",
      " 'obama_speeches_046.txt' 'obama_speeches_021.txt'\n",
      " 'obama_speeches_007.txt' 'obama_speeches_041.txt'\n",
      " 'obama_speeches_013.txt' 'obama_speeches_003.txt'\n",
      " 'obama_speeches_036.txt' 'obama_speeches_039.txt'\n",
      " 'obama_speeches_001.txt' 'obama_speeches_020.txt'\n",
      " 'obama_speeches_044.txt' 'obama_speeches_023.txt'\n",
      " 'obama_speeches_022.txt' 'obama_speeches_009.txt'\n",
      " 'obama_speeches_019.txt' 'obama_speeches_010.txt'\n",
      " 'obama_speeches_027.txt']\n",
      "Validation set:\n",
      "['obama_speeches_049.txt' 'obama_speeches_026.txt'\n",
      " 'obama_speeches_032.txt' 'obama_speeches_048.txt'\n",
      " 'obama_speeches_018.txt' 'obama_speeches_017.txt'\n",
      " 'obama_speeches_000.txt' 'obama_speeches_002.txt'\n",
      " 'obama_speeches_043.txt' 'obama_speeches_031.txt'\n",
      " 'obama_speeches_045.txt' 'obama_speeches_015.txt'\n",
      " 'obama_speeches_014.txt' 'obama_speeches_006.txt']\n",
      "Test set:\n",
      "['obama_speeches_029.txt' 'obama_speeches_038.txt'\n",
      " 'obama_speeches_047.txt' 'obama_speeches_012.txt'\n",
      " 'obama_speeches_042.txt' 'obama_speeches_030.txt'\n",
      " 'obama_speeches_008.txt' 'obama_speeches_040.txt'\n",
      " 'obama_speeches_016.txt' 'obama_speeches_034.txt'\n",
      " 'obama_speeches_011.txt' 'obama_speeches_028.txt'\n",
      " 'obama_speeches_037.txt' 'obama_speeches_004.txt'\n",
      " 'obama_speeches_035.txt']\n",
      "Done\n",
      "Training set:\n",
      "['harding_speeches_005.txt' 'harding_speeches_000.txt'\n",
      " 'harding_speeches_010.txt' 'harding_speeches_001.txt'\n",
      " 'harding_speeches_012.txt' 'harding_speeches_014.txt'\n",
      " 'harding_speeches_006.txt']\n",
      "Validation set:\n",
      "['harding_speeches_007.txt' 'harding_speeches_002.txt'\n",
      " 'harding_speeches_015.txt' 'harding_speeches_011.txt'\n",
      " 'harding_speeches_013.txt' 'harding_speeches_004.txt']\n",
      "Test set:\n",
      "['harding_speeches_003.txt' 'harding_speeches_008.txt'\n",
      " 'harding_speeches_009.txt' 'harding_speeches_016.txt'\n",
      " 'harding_speeches_017.txt']\n",
      "Done\n",
      "Training set:\n",
      "['wilson_speeches_031.txt' 'wilson_speeches_027.txt'\n",
      " 'wilson_speeches_024.txt' 'wilson_speeches_016.txt'\n",
      " 'wilson_speeches_000.txt' 'wilson_speeches_025.txt'\n",
      " 'wilson_speeches_029.txt' 'wilson_speeches_017.txt'\n",
      " 'wilson_speeches_004.txt' 'wilson_speeches_014.txt'\n",
      " 'wilson_speeches_003.txt' 'wilson_speeches_001.txt'\n",
      " 'wilson_speeches_005.txt']\n",
      "Validation set:\n",
      "['wilson_speeches_009.txt' 'wilson_speeches_022.txt'\n",
      " 'wilson_speeches_023.txt' 'wilson_speeches_021.txt'\n",
      " 'wilson_speeches_010.txt' 'wilson_speeches_013.txt'\n",
      " 'wilson_speeches_028.txt' 'wilson_speeches_012.txt'\n",
      " 'wilson_speeches_020.txt' 'wilson_speeches_015.txt']\n",
      "Test set:\n",
      "['wilson_speeches_008.txt' 'wilson_speeches_026.txt'\n",
      " 'wilson_speeches_011.txt' 'wilson_speeches_030.txt'\n",
      " 'wilson_speeches_007.txt' 'wilson_speeches_019.txt'\n",
      " 'wilson_speeches_006.txt' 'wilson_speeches_002.txt'\n",
      " 'wilson_speeches_018.txt']\n",
      "Done\n",
      "Training set:\n",
      "['taylor_speeches_002.txt' 'taylor_speeches_001.txt']\n",
      "Validation set:\n",
      "['taylor_speeches_000.txt']\n",
      "Test set:\n",
      "['taylor_speeches_003.txt']\n",
      "Done\n",
      "Training set:\n",
      "['monroe_speeches_005.txt' 'monroe_speeches_000.txt'\n",
      " 'monroe_speeches_007.txt' 'monroe_speeches_001.txt']\n",
      "Validation set:\n",
      "['monroe_speeches_006.txt' 'monroe_speeches_009.txt'\n",
      " 'monroe_speeches_004.txt']\n",
      "Test set:\n",
      "['monroe_speeches_002.txt' 'monroe_speeches_008.txt'\n",
      " 'monroe_speeches_003.txt']\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['lincoln_speeches_011.txt' 'lincoln_speeches_006.txt'\n",
      " 'lincoln_speeches_009.txt' 'lincoln_speeches_010.txt'\n",
      " 'lincoln_speeches_007.txt' 'lincoln_speeches_008.txt']\n",
      "Validation set:\n",
      "['lincoln_speeches_014.txt' 'lincoln_speeches_003.txt'\n",
      " 'lincoln_speeches_013.txt' 'lincoln_speeches_005.txt']\n",
      "Test set:\n",
      "['lincoln_speeches_001.txt' 'lincoln_speeches_000.txt'\n",
      " 'lincoln_speeches_004.txt' 'lincoln_speeches_002.txt'\n",
      " 'lincoln_speeches_012.txt']\n",
      "Done\n",
      "Training set:\n",
      "[]\n",
      "Validation set:\n",
      "[]\n",
      "Test set:\n",
      "['BERT v1-checkpoint.ipynb']\n",
      "Done\n",
      "Training set:\n",
      "['jefferson_speeches_013.txt' 'jefferson_speeches_012.txt'\n",
      " 'jefferson_speeches_010.txt' 'jefferson_speeches_014.txt'\n",
      " 'jefferson_speeches_008.txt' 'jefferson_speeches_018.txt'\n",
      " 'jefferson_speeches_021.txt' 'jefferson_speeches_003.txt'\n",
      " 'jefferson_speeches_005.txt' 'jefferson_speeches_015.txt']\n",
      "Validation set:\n",
      "['jefferson_speeches_019.txt' 'jefferson_speeches_007.txt'\n",
      " 'jefferson_speeches_002.txt' 'jefferson_speeches_001.txt'\n",
      " 'jefferson_speeches_016.txt' 'jefferson_speeches_004.txt'\n",
      " 'jefferson_speeches_006.txt']\n",
      "Test set:\n",
      "['jefferson_speeches_000.txt' 'jefferson_speeches_017.txt'\n",
      " 'jefferson_speeches_009.txt' 'jefferson_speeches_020.txt'\n",
      " 'jefferson_speeches_011.txt' 'jefferson_speeches_022.txt'\n",
      " 'jefferson_speeches_023.txt']\n",
      "Done\n",
      "Training set:\n",
      "['vanburen_speeches_001.txt' 'vanburen_speeches_002.txt'\n",
      " 'vanburen_speeches_004.txt' 'vanburen_speeches_009.txt']\n",
      "Validation set:\n",
      "['vanburen_speeches_005.txt' 'vanburen_speeches_006.txt'\n",
      " 'vanburen_speeches_008.txt']\n",
      "Test set:\n",
      "['vanburen_speeches_003.txt' 'vanburen_speeches_000.txt'\n",
      " 'vanburen_speeches_007.txt']\n",
      "Done\n",
      "Training set:\n",
      "['jackson_speeches_024.txt' 'jackson_speeches_002.txt'\n",
      " 'jackson_speeches_008.txt' 'jackson_speeches_015.txt'\n",
      " 'jackson_speeches_013.txt' 'jackson_speeches_005.txt'\n",
      " 'jackson_speeches_012.txt' 'jackson_speeches_014.txt'\n",
      " 'jackson_speeches_025.txt' 'jackson_speeches_009.txt']\n",
      "Validation set:\n",
      "['jackson_speeches_003.txt' 'jackson_speeches_000.txt'\n",
      " 'jackson_speeches_010.txt' 'jackson_speeches_001.txt'\n",
      " 'jackson_speeches_023.txt' 'jackson_speeches_021.txt'\n",
      " 'jackson_speeches_018.txt' 'jackson_speeches_016.txt']\n",
      "Test set:\n",
      "['jackson_speeches_020.txt' 'jackson_speeches_011.txt'\n",
      " 'jackson_speeches_007.txt' 'jackson_speeches_019.txt'\n",
      " 'jackson_speeches_017.txt' 'jackson_speeches_022.txt'\n",
      " 'jackson_speeches_004.txt' 'jackson_speeches_006.txt']\n",
      "Done\n",
      "Training set:\n",
      "['washington_speeches_004.txt' 'washington_speeches_017.txt'\n",
      " 'washington_speeches_019.txt' 'washington_speeches_013.txt'\n",
      " 'washington_speeches_015.txt' 'washington_speeches_000.txt'\n",
      " 'washington_speeches_009.txt' 'washington_speeches_012.txt']\n",
      "Validation set:\n",
      "['washington_speeches_014.txt' 'washington_speeches_006.txt'\n",
      " 'washington_speeches_007.txt' 'washington_speeches_020.txt'\n",
      " 'washington_speeches_008.txt' 'washington_speeches_010.txt']\n",
      "Test set:\n",
      "['washington_speeches_003.txt' 'washington_speeches_011.txt'\n",
      " 'washington_speeches_018.txt' 'washington_speeches_001.txt'\n",
      " 'washington_speeches_016.txt' 'washington_speeches_002.txt'\n",
      " 'washington_speeches_005.txt']\n",
      "Done\n",
      "Training set:\n",
      "['polk_speeches_020.txt' 'polk_speeches_011.txt' 'polk_speeches_019.txt'\n",
      " 'polk_speeches_003.txt' 'polk_speeches_010.txt' 'polk_speeches_014.txt'\n",
      " 'polk_speeches_005.txt' 'polk_speeches_018.txt' 'polk_speeches_004.txt'\n",
      " 'polk_speeches_000.txt']\n",
      "Validation set:\n",
      "['polk_speeches_009.txt' 'polk_speeches_021.txt' 'polk_speeches_006.txt'\n",
      " 'polk_speeches_024.txt' 'polk_speeches_013.txt' 'polk_speeches_012.txt'\n",
      " 'polk_speeches_015.txt' 'polk_speeches_016.txt']\n",
      "Test set:\n",
      "['polk_speeches_023.txt' 'polk_speeches_007.txt' 'polk_speeches_017.txt'\n",
      " 'polk_speeches_008.txt' 'polk_speeches_001.txt' 'polk_speeches_022.txt'\n",
      " 'polk_speeches_002.txt']\n",
      "Done\n",
      "Training set:\n",
      "['bush_speeches_013.txt' 'bush_speeches_014.txt' 'bush_speeches_019.txt'\n",
      " 'bush_speeches_001.txt' 'bush_speeches_007.txt' 'bush_speeches_005.txt'\n",
      " 'bush_speeches_011.txt' 'bush_speeches_015.txt' 'bush_speeches_012.txt']\n",
      "Validation set:\n",
      "['bush_speeches_008.txt' 'bush_speeches_020.txt' 'bush_speeches_009.txt'\n",
      " 'bush_speeches_002.txt' 'bush_speeches_003.txt' 'bush_speeches_021.txt'\n",
      " 'bush_speeches_016.txt']\n",
      "Test set:\n",
      "['bush_speeches_000.txt' 'bush_speeches_004.txt' 'bush_speeches_017.txt'\n",
      " 'bush_speeches_006.txt' 'bush_speeches_018.txt' 'bush_speeches_010.txt'\n",
      " 'bush_speeches_022.txt']\n",
      "Done\n",
      "Training set:\n",
      "['gwbush_speeches_016.txt' 'gwbush_speeches_033.txt'\n",
      " 'gwbush_speeches_021.txt' 'gwbush_speeches_024.txt'\n",
      " 'gwbush_speeches_009.txt' 'gwbush_speeches_029.txt'\n",
      " 'gwbush_speeches_020.txt' 'gwbush_speeches_032.txt'\n",
      " 'gwbush_speeches_031.txt' 'gwbush_speeches_005.txt'\n",
      " 'gwbush_speeches_017.txt' 'gwbush_speeches_037.txt'\n",
      " 'gwbush_speeches_019.txt' 'gwbush_speeches_036.txt'\n",
      " 'gwbush_speeches_001.txt' 'gwbush_speeches_034.txt']\n",
      "Validation set:\n",
      "['gwbush_speeches_035.txt' 'gwbush_speeches_018.txt'\n",
      " 'gwbush_speeches_028.txt' 'gwbush_speeches_000.txt'\n",
      " 'gwbush_speeches_030.txt' 'gwbush_speeches_015.txt'\n",
      " 'gwbush_speeches_022.txt' 'gwbush_speeches_003.txt'\n",
      " 'gwbush_speeches_023.txt' 'gwbush_speeches_025.txt'\n",
      " 'gwbush_speeches_002.txt' 'gwbush_speeches_026.txt']\n",
      "Test set:\n",
      "['gwbush_speeches_006.txt' 'gwbush_speeches_014.txt'\n",
      " 'gwbush_speeches_011.txt' 'gwbush_speeches_027.txt'\n",
      " 'gwbush_speeches_010.txt' 'gwbush_speeches_004.txt'\n",
      " 'gwbush_speeches_007.txt' 'gwbush_speeches_008.txt'\n",
      " 'gwbush_speeches_038.txt' 'gwbush_speeches_013.txt'\n",
      " 'gwbush_speeches_012.txt']\n",
      "Done\n",
      "Training set:\n",
      "['pierce_speeches_012.txt' 'pierce_speeches_006.txt'\n",
      " 'pierce_speeches_003.txt' 'pierce_speeches_001.txt'\n",
      " 'pierce_speeches_009.txt' 'pierce_speeches_014.txt']\n",
      "Validation set:\n",
      "['pierce_speeches_008.txt' 'pierce_speeches_007.txt'\n",
      " 'pierce_speeches_005.txt' 'pierce_speeches_010.txt']\n",
      "Test set:\n",
      "['pierce_speeches_000.txt' 'pierce_speeches_002.txt'\n",
      " 'pierce_speeches_011.txt' 'pierce_speeches_013.txt'\n",
      " 'pierce_speeches_004.txt']\n",
      "Done\n",
      "Training set:\n",
      "['reagan_speeches_038.txt' 'reagan_speeches_017.txt'\n",
      " 'reagan_speeches_019.txt' 'reagan_speeches_056.txt'\n",
      " 'reagan_speeches_009.txt' 'reagan_speeches_051.txt'\n",
      " 'reagan_speeches_037.txt' 'reagan_speeches_002.txt'\n",
      " 'reagan_speeches_018.txt' 'reagan_speeches_052.txt'\n",
      " 'reagan_speeches_057.txt' 'reagan_speeches_012.txt'\n",
      " 'reagan_speeches_047.txt' 'reagan_speeches_039.txt'\n",
      " 'reagan_speeches_014.txt' 'reagan_speeches_027.txt'\n",
      " 'reagan_speeches_024.txt' 'reagan_speeches_050.txt'\n",
      " 'reagan_speeches_000.txt' 'reagan_speeches_007.txt'\n",
      " 'reagan_speeches_011.txt' 'reagan_speeches_003.txt'\n",
      " 'reagan_speeches_046.txt' 'reagan_speeches_005.txt']\n",
      "Validation set:\n",
      "['reagan_speeches_010.txt' 'reagan_speeches_022.txt'\n",
      " 'reagan_speeches_042.txt' 'reagan_speeches_016.txt'\n",
      " 'reagan_speeches_033.txt' 'reagan_speeches_021.txt'\n",
      " 'reagan_speeches_029.txt' 'reagan_speeches_020.txt'\n",
      " 'reagan_speeches_030.txt' 'reagan_speeches_013.txt'\n",
      " 'reagan_speeches_053.txt' 'reagan_speeches_026.txt'\n",
      " 'reagan_speeches_045.txt' 'reagan_speeches_025.txt'\n",
      " 'reagan_speeches_008.txt' 'reagan_speeches_023.txt'\n",
      " 'reagan_speeches_004.txt' 'reagan_speeches_044.txt']\n",
      "Test set:\n",
      "['reagan_speeches_036.txt' 'reagan_speeches_058.txt'\n",
      " 'reagan_speeches_049.txt' 'reagan_speeches_028.txt'\n",
      " 'reagan_speeches_006.txt' 'reagan_speeches_031.txt'\n",
      " 'reagan_speeches_055.txt' 'reagan_speeches_032.txt'\n",
      " 'reagan_speeches_015.txt' 'reagan_speeches_035.txt'\n",
      " 'reagan_speeches_001.txt' 'reagan_speeches_034.txt'\n",
      " 'reagan_speeches_054.txt' 'reagan_speeches_041.txt'\n",
      " 'reagan_speeches_043.txt' 'reagan_speeches_040.txt'\n",
      " 'reagan_speeches_048.txt']\n",
      "Done\n",
      "Training set:\n",
      "[]\n",
      "Validation set:\n",
      "[]\n",
      "Test set:\n",
      "['garfield_speeches_000.txt']\n",
      "Done\n",
      "Training set:\n",
      "['mckinley_speeches_008.txt' 'mckinley_speeches_007.txt'\n",
      " 'mckinley_speeches_006.txt' 'mckinley_speeches_000.txt'\n",
      " 'mckinley_speeches_003.txt' 'mckinley_speeches_011.txt']\n",
      "Validation set:\n",
      "['mckinley_speeches_013.txt' 'mckinley_speeches_002.txt'\n",
      " 'mckinley_speeches_005.txt' 'mckinley_speeches_012.txt']\n",
      "Test set:\n",
      "['mckinley_speeches_001.txt' 'mckinley_speeches_004.txt'\n",
      " 'mckinley_speeches_010.txt' 'mckinley_speeches_009.txt']\n",
      "Done\n",
      "Training set:\n",
      "['coolidge_speeches_007.txt' 'coolidge_speeches_008.txt'\n",
      " 'coolidge_speeches_009.txt' 'coolidge_speeches_003.txt'\n",
      " 'coolidge_speeches_005.txt']\n",
      "Validation set:\n",
      "['coolidge_speeches_010.txt' 'coolidge_speeches_001.txt'\n",
      " 'coolidge_speeches_004.txt' 'coolidge_speeches_011.txt']\n",
      "Test set:\n",
      "['coolidge_speeches_002.txt' 'coolidge_speeches_000.txt'\n",
      " 'coolidge_speeches_006.txt']\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['roosevelt_speeches_013.txt' 'roosevelt_speeches_002.txt'\n",
      " 'roosevelt_speeches_011.txt' 'roosevelt_speeches_015.txt'\n",
      " 'roosevelt_speeches_004.txt' 'roosevelt_speeches_017.txt'\n",
      " 'roosevelt_speeches_020.txt' 'roosevelt_speeches_019.txt'\n",
      " 'roosevelt_speeches_005.txt']\n",
      "Validation set:\n",
      "['roosevelt_speeches_006.txt' 'roosevelt_speeches_012.txt'\n",
      " 'roosevelt_speeches_008.txt' 'roosevelt_speeches_000.txt'\n",
      " 'roosevelt_speeches_014.txt' 'roosevelt_speeches_018.txt']\n",
      "Test set:\n",
      "['roosevelt_speeches_010.txt' 'roosevelt_speeches_007.txt'\n",
      " 'roosevelt_speeches_001.txt' 'roosevelt_speeches_009.txt'\n",
      " 'roosevelt_speeches_003.txt' 'roosevelt_speeches_016.txt'\n",
      " 'roosevelt_speeches_021.txt']\n",
      "Done\n",
      "Training set:\n",
      "['fillmore_speeches_002.txt' 'fillmore_speeches_001.txt'\n",
      " 'fillmore_speeches_006.txt']\n",
      "Validation set:\n",
      "['fillmore_speeches_003.txt' 'fillmore_speeches_000.txt']\n",
      "Test set:\n",
      "['fillmore_speeches_004.txt' 'fillmore_speeches_005.txt']\n",
      "Done\n",
      "Training set:\n",
      "['johnson_speeches_013.txt' 'johnson_speeches_026.txt'\n",
      " 'johnson_speeches_001.txt' 'johnson_speeches_028.txt'\n",
      " 'johnson_speeches_017.txt' 'johnson_speeches_023.txt'\n",
      " 'johnson_speeches_006.txt' 'johnson_speeches_022.txt'\n",
      " 'johnson_speeches_019.txt' 'johnson_speeches_014.txt'\n",
      " 'johnson_speeches_020.txt' 'johnson_speeches_027.txt']\n",
      "Validation set:\n",
      "['johnson_speeches_024.txt' 'johnson_speeches_003.txt'\n",
      " 'johnson_speeches_012.txt' 'johnson_speeches_005.txt'\n",
      " 'johnson_speeches_030.txt' 'johnson_speeches_010.txt'\n",
      " 'johnson_speeches_018.txt' 'johnson_speeches_008.txt'\n",
      " 'johnson_speeches_002.txt' 'johnson_speeches_000.txt']\n",
      "Test set:\n",
      "['johnson_speeches_009.txt' 'johnson_speeches_021.txt'\n",
      " 'johnson_speeches_029.txt' 'johnson_speeches_015.txt'\n",
      " 'johnson_speeches_025.txt' 'johnson_speeches_016.txt'\n",
      " 'johnson_speeches_004.txt' 'johnson_speeches_007.txt'\n",
      " 'johnson_speeches_011.txt']\n",
      "Done\n",
      "Training set:\n",
      "[]\n",
      "Validation set:\n",
      "[]\n",
      "Test set:\n",
      "['harrison_speeches_000.txt']\n",
      "Done\n",
      "Training set:\n",
      "['taft_speeches_000.txt' 'taft_speeches_009.txt' 'taft_speeches_008.txt'\n",
      " 'taft_speeches_002.txt']\n",
      "Validation set:\n",
      "['taft_speeches_005.txt' 'taft_speeches_004.txt' 'taft_speeches_001.txt'\n",
      " 'taft_speeches_010.txt']\n",
      "Test set:\n",
      "['taft_speeches_006.txt' 'taft_speeches_007.txt' 'taft_speeches_003.txt']\n",
      "Done\n",
      "Training set:\n",
      "['lbjohnson_speeches_051.txt' 'lbjohnson_speeches_032.txt'\n",
      " 'lbjohnson_speeches_021.txt' 'lbjohnson_speeches_061.txt'\n",
      " 'lbjohnson_speeches_034.txt' 'lbjohnson_speeches_049.txt'\n",
      " 'lbjohnson_speeches_026.txt' 'lbjohnson_speeches_059.txt'\n",
      " 'lbjohnson_speeches_065.txt' 'lbjohnson_speeches_045.txt'\n",
      " 'lbjohnson_speeches_008.txt' 'lbjohnson_speeches_011.txt'\n",
      " 'lbjohnson_speeches_015.txt' 'lbjohnson_speeches_001.txt'\n",
      " 'lbjohnson_speeches_067.txt' 'lbjohnson_speeches_069.txt'\n",
      " 'lbjohnson_speeches_028.txt' 'lbjohnson_speeches_042.txt'\n",
      " 'lbjohnson_speeches_058.txt' 'lbjohnson_speeches_036.txt'\n",
      " 'lbjohnson_speeches_052.txt' 'lbjohnson_speeches_017.txt'\n",
      " 'lbjohnson_speeches_053.txt' 'lbjohnson_speeches_005.txt'\n",
      " 'lbjohnson_speeches_031.txt' 'lbjohnson_speeches_009.txt'\n",
      " 'lbjohnson_speeches_063.txt' 'lbjohnson_speeches_022.txt']\n",
      "Validation set:\n",
      "['lbjohnson_speeches_014.txt' 'lbjohnson_speeches_020.txt'\n",
      " 'lbjohnson_speeches_057.txt' 'lbjohnson_speeches_000.txt'\n",
      " 'lbjohnson_speeches_006.txt' 'lbjohnson_speeches_056.txt'\n",
      " 'lbjohnson_speeches_039.txt' 'lbjohnson_speeches_019.txt'\n",
      " 'lbjohnson_speeches_029.txt' 'lbjohnson_speeches_010.txt'\n",
      " 'lbjohnson_speeches_055.txt' 'lbjohnson_speeches_012.txt'\n",
      " 'lbjohnson_speeches_040.txt' 'lbjohnson_speeches_003.txt'\n",
      " 'lbjohnson_speeches_004.txt' 'lbjohnson_speeches_050.txt'\n",
      " 'lbjohnson_speeches_030.txt' 'lbjohnson_speeches_054.txt'\n",
      " 'lbjohnson_speeches_048.txt' 'lbjohnson_speeches_013.txt'\n",
      " 'lbjohnson_speeches_060.txt' 'lbjohnson_speeches_064.txt']\n",
      "Test set:\n",
      "['lbjohnson_speeches_046.txt' 'lbjohnson_speeches_068.txt'\n",
      " 'lbjohnson_speeches_066.txt' 'lbjohnson_speeches_016.txt'\n",
      " 'lbjohnson_speeches_062.txt' 'lbjohnson_speeches_002.txt'\n",
      " 'lbjohnson_speeches_070.txt' 'lbjohnson_speeches_023.txt'\n",
      " 'lbjohnson_speeches_047.txt' 'lbjohnson_speeches_043.txt'\n",
      " 'lbjohnson_speeches_037.txt' 'lbjohnson_speeches_025.txt'\n",
      " 'lbjohnson_speeches_024.txt' 'lbjohnson_speeches_033.txt'\n",
      " 'lbjohnson_speeches_027.txt' 'lbjohnson_speeches_007.txt'\n",
      " 'lbjohnson_speeches_038.txt' 'lbjohnson_speeches_018.txt'\n",
      " 'lbjohnson_speeches_044.txt' 'lbjohnson_speeches_041.txt'\n",
      " 'lbjohnson_speeches_035.txt']\n",
      "Done\n",
      "Training set:\n",
      "['jqadams_speeches_003.txt' 'jqadams_speeches_004.txt'\n",
      " 'jqadams_speeches_005.txt']\n",
      "Validation set:\n",
      "['jqadams_speeches_006.txt' 'jqadams_speeches_000.txt']\n",
      "Test set:\n",
      "['jqadams_speeches_007.txt' 'jqadams_speeches_001.txt'\n",
      " 'jqadams_speeches_002.txt']\n",
      "Done\n",
      "Training set:\n",
      "['tyler_speeches_003.txt' 'tyler_speeches_001.txt'\n",
      " 'tyler_speeches_010.txt' 'tyler_speeches_008.txt'\n",
      " 'tyler_speeches_006.txt' 'tyler_speeches_015.txt'\n",
      " 'tyler_speeches_007.txt']\n",
      "Validation set:\n",
      "['tyler_speeches_017.txt' 'tyler_speeches_014.txt'\n",
      " 'tyler_speeches_002.txt' 'tyler_speeches_004.txt'\n",
      " 'tyler_speeches_013.txt' 'tyler_speeches_000.txt']\n",
      "Test set:\n",
      "['tyler_speeches_016.txt' 'tyler_speeches_009.txt'\n",
      " 'tyler_speeches_012.txt' 'tyler_speeches_011.txt'\n",
      " 'tyler_speeches_005.txt']\n",
      "Done\n",
      "Training set:\n",
      "['clinton_speeches_017.txt' 'clinton_speeches_023.txt'\n",
      " 'clinton_speeches_007.txt' 'clinton_speeches_034.txt'\n",
      " 'clinton_speeches_002.txt' 'clinton_speeches_029.txt'\n",
      " 'clinton_speeches_018.txt' 'clinton_speeches_003.txt'\n",
      " 'clinton_speeches_037.txt' 'clinton_speeches_020.txt'\n",
      " 'clinton_speeches_014.txt' 'clinton_speeches_026.txt'\n",
      " 'clinton_speeches_033.txt' 'clinton_speeches_010.txt'\n",
      " 'clinton_speeches_016.txt' 'clinton_speeches_004.txt']\n",
      "Validation set:\n",
      "['clinton_speeches_021.txt' 'clinton_speeches_012.txt'\n",
      " 'clinton_speeches_030.txt' 'clinton_speeches_032.txt'\n",
      " 'clinton_speeches_028.txt' 'clinton_speeches_025.txt'\n",
      " 'clinton_speeches_005.txt' 'clinton_speeches_038.txt'\n",
      " 'clinton_speeches_022.txt' 'clinton_speeches_027.txt'\n",
      " 'clinton_speeches_015.txt' 'clinton_speeches_008.txt']\n",
      "Test set:\n",
      "['clinton_speeches_024.txt' 'clinton_speeches_036.txt'\n",
      " 'clinton_speeches_013.txt' 'clinton_speeches_000.txt'\n",
      " 'clinton_speeches_009.txt' 'clinton_speeches_019.txt'\n",
      " 'clinton_speeches_001.txt' 'clinton_speeches_006.txt'\n",
      " 'clinton_speeches_035.txt' 'clinton_speeches_031.txt'\n",
      " 'clinton_speeches_011.txt']\n",
      "Done\n",
      "Training set:\n",
      "['kennedy_speeches_034.txt' 'kennedy_speeches_022.txt'\n",
      " 'kennedy_speeches_019.txt' 'kennedy_speeches_039.txt'\n",
      " 'kennedy_speeches_028.txt' 'kennedy_speeches_029.txt'\n",
      " 'kennedy_speeches_015.txt' 'kennedy_speeches_013.txt'\n",
      " 'kennedy_speeches_010.txt' 'kennedy_speeches_026.txt'\n",
      " 'kennedy_speeches_006.txt' 'kennedy_speeches_035.txt'\n",
      " 'kennedy_speeches_032.txt' 'kennedy_speeches_021.txt'\n",
      " 'kennedy_speeches_038.txt' 'kennedy_speeches_007.txt'\n",
      " 'kennedy_speeches_044.txt' 'kennedy_speeches_042.txt']\n",
      "Validation set:\n",
      "['kennedy_speeches_003.txt' 'kennedy_speeches_024.txt'\n",
      " 'kennedy_speeches_020.txt' 'kennedy_speeches_017.txt'\n",
      " 'kennedy_speeches_001.txt' 'kennedy_speeches_023.txt'\n",
      " 'kennedy_speeches_016.txt' 'kennedy_speeches_037.txt'\n",
      " 'kennedy_speeches_030.txt' 'kennedy_speeches_043.txt'\n",
      " 'kennedy_speeches_018.txt' 'kennedy_speeches_009.txt'\n",
      " 'kennedy_speeches_014.txt' 'kennedy_speeches_025.txt']\n",
      "Test set:\n",
      "['kennedy_speeches_008.txt' 'kennedy_speeches_005.txt'\n",
      " 'kennedy_speeches_011.txt' 'kennedy_speeches_002.txt'\n",
      " 'kennedy_speeches_031.txt' 'kennedy_speeches_036.txt'\n",
      " 'kennedy_speeches_041.txt' 'kennedy_speeches_027.txt'\n",
      " 'kennedy_speeches_040.txt' 'kennedy_speeches_004.txt'\n",
      " 'kennedy_speeches_033.txt' 'kennedy_speeches_000.txt'\n",
      " 'kennedy_speeches_012.txt']\n",
      "Done\n",
      "Training set:\n",
      "['eisenhower_speeches_000.txt' 'eisenhower_speeches_004.txt']\n",
      "Validation set:\n",
      "['eisenhower_speeches_002.txt' 'eisenhower_speeches_005.txt']\n",
      "Test set:\n",
      "['eisenhower_speeches_003.txt' 'eisenhower_speeches_001.txt']\n",
      "Done\n",
      "Training set:\n",
      "['nixon_speeches_010.txt' 'nixon_speeches_020.txt'\n",
      " 'nixon_speeches_016.txt' 'nixon_speeches_004.txt'\n",
      " 'nixon_speeches_014.txt' 'nixon_speeches_011.txt'\n",
      " 'nixon_speeches_008.txt' 'nixon_speeches_022.txt'\n",
      " 'nixon_speeches_019.txt']\n",
      "Validation set:\n",
      "['nixon_speeches_007.txt' 'nixon_speeches_018.txt'\n",
      " 'nixon_speeches_006.txt' 'nixon_speeches_009.txt'\n",
      " 'nixon_speeches_013.txt' 'nixon_speeches_005.txt'\n",
      " 'nixon_speeches_001.txt']\n",
      "Test set:\n",
      "['nixon_speeches_021.txt' 'nixon_speeches_002.txt'\n",
      " 'nixon_speeches_000.txt' 'nixon_speeches_003.txt'\n",
      " 'nixon_speeches_017.txt' 'nixon_speeches_015.txt'\n",
      " 'nixon_speeches_012.txt']\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['arthur_speeches_010.txt' 'arthur_speeches_001.txt'\n",
      " 'arthur_speeches_006.txt' 'arthur_speeches_009.txt']\n",
      "Validation set:\n",
      "['arthur_speeches_007.txt' 'arthur_speeches_008.txt'\n",
      " 'arthur_speeches_003.txt' 'arthur_speeches_000.txt']\n",
      "Test set:\n",
      "['arthur_speeches_004.txt' 'arthur_speeches_002.txt'\n",
      " 'arthur_speeches_005.txt']\n",
      "Done\n",
      "Training set:\n",
      "['grant_speeches_030.txt' 'grant_speeches_006.txt'\n",
      " 'grant_speeches_001.txt' 'grant_speeches_024.txt'\n",
      " 'grant_speeches_018.txt' 'grant_speeches_019.txt'\n",
      " 'grant_speeches_026.txt' 'grant_speeches_022.txt'\n",
      " 'grant_speeches_000.txt' 'grant_speeches_020.txt'\n",
      " 'grant_speeches_009.txt' 'grant_speeches_015.txt'\n",
      " 'grant_speeches_007.txt']\n",
      "Validation set:\n",
      "['grant_speeches_027.txt' 'grant_speeches_016.txt'\n",
      " 'grant_speeches_014.txt' 'grant_speeches_031.txt'\n",
      " 'grant_speeches_013.txt' 'grant_speeches_002.txt'\n",
      " 'grant_speeches_011.txt' 'grant_speeches_029.txt'\n",
      " 'grant_speeches_005.txt' 'grant_speeches_012.txt']\n",
      "Test set:\n",
      "['grant_speeches_008.txt' 'grant_speeches_003.txt'\n",
      " 'grant_speeches_023.txt' 'grant_speeches_021.txt'\n",
      " 'grant_speeches_025.txt' 'grant_speeches_004.txt'\n",
      " 'grant_speeches_010.txt' 'grant_speeches_017.txt'\n",
      " 'grant_speeches_028.txt']\n",
      "Done\n",
      "Training set:\n",
      "['buchanan_speeches_007.txt' 'buchanan_speeches_010.txt'\n",
      " 'buchanan_speeches_009.txt' 'buchanan_speeches_003.txt'\n",
      " 'buchanan_speeches_012.txt' 'buchanan_speeches_013.txt']\n",
      "Validation set:\n",
      "['buchanan_speeches_000.txt' 'buchanan_speeches_004.txt'\n",
      " 'buchanan_speeches_011.txt' 'buchanan_speeches_006.txt']\n",
      "Test set:\n",
      "['buchanan_speeches_005.txt' 'buchanan_speeches_002.txt'\n",
      " 'buchanan_speeches_008.txt' 'buchanan_speeches_001.txt']\n",
      "Done\n",
      "Training set:\n",
      "['bharrison_speeches_014.txt' 'bharrison_speeches_002.txt'\n",
      " 'bharrison_speeches_004.txt' 'bharrison_speeches_007.txt'\n",
      " 'bharrison_speeches_013.txt' 'bharrison_speeches_000.txt']\n",
      "Validation set:\n",
      "['bharrison_speeches_005.txt' 'bharrison_speeches_010.txt'\n",
      " 'bharrison_speeches_001.txt' 'bharrison_speeches_009.txt'\n",
      " 'bharrison_speeches_015.txt']\n",
      "Test set:\n",
      "['bharrison_speeches_011.txt' 'bharrison_speeches_003.txt'\n",
      " 'bharrison_speeches_012.txt' 'bharrison_speeches_008.txt'\n",
      " 'bharrison_speeches_006.txt']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Select a president to build models on\n",
    "pres = ['obama']\n",
    "all_pres = list(os.walk(f\"CorpusOfPresidentialSpeeches\"))[0][1]\n",
    "\n",
    "# split_pct = [training_pct, validation_pct, test_pct]\n",
    "split_pct = [.4, .3, .3]\n",
    "\n",
    "# Set sed number\n",
    "np.random.seed(266)\n",
    "\n",
    "# Get current directory\n",
    "cwd = getcwd()\n",
    "\n",
    "for pre in all_pres:\n",
    "    \n",
    "    dir_output = f\"1.DataPreparationResults/{pre}\"\n",
    "    \n",
    "    if not os.path.exists(dir_output):\n",
    "        os.makedirs(dir_output)\n",
    "    \n",
    "    out_train = open(f\"{dir_output}/train.txt\",\"w+\")\n",
    "    out_val = open(f\"{dir_output}/val.txt\",\"w+\")\n",
    "    out_test = open(f\"{dir_output}/test.txt\",\"w+\")\n",
    "    dir_president = f\"CorpusOfPresidentialSpeeches/{pre}\"\n",
    "    \n",
    "    # onlyfiles contains a list of files (not directories) under path_president\n",
    "    # Reference: https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "    onlyfiles_lst = [f for f in listdir(dir_president) if isfile(join(dir_president, f))]\n",
    "    num_of_files = len(onlyfiles_lst)\n",
    "\n",
    "    # Reference: https://stackoverflow.com/questions/15511349/select-50-items-from-list-at-random-to-write-to-file/39585770\n",
    "    files_train_arr = np.random.choice(onlyfiles_lst, round(num_of_files*split_pct[0]), replace=False)\n",
    "\n",
    "    # Set substraction: https://stackoverflow.com/questions/3428536/python-list-subtraction-operation\n",
    "    files_val_test_lst = list(set(onlyfiles_lst) - set(files_train_arr))\n",
    "    files_val_arr = np.random.choice(files_val_test_lst, round(len(files_val_test_lst)*split_pct[1]/(split_pct[1]+split_pct[2])), replace=False)\n",
    "    files_test_arr = np.array(list((set(files_val_test_lst) - set(files_val_arr))))\n",
    "    \n",
    "    for root, dirs, files in os.walk(dir_president, topdown=False):\n",
    "        for file in files:\n",
    "            path = f\"{root}/{file}\"\n",
    "            out_text = tokenize_words(path)\n",
    "            \n",
    "            if file in files_train_arr:\n",
    "                out_train.write(out_text)\n",
    "            elif file in files_val_arr:\n",
    "                out_val.write(out_text)\n",
    "            elif file in files_test_arr:\n",
    "                out_test.write(out_text)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training set:')\n",
    "    print(files_train_arr)\n",
    "    print('Validation set:')\n",
    "    print(files_val_arr)\n",
    "    print('Test set:')\n",
    "    print(files_test_arr)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I stand here today humbled by the task before us , grateful for the trust you have bestowed , mindful of the sacrifices borne by our ancestors . I thank President Bush for his service to our nation , as well as the generosity and cooperation he has shown throughout this transition . Forty four Americans have now taken the presidential oath . The words have been spoken during rising tides of prosperity and the still waters of peace . Yet , every so often the oath is taken amidst gathering clouds and raging storms . At these moments , America has carried on not simply because of the skill or vision of those in high office , but because We the People have remained faithful to the ideals of our forbearers , and true to our founding documents . So it has been . So it must be with this generation of Americans . That we are in the midst of crisis is now well understood . Our nation is at war , against a far reaching network of violence and hatred . Our economy is badly weakened , a consequence of greed and irresponsibility on the part of some , but also our collective failure to make hard choices and prepare the nation for a new age . Homes have been lost jobs shed businesses shuttered . Our health care is too costly our schools fail too many and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet . These are the indicators of crisis , subject to data and statistics . Less measurable but no less profound is a sapping of confidence across our land a nagging fear that America s decline is inevitable , and that the next generation must lower its sights . Today I say to you that the challenges we face are real . They are serious and they are many . They will not be met easily or in a short span of time . But know this , America they will be met . On this day , we gather because we have chosen hope over fear , unity of purpose over conflict and discord . On this day , we come to proclaim an end to the petty grievances and false promises , the recriminations and worn out dogmas , that for far too long have strangled our politics . We remain a young nation , but in the words of Scripture , the time has come to set aside childish things . The time has come to reaffirm our enduring spirit to choose our better history to carry forward that precious gift , that noble idea , passed on from generation to generation the God given promise that all are equal , all are free , and all deserve a chance to pursue their full measure of happiness . In reaffirming the greatness of our nation , we understand that greatness is never a given . It must be earned . Our journey has never been one of short cuts or settling for less . It has not been the path for the faint hearted for those who prefer leisure over work , or seek only the pleasures of riches and fame . Rather , it has been the risk takers , the doers , the makers of things some celebrated but more often men and women obscure in their labor , who have carried us up the long , rugged path towards prosperity and freedom . For us , they packed up their few worldly possessions and traveled across oceans in search of a new life . For us , they toiled in sweatshops and settled the West endured the lash of the whip and plowed the hard earth . For us , they fought and died , in places like Concord and Gettysburg Normandy and Khe Sahn . Time and again these men and women struggled and sacrificed and worked till their hands were raw so that we might live a better life . They saw America as bigger than the sum of our individual ambitions greater than all the differences of birth or wealth or faction . This is the journey we continue today . We remain the most prosperous , powerful nation on Earth . Our workers are no less productive than when this crisis began . Our minds are no less inventive , our goods and services no less needed than they were last week or last month or last year . Our capacity remains undiminished . But our time of standing pat , of protecting narrow interests and putting off unpleasant decisions that time has surely passed . Starting today , we must pick ourselves up , dust ourselves off , and begin again the work of remaking America . For everywhere we look , there is work to be done . The state of the economy calls for action , bold and swift , and we will act not only to create new jobs , but to lay a new foundation for growth . We will build the roads and bridges , the electric grids and digital lines that feed our commerce and bind us together . We will restore science to its rightful place , and wield technology s wonders to raise health care s quality and lower its cost . We will harness the sun and the winds and the soil to fuel our cars and run our factories . And we will transform our schools and colleges and universities to meet the demands of a new age . All this we can do . And all this we will do . Now , there are some who question the scale of our ambitions who suggest that our system cannot tolerate too many big plans . Their memories are short . For they have forgotten what this country has already done what free men and women can achieve when imagination is joined to common purpose , and necessity to courage . What the cynics fail to understand is that the ground has shifted beneath them that the stale political arguments that have consumed us for so long no longer apply . The question we ask today is not whether our government is too big or too small , but whether it works whether it helps families find jobs at a decent wage , care they can afford , a retirement that is dignified . Where the answer is yes , we intend to move forward . Where the answer is no , programs will end . And those of us who manage the public s dollars will be held to account to spend wisely , reform bad habits , and do our business in the light of day because only then can we restore the vital trust between a people and their government . Nor is the question before us whether the market is a force for good or ill . Its power to generate wealth and expand freedom is unmatched , but this crisis has reminded us that without a watchful eye , the market can spin out of control and that a nation cannot prosper long when it favors only the prosperous . The success of our economy has always depended not just on the size of our Gross Domestic Product , but on the reach of our prosperity on our ability to extend opportunity to every willing heart not out of charity , but because it is the surest route to our common good . As for our common defense , we reject as false the choice between our safety and our ideals . Our Founding Fathers , faced with perils we can scarcely imagine , drafted a charter to assure the rule of law and the rights of man , a charter expanded by the blood of generations . Those ideals still light the world , and we will not give them up for expedience s sake . And so to all other peoples and governments who are watching today , from the grandest capitals to the small village where my father was born know that America is a friend of each nation and every man , woman , and child who seeks a future of peace and dignity , and that we are ready to lead once more . Recall that earlier generations faced down fascism and communism not just with missiles and tanks , but with sturdy alliances and enduring convictions . They understood that our power alone cannot protect us , nor does it entitle us to do as we please . Instead , they knew that our power grows through its prudent use our security emanates from the justness of our cause , the force of our example , the tempering qualities of humility and restraint . We are the keepers of this legacy . Guided by these principles once more , we can meet those new threats that demand even greater effort even greater cooperation and understanding between nations . We will begin to responsibly leave Iraq to its people , and forge a hard earned peace in Afghanistan . With old friends and former foes , we will work tirelessly to lessen the nuclear threat , and roll back the specter of a warming planet . We will not apologize for our way of life , nor will we waver in its defense , and for those who seek to advance their aims by inducing terror and slaughtering innocents , we say to you now that our spirit is stronger and cannot be broken you cannot outlast us , and we will defeat you . For we know that our patchwork heritage is a strength , not a weakness . We are a nation of Christians and Muslims , Jews and Hindus and non believers . We are shaped by every language and culture , drawn from every end of this Earth and because we have tasted the bitter swill of civil war and segregation , and emerged from that dark chapter stronger and more united , we cannot help but believe that the old hatreds shall someday pass that the lines of tribe shall soon dissolve that as the world grows smaller , our common humanity shall reveal itself and that America must play its role in ushering in a new era of peace . To the Muslim world , we seek a new way forward , based on mutual interest and mutual respect . To those leaders around the globe who seek to sow conflict , or blame their society s ills on the West , know that your people will judge you on what you can build , not what you destroy . To those who cling to power through corruption and deceit and the silencing of dissent , know that you are on the wrong side of history but that we will extend a hand if you are willing to unclench your fist . To the people of poor nations , we pledge to work alongside you to make your farms flourish and let clean waters flow to nourish starved bodies and feed hungry minds . And to those nations like ours that enjoy relative plenty , we say we can no longer afford indifference to suffering outside our borders nor can we consume the world s resources without regard to effect . For the world has changed , and we must change with it . As we consider the road that unfolds before us , we remember with humble gratitude those brave Americans who , at this very hour , patrol far off deserts and distant mountains . They have something to tell us today , just as the fallen heroes who lie in Arlington whisper through the ages . We honor them not only because they are guardians of our liberty , but because they embody the spirit of service a willingness to find meaning in something greater than themselves . And yet , at this moment a moment that will define a generation it is precisely this spirit that must inhabit us all . For as much as government can do and must do , it is ultimately the faith and determination of the American people upon which this nation relies . It is the kindness to take in a stranger when the levees break , the selflessness of workers who would rather cut their hours than see a friend lose their job which sees us through our darkest hours . It is the firefighter s courage to storm a stairway filled with smoke , but also a parent s willingness to nurture a child , that finally decides our fate . Our challenges may be new . The instruments with which we meet them may be new . But those values upon which our success depends hard work and honesty , courage and fair play , tolerance and curiosity , loyalty and patriotism these things are old . These things are true . They have been the quiet force of progress throughout our history . What is demanded then is a return to these truths . What is required of us now is a new era of responsibility a recognition , on the part of every American , that we have duties to ourselves , our nation , and the world , duties that we do not grudgingly accept but rather seize gladly , firm in the knowledge that there is nothing so satisfying to the spirit , so defining of our character , than giving our all to a difficult task . This is the price and the promise of citizenship . This is the source of our confidence the knowledge that God calls on us to shape an uncertain destiny . This is the meaning of our liberty and our creed why men and women and children of every race and every faith can join in celebration across this magnificent mall , and why a man whose father less than sixty years ago might not have been served at a local restaurant can now stand before you to take a most sacred oath . So let us mark this day with remembrance , of who we are and how far we have traveled . In the year of America s birth , in the coldest of months , a small band of patriots huddled by dying campfires on the shores of an icy river . The capital was abandoned . The enemy was advancing . The snow was stained with blood . At a moment when the outcome of our revolution was most in doubt , the father of our nation ordered these words be read to the people Let it be told to the future world . . . that in the depth of winter , when nothing but hope and virtue could survive . . . that the city and the country , alarmed at one common danger , came forth to meet . America . In the face of our common dangers , in this winter of our hardship , let us remember these timeless words . With hope and virtue , let us brave once more the icy currents , and endure what storms may come . Let it be said by our children s children that when we were tested we refused to let this journey end , that we did not turn back nor did we falter and with eyes fixed on the horizon and God s grace upon us , we carried forth that great gift of freedom and delivered it safely to future generations . Thank you . God bless you . And may God bless the United States of America .<speech_sep>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_words(\"CorpusOfPresidentialSpeeches/obama/obama_speeches_001.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible that digits in the validation/test sets are not training set\n",
    "# To make sure every character/digit can be converted to a number \n",
    "#     and subsequently scored appropriately for validation/test sets,\n",
    "# We define chars_lst as all possible characters/digits we can observe from training/validaiton/test sets\n",
    "# The code below only captures characters/digits in the training set and thus inappropriate\n",
    "#     chars_lst = sorted(list(set(tokenized_file)))\n",
    "# Reference: https://stackoverflow.com/questions/16060899/alphabet-range-on-python\n",
    "\n",
    "chars_lst = [' ',',','.'] + [str(i) for i in range(10)] + [chr(i) for i in range(ord('a'),ord('z')+1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
