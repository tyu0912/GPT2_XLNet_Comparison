{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts here are used for tokenization and preparing the train/dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, getcwd\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are titles, punctuations, and contractions that need to be replaced.\n",
    "\n",
    "replacing_titles = ['Dr.','Esq.','Hon.','Jr.','Mr.','Mrs.','Ms.','Messrs.','Mmes.','Msgr.','Prof.','Rev.','Rt. Hon.','Sr.','St.']\n",
    "punctuation_marks = ['\"', \"'\", \"!\", \"@\", \"#\", \"$\", \"%\", \"&\", \"*\", \"(\", \")\", \"-\", \"?\", \",\", \".\"]\n",
    "contractions_dict = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he had\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I had\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"iit will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she had\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that had\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they had\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "country_acronyms = {\"U.S\": \"United States\", \"U.S.A\": \"United States of America\", \"U.A.E\": \"United Arab Emirates\", \"U.S.S.R\": \"Union of Soviet Socialist Republics\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input_file):\n",
    "    \"\"\"\n",
    "    This function accomplishes four purposes:\n",
    "    1. Remove the title and date (the first two rows) from the input file\n",
    "    2. Remove all special characters except for . and ,\n",
    "    3. Convert all characters to lower case\n",
    "    4. Tokenize words\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): input file\n",
    "        \n",
    "    Returns:\n",
    "        output_file (str): tokenized strings separated by space\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    input_file = open(input_file, 'r').read()\n",
    "    \n",
    "    # Remove the title and date (the first two rows)\n",
    "    \n",
    "    startChar = [word.end() for word in re.finditer(\"\\n\", input_file)][1]\n",
    "    input2 = input_file[startChar:]\n",
    "    \n",
    "    # Remove things in angle quotes which are added to account for crowd reactions\n",
    "    input2 = re.sub(r\"\\<[^\\>]*\\>\", '', input2)\n",
    "    \n",
    "    # lowercase everything so that we have less tokens to predict. i.e., no need to distinguish a vs. A\n",
    "    #input2 = input2.lower()\n",
    "    \n",
    "    # Standardize contractions\n",
    "    for k, v in contractions_dict.items():\n",
    "        input2 = input2.replace(k, v) \n",
    "        \n",
    "        k_caps = k[:1].upper() + k[1:]\n",
    "        v_caps = v[:1].upper() + v[1:]\n",
    "        \n",
    "        input2 = input2.replace(k_caps, v_caps)\n",
    "        \n",
    "    # Replace country acronyms\n",
    "    for k, v in country_acronyms.items():\n",
    "        input2 = input2.replace(k, v)\n",
    "        \n",
    "    # Remove middle initial\n",
    "    input2 = re.sub(r\"([A-Z])\\W \", '', input2)\n",
    "    \n",
    "    # Keep all the words and digitis\n",
    "    # Keep only two special characters: . and ,\n",
    "    # If we want to keep carriage return, add |\\n\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[\\.\\,]')\n",
    "    tokens = tokenizer.tokenize(input2)\n",
    "    output_file = \" \".join(tokens)\n",
    "    output_file = output_file + \"<speech_sep>\"\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['obama_speeches_028.txt' 'obama_speeches_015.txt'\n",
      " 'obama_speeches_018.txt' 'obama_speeches_045.txt'\n",
      " 'obama_speeches_035.txt' 'obama_speeches_000.txt'\n",
      " 'obama_speeches_041.txt' 'obama_speeches_039.txt'\n",
      " 'obama_speeches_019.txt' 'obama_speeches_001.txt'\n",
      " 'obama_speeches_003.txt' 'obama_speeches_032.txt'\n",
      " 'obama_speeches_040.txt' 'obama_speeches_029.txt'\n",
      " 'obama_speeches_002.txt' 'obama_speeches_014.txt'\n",
      " 'obama_speeches_005.txt' 'obama_speeches_033.txt'\n",
      " 'obama_speeches_016.txt']\n",
      "Validation set:\n",
      "['obama_speeches_004.txt' 'obama_speeches_031.txt'\n",
      " 'obama_speeches_027.txt' 'obama_speeches_043.txt'\n",
      " 'obama_speeches_047.txt' 'obama_speeches_017.txt'\n",
      " 'obama_speeches_037.txt' 'obama_speeches_011.txt'\n",
      " 'obama_speeches_007.txt' 'obama_speeches_022.txt'\n",
      " 'obama_speeches_042.txt' 'obama_speeches_044.txt'\n",
      " 'obama_speeches_034.txt' 'obama_speeches_048.txt'\n",
      " 'obama_speeches_021.txt' 'obama_speeches_049.txt'\n",
      " 'obama_speeches_010.txt']\n",
      "Test set:\n",
      "['obama_speeches_046.txt' 'obama_speeches_026.txt'\n",
      " 'obama_speeches_009.txt' 'obama_speeches_012.txt'\n",
      " 'obama_speeches_030.txt' 'obama_speeches_006.txt'\n",
      " 'obama_speeches_008.txt' 'obama_speeches_036.txt'\n",
      " 'obama_speeches_023.txt' 'obama_speeches_013.txt'\n",
      " 'obama_speeches_038.txt' 'obama_speeches_020.txt']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Select a president to build models on\n",
    "pres = ['obama']\n",
    "\n",
    "# split_pct = [training_pct, validation_pct, test_pct]\n",
    "split_pct = [.4, .4, .3]\n",
    "\n",
    "# Set sed number\n",
    "np.random.seed(266)\n",
    "\n",
    "# Get current directory\n",
    "cwd = getcwd()\n",
    "\n",
    "for pre in pres:\n",
    "    \n",
    "    dir_output = f\"1.DataPreparationResults/{pre}\"\n",
    "    \n",
    "    if not os.path.exists(dir_output):\n",
    "        os.makedirs(dir_output)\n",
    "    \n",
    "    out_train = open(f\"{dir_output}/train.txt\",\"w+\")\n",
    "    out_val = open(f\"{dir_output}/val.txt\",\"w+\")\n",
    "    out_test = open(f\"{dir_output}/test.txt\",\"w+\")\n",
    "    dir_president = f\"CorpusOfPresidentialSpeeches/{pre}\"\n",
    "    \n",
    "    # onlyfiles contains a list of files (not directories) under path_president\n",
    "    # Reference: https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "    onlyfiles_lst = [f for f in listdir(dir_president) if isfile(join(dir_president, f))]\n",
    "    num_of_files = len(onlyfiles_lst)\n",
    "\n",
    "    # Reference: https://stackoverflow.com/questions/15511349/select-50-items-from-list-at-random-to-write-to-file/39585770\n",
    "    files_train_arr = np.random.choice(onlyfiles_lst, round(num_of_files*split_pct[0]), replace=False)\n",
    "\n",
    "    # Set substraction: https://stackoverflow.com/questions/3428536/python-list-subtraction-operation\n",
    "    files_val_test_lst = list(set(onlyfiles_lst) - set(files_train_arr))\n",
    "    files_val_arr = np.random.choice(files_val_test_lst, round(len(files_val_test_lst)*split_pct[1]/(split_pct[1]+split_pct[2])), replace=False)\n",
    "    files_test_arr = np.array(list((set(files_val_test_lst) - set(files_val_arr))))\n",
    "    \n",
    "    for root, dirs, files in os.walk(dir_president, topdown=False):\n",
    "        for file in files:\n",
    "            path = f\"{root}/{file}\"\n",
    "            out_text = tokenize_words(path)\n",
    "            \n",
    "            if file in files_train_arr:\n",
    "                out_train.write(out_text)\n",
    "            elif file in files_val_arr:\n",
    "                out_val.write(out_text)\n",
    "            elif file in files_test_arr:\n",
    "                out_test.write(out_text)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training set:')\n",
    "    print(files_train_arr)\n",
    "    print('Validation set:')\n",
    "    print(files_val_arr)\n",
    "    print('Test set:')\n",
    "    print(files_test_arr)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words(\"CorpusOfPresidentialSpeeches/obama/obama_speeches_000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible that digits in the validation/test sets are not training set\n",
    "# To make sure every character/digit can be converted to a number \n",
    "#     and subsequently scored appropriately for validation/test sets,\n",
    "# We define chars_lst as all possible characters/digits we can observe from training/validaiton/test sets\n",
    "# The code below only captures characters/digits in the training set and thus inappropriate\n",
    "#     chars_lst = sorted(list(set(tokenized_file)))\n",
    "# Reference: https://stackoverflow.com/questions/16060899/alphabet-range-on-python\n",
    "\n",
    "chars_lst = [' ',',','.'] + [str(i) for i in range(10)] + [chr(i) for i in range(ord('a'),ord('z')+1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
