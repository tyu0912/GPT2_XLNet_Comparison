{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "# from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "# from keras.utils import np_utils\n",
    "# from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will not remove stopwords in this exercise\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Select Training/Validaiton/Test Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Modeler Input:_  \n",
    "* Select a president to build models on  \n",
    "* Select the percentages of files in training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# Select a president to build models on\n",
    "dir_president = \"CorpusOfPresidentialSpeeches/obama\"\n",
    "# split_pct = [training_pct, validation_pct, test_pct]\n",
    "split_pct = [.4, .4, .3]\n",
    "# Use x number of characters/digits to predict the next character\n",
    "seq_length = 100\n",
    "# Set sed number\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select training/validaiton/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "['obama_speeches_028.txt' 'obama_speeches_015.txt'\n",
      " 'obama_speeches_018.txt' 'obama_speeches_045.txt'\n",
      " 'obama_speeches_035.txt' 'obama_speeches_000.txt'\n",
      " 'obama_speeches_041.txt' 'obama_speeches_039.txt'\n",
      " 'obama_speeches_019.txt' 'obama_speeches_001.txt'\n",
      " 'obama_speeches_003.txt' 'obama_speeches_032.txt'\n",
      " 'obama_speeches_040.txt' 'obama_speeches_029.txt'\n",
      " 'obama_speeches_002.txt' 'obama_speeches_014.txt'\n",
      " 'obama_speeches_005.txt' 'obama_speeches_033.txt'\n",
      " 'obama_speeches_016.txt']\n",
      "Validation set:\n",
      "['obama_speeches_031.txt' 'obama_speeches_043.txt'\n",
      " 'obama_speeches_021.txt' 'obama_speeches_012.txt'\n",
      " 'obama_speeches_030.txt' 'obama_speeches_026.txt'\n",
      " 'obama_speeches_038.txt' 'obama_speeches_047.txt'\n",
      " 'obama_speeches_022.txt' 'obama_speeches_009.txt'\n",
      " 'obama_speeches_023.txt' 'obama_speeches_020.txt'\n",
      " 'obama_speeches_036.txt' 'obama_speeches_017.txt'\n",
      " 'obama_speeches_008.txt' 'obama_speeches_034.txt'\n",
      " 'obama_speeches_049.txt']\n",
      "Test set:\n",
      "['obama_speeches_027.txt' 'obama_speeches_046.txt'\n",
      " 'obama_speeches_004.txt' 'obama_speeches_010.txt'\n",
      " 'obama_speeches_042.txt' 'obama_speeches_007.txt'\n",
      " 'obama_speeches_044.txt' 'obama_speeches_013.txt'\n",
      " 'obama_speeches_006.txt' 'obama_speeches_037.txt'\n",
      " 'obama_speeches_048.txt' 'obama_speeches_011.txt']\n"
     ]
    }
   ],
   "source": [
    "# onlyfiles contains a list of files (not directories) under path_president\n",
    "# Reference: https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "onlyfiles_lst = [f for f in listdir(dir_president) if isfile(join(dir_president, f))]\n",
    "num_of_files = len(onlyfiles_lst)\n",
    "# Reference: https://stackoverflow.com/questions/15511349/select-50-items-from-list-at-random-to-write-to-file/39585770\n",
    "files_train_arr = np.random.choice(onlyfiles_lst, round(num_of_files*split_pct[0]), replace=False)\n",
    "# Set substraction: https://stackoverflow.com/questions/3428536/python-list-subtraction-operation\n",
    "files_val_test_lst = list(set(onlyfiles_lst) - set(files_train_arr))\n",
    "files_val_arr = np.random.choice(files_val_test_lst, round(len(files_val_test_lst)*split_pct[1]/(split_pct[1]+split_pct[2])), replace=False)\n",
    "files_test_arr = np.array(list((set(files_val_test_lst) - set(files_val_arr))))\n",
    "\n",
    "print('Training set:')\n",
    "print(files_train_arr)\n",
    "print('Validation set:')\n",
    "print(files_val_arr)\n",
    "print('Test set:')\n",
    "print(files_test_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Pre-processing Data so that It Can Be Consumed by _tensorflow.keras.layers.LSTM_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Questions**_:\n",
    "* Why remove special characters?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_words(input_file):\n",
    "    \"\"\"\n",
    "    This function accomplishes four purposes:\n",
    "    1. Remove the title and date (the first two rows) from the input file\n",
    "    2. Remove all special characters except for . and ,\n",
    "    3. Convert all characters to lower case\n",
    "    4. Tokenize words\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): input file\n",
    "        \n",
    "    Returns:\n",
    "        output_file (str): tokenized strings separated by space\n",
    "    \"\"\"\n",
    "    # Remove the title and date (the first two rows)\n",
    "    startChar = [word.end() for word in re.finditer(\"\\n\",file)][1]\n",
    "    input2 = input_file[startChar:]\n",
    "    \n",
    "    # lowercase everything so that we have less tokens to predict\n",
    "    #     i.e., no need to distinguish a vs. A\n",
    "    input2 = input2.lower()\n",
    "\n",
    "    # Keep all the words and digitis\n",
    "    # Keep only two special characters: . and ,\n",
    "    # If we want to keep carriage return, add |\\n\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[\\.\\,]')\n",
    "    tokens = tokenizer.tokenize(input2)\n",
    "    output_file = \" \".join(tokens)\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of all possible characters and digits in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It's possible that digits in the validation/test sets are not training set\n",
    "# To make sure every character/digit can be converted to a number \n",
    "#     and subsequently scored appropriately for validation/test sets,\n",
    "# We define chars_lst as all possible characters/digits we can observe from training/validaiton/test sets\n",
    "# The code below only captures characters/digits in the training set and thus inappropriate\n",
    "#     chars_lst = sorted(list(set(tokenized_file)))\n",
    "# Reference: https://stackoverflow.com/questions/16060899/alphabet-range-on-python\n",
    "chars_lst = [' ',',','.'] + [str(i) for i in range(10)] + [chr(i) for i in range(ord('a'),ord('z')+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**_:\n",
    "* I don't understand the logic of converting `X` to float or divided by vocab_len so that all Xs are smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_x_y_num(input_file, chars_lst, seq_length):\n",
    "    \"\"\"\n",
    "    This function creates raw input data and raw target character.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): tokenized file\n",
    "        chars_list (list): a list of all possible characters and digits in the data\n",
    "        seq_length (int): the number of characters/digits as input\n",
    "        \n",
    "    Returns:\n",
    "        x_data (list): a list of rolling ?-character sequences converted to floats\n",
    "            number of elements (i.e., sequences) in the list = input_len - seq_length\n",
    "            every element is an array with dimension (seq_length x 1)\n",
    "        y_data (list): the next character for every rolling sequence\n",
    "            number of elements = input_len - seq_length\n",
    "    \"\"\"\n",
    "    \n",
    "    # input_len - seq_length = the beginning character of the last row of input data\n",
    "    # vocab_len is used to standardized the input data\n",
    "    input_len = len(input_file)\n",
    "    vocab_len = len(chars_lst)\n",
    "    # print (\"Total number of characters:\", input_len)\n",
    "    # print (\"Total vocab:\", vocab_len)\n",
    "    \n",
    "    # Define the dictionary that map characters/digits to numbers\n",
    "    char_to_num = dict((c, i) for i, c in enumerate(chars_lst))\n",
    "\n",
    "    # Initialize the data\n",
    "    x_data_temp = []\n",
    "    y_data = []\n",
    "    \n",
    "    # loop through inputs, start at the beginning and go until we hit\n",
    "    # the final character we can create a sequence out of\n",
    "    for i in range(0, input_len - seq_length, 1):\n",
    "        # Define input and output sequences\n",
    "        # Input is the current character plus desired sequence length\n",
    "        in_seq = input_file[i:i + seq_length]\n",
    "\n",
    "        # Out sequence is the initial character plus total sequence length\n",
    "        out_seq = input_file[i + seq_length]\n",
    "\n",
    "        # Convert list of characters to integers \n",
    "        x_data_temp.append([char_to_num[char] for char in in_seq])\n",
    "        y_data.append(char_to_num[out_seq])\n",
    "        \n",
    "    # Convert the input sequences \n",
    "    #     (a list containning sublists, with each sublist represent a 100-character sequence)\n",
    "    #     into a processed numpy array that our network can use\n",
    "    n_patterns = len(x_data_temp)\n",
    "    x_data_reshape = np.reshape(x_data_temp, (n_patterns, seq_length, 1))\n",
    "\n",
    "    # Convert intergers into floats \n",
    "    # so that the sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1\n",
    "    x_data = list(x_data_reshape/float(vocab_len))\n",
    "        \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_x_y(dir_president, files_arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_temp = []\n",
    "    Y_temp = []\n",
    "    for i in range(files_arr.shape[0]):\n",
    "        file = open(join(dir_president, files_arr[i])).read()\n",
    "        \n",
    "        # Tokenize the file\n",
    "        tokenized_file = tokenize_words(file)\n",
    "        \n",
    "        # Create raw x and y for a given file in a format that can be merged with other files\n",
    "        x_data, y_data = create_x_y_num(tokenized_file, chars_lst, seq_length)\n",
    "        \n",
    "        # Use extend not append\n",
    "        #     append adds an element that's a list itself\n",
    "        #     extend adds elements from the new list to the existing list\n",
    "        # Reference: https://stackabuse.com/append-vs-extend-in-python-lists/\n",
    "        X_temp.extend(x_data)\n",
    "        Y_temp.extend(y_data)\n",
    "    \n",
    "    x = np.array(X_temp)\n",
    "    # One-hot encode the label data\n",
    "    y = keras.utils.to_categorical(Y_temp)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422133, 100, 1)\n",
      "(422133, 39)\n",
      "(380467, 100, 1)\n",
      "(380467, 39)\n",
      "(345024, 100, 1)\n",
      "(345024, 39)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = combine_x_y(dir_president, files_train_arr)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "val_X, val_Y = combine_x_y(dir_president, files_val_arr)\n",
    "print(val_X.shape)\n",
    "print(val_Y.shape)\n",
    "\n",
    "test_X, test_Y = combine_x_y(dir_president, files_test_arr)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(256, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(train_Y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default learning rate for adam optimizer is 0.001.  \n",
    "(Reference: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)  \n",
    "To change the learning rate, see https://www.tensorflow.org/guide/keras/train_and_evaluate (tensor), https://keras.io/optimizers/ (keras)  \n",
    "\n",
    "_**Note**_: maybe research on the optimizer to use??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam'(learning_rate=1e-3))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 422133 samples, validate on 380467 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 23040/422133 [>.............................] - ETA: 2:04:53 - loss: 3.0399"
     ]
    }
   ],
   "source": [
    "# Capture fit history\n",
    "# Reference: https://chrisalbon.com/deep_learning/keras/visualize_loss_history/\n",
    "history = model.fit(train_X, train_Y, epochs=2, batch_size=256, validation_data=(val_X,val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference on `keras.callbacks.ModelCheckpoint`:\n",
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_weights_LSTM_character_Angela_attemp2.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10496/10503 [============================>.] - ETA: 0s - loss: 2.8756 \n",
      "Epoch 00001: loss improved from 2.88151 to 2.87522, saving model to model_weights_LSTM_Angela_attemp2.hdf5\n",
      "10503/10503 [==============================] - 455s 43ms/sample - loss: 2.8752\n",
      "Epoch 2/2\n",
      "10496/10503 [============================>.] - ETA: 0s - loss: 2.8725 \n",
      "Epoch 00002: loss improved from 2.87522 to 2.87276, saving model to model_weights_LSTM_Angela_attemp2.hdf5\n",
      "10503/10503 [==============================] - 464s 44ms/sample - loss: 2.8728\n"
     ]
    }
   ],
   "source": [
    "# Capture fit history\n",
    "# Reference: https://chrisalbon.com/deep_learning/keras/visualize_loss_history/\n",
    "history = model.fit(X, y, epochs=2, batch_size=256, validation_data=(), callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.9931623756142103, 2.8815136720348082]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://chrisalbon.com/deep_learning/keras/visualize_loss_history/\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "loss_history_arr = np.array(loss_history)\n",
    "np.savetxt(\"loss_history.txt\", loss_history_arr, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the same session, I can continue to train using another `fit`. If the session was restarted or interrupted, to continue the fit see  \n",
    "* https://stackoverflow.com/questions/45393429/keras-how-to-save-model-and-continue-training  \n",
    "* https://www.mikulskibartosz.name/save-and-restore-a-tensorflow-model-using-keras-for-continuous-model-training/  \n",
    "(Haven't implemented it yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "10496/10503 [============================>.] - ETA: 0s - loss: 2.8721\n",
      "Epoch 00001: loss improved from inf to 2.87206, saving model to model_weights_LSTM_Angela_attemp2.hdf5\n",
      "10503/10503 [==============================] - 146s 14ms/sample - loss: 2.8721\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "# No checkpoint needed to save the model\n",
    "checkpoint_path = \"model_weights_LSTM_character_Angela_attemp2.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.load_weights(checkpoint_path)\n",
    "desired_callbacks = [checkpoint]\n",
    "history = model.fit(X, y, epochs=1, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_LSTM_character_Angela_attemp2.hdf5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Angela\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model = keras.models.load_model('model_LSTM_character_Angela_attemp2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10496/10503 [============================>.] - ETA: 0s - loss: 2.8712\n",
      "Epoch 00001: loss improved from 2.87206 to 2.87085, saving model to model_weights_LSTM_Angela_attemp2.hdf5\n",
      "10503/10503 [==============================] - 190s 18ms/sample - loss: 2.8709\n",
      "Epoch 2/2\n",
      "10496/10503 [============================>.] - ETA: 0s - loss: 2.8641\n",
      "Epoch 00002: loss improved from 2.87085 to 2.86386, saving model to model_weights_LSTM_Angela_attemp2.hdf5\n",
      "10503/10503 [==============================] - 284s 27ms/sample - loss: 2.8639\n"
     ]
    }
   ],
   "source": [
    "history = new_model.fit(X, y, epochs=2, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.4368\n",
      "\n",
      "Epoch 00001: loss improved from 2.45532 to 2.43682, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.4065\n",
      "\n",
      "Epoch 00002: loss improved from 2.43682 to 2.40654, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.3761\n",
      "\n",
      "Epoch 00003: loss improved from 2.40654 to 2.37607, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 2.3542\n",
      "\n",
      "Epoch 00004: loss improved from 2.37607 to 2.35417, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/20\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 2.3160\n",
      "\n",
      "Epoch 00005: loss improved from 2.35417 to 2.31597, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 2.2786\n",
      "\n",
      "Epoch 00006: loss improved from 2.31597 to 2.27857, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 2.2432\n",
      "\n",
      "Epoch 00007: loss improved from 2.27857 to 2.24322, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "5929/5929 [==============================] - 155s 26ms/step - loss: 2.2074\n",
      "\n",
      "Epoch 00008: loss improved from 2.24322 to 2.20737, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 2.1708\n",
      "\n",
      "Epoch 00009: loss improved from 2.20737 to 2.17077, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 2.1280\n",
      "\n",
      "Epoch 00010: loss improved from 2.17077 to 2.12795, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 2.1005\n",
      "\n",
      "Epoch 00011: loss improved from 2.12795 to 2.10054, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 2.0497\n",
      "\n",
      "Epoch 00012: loss improved from 2.10054 to 2.04972, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 2.0118\n",
      "\n",
      "Epoch 00013: loss improved from 2.04972 to 2.01179, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 1.9692\n",
      "\n",
      "Epoch 00014: loss improved from 2.01179 to 1.96918, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.9003\n",
      "\n",
      "Epoch 00015: loss improved from 1.96918 to 1.90035, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/20\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 1.8625\n",
      "\n",
      "Epoch 00016: loss improved from 1.90035 to 1.86251, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/20\n",
      "5929/5929 [==============================] - 146s 25ms/step - loss: 1.8237\n",
      "\n",
      "Epoch 00017: loss improved from 1.86251 to 1.82370, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.7783\n",
      "\n",
      "Epoch 00018: loss improved from 1.82370 to 1.77832, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/20\n",
      "5929/5929 [==============================] - 158s 27ms/step - loss: 1.7272\n",
      "\n",
      "Epoch 00019: loss improved from 1.77832 to 1.72723, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.6870\n",
      "\n",
      "Epoch 00020: loss improved from 1.72723 to 1.68699, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f56d00080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.6292\n",
      "\n",
      "Epoch 00001: loss improved from 1.68699 to 1.62925, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.5841\n",
      "\n",
      "Epoch 00002: loss improved from 1.62925 to 1.58411, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.5526\n",
      "\n",
      "Epoch 00003: loss improved from 1.58411 to 1.55259, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.4836\n",
      "\n",
      "Epoch 00004: loss improved from 1.55259 to 1.48358, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.4374\n",
      "\n",
      "Epoch 00005: loss improved from 1.48358 to 1.43740, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/20\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 1.4047\n",
      "\n",
      "Epoch 00006: loss improved from 1.43740 to 1.40468, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/20\n",
      "5929/5929 [==============================] - 156s 26ms/step - loss: 1.3731\n",
      "\n",
      "Epoch 00007: loss improved from 1.40468 to 1.37305, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 1.3045\n",
      "\n",
      "Epoch 00008: loss improved from 1.37305 to 1.30448, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/20\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 1.2809\n",
      "\n",
      "Epoch 00009: loss improved from 1.30448 to 1.28089, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.2314\n",
      "\n",
      "Epoch 00010: loss improved from 1.28089 to 1.23145, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1794\n",
      "\n",
      "Epoch 00011: loss improved from 1.23145 to 1.17941, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1421\n",
      "\n",
      "Epoch 00012: loss improved from 1.17941 to 1.14205, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.1276\n",
      "\n",
      "Epoch 00013: loss improved from 1.14205 to 1.12757, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 1.0674\n",
      "\n",
      "Epoch 00014: loss improved from 1.12757 to 1.06736, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 1.0508\n",
      "\n",
      "Epoch 00015: loss improved from 1.06736 to 1.05076, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 1.0113\n",
      "\n",
      "Epoch 00016: loss improved from 1.05076 to 1.01128, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/20\n",
      "5929/5929 [==============================] - 150s 25ms/step - loss: 0.9626\n",
      "\n",
      "Epoch 00017: loss improved from 1.01128 to 0.96265, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/20\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.9304\n",
      "\n",
      "Epoch 00018: loss improved from 0.96265 to 0.93042, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/20\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.9037\n",
      "\n",
      "Epoch 00019: loss improved from 0.93042 to 0.90374, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/20\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.8637\n",
      "\n",
      "Epoch 00020: loss improved from 0.90374 to 0.86370, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f02f97978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.8390\n",
      "\n",
      "Epoch 00001: loss improved from 0.86370 to 0.83903, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.8051\n",
      "\n",
      "Epoch 00002: loss improved from 0.83903 to 0.80507, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.7663\n",
      "\n",
      "Epoch 00003: loss improved from 0.80507 to 0.76631, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.7368\n",
      "\n",
      "Epoch 00004: loss improved from 0.76631 to 0.73677, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.7190\n",
      "\n",
      "Epoch 00005: loss improved from 0.73677 to 0.71900, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.6889\n",
      "\n",
      "Epoch 00006: loss improved from 0.71900 to 0.68887, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.6558\n",
      "\n",
      "Epoch 00007: loss improved from 0.68887 to 0.65578, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/40\n",
      "5929/5929 [==============================] - 154s 26ms/step - loss: 0.6278\n",
      "\n",
      "Epoch 00008: loss improved from 0.65578 to 0.62784, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/40\n",
      "5929/5929 [==============================] - 151s 25ms/step - loss: 0.6190\n",
      "\n",
      "Epoch 00009: loss improved from 0.62784 to 0.61905, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.6004\n",
      "\n",
      "Epoch 00010: loss improved from 0.61905 to 0.60044, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.5642\n",
      "\n",
      "Epoch 00011: loss improved from 0.60044 to 0.56416, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.5591\n",
      "\n",
      "Epoch 00012: loss improved from 0.56416 to 0.55909, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.5407\n",
      "\n",
      "Epoch 00013: loss improved from 0.55909 to 0.54067, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.5241\n",
      "\n",
      "Epoch 00014: loss improved from 0.54067 to 0.52412, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.5024\n",
      "\n",
      "Epoch 00015: loss improved from 0.52412 to 0.50243, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4667\n",
      "\n",
      "Epoch 00016: loss improved from 0.50243 to 0.46669, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4619\n",
      "\n",
      "Epoch 00017: loss improved from 0.46669 to 0.46194, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4424\n",
      "\n",
      "Epoch 00018: loss improved from 0.46194 to 0.44238, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4270\n",
      "\n",
      "Epoch 00019: loss improved from 0.44238 to 0.42700, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.4117\n",
      "\n",
      "Epoch 00020: loss improved from 0.42700 to 0.41169, saving model to model_weights_saved.hdf5\n",
      "Epoch 21/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3887\n",
      "\n",
      "Epoch 00021: loss improved from 0.41169 to 0.38866, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3865\n",
      "\n",
      "Epoch 00022: loss improved from 0.38866 to 0.38649, saving model to model_weights_saved.hdf5\n",
      "Epoch 23/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3602\n",
      "\n",
      "Epoch 00023: loss improved from 0.38649 to 0.36019, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3662\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.36019\n",
      "Epoch 25/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3583\n",
      "\n",
      "Epoch 00025: loss improved from 0.36019 to 0.35831, saving model to model_weights_saved.hdf5\n",
      "Epoch 26/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.3340\n",
      "\n",
      "Epoch 00026: loss improved from 0.35831 to 0.33399, saving model to model_weights_saved.hdf5\n",
      "Epoch 27/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.3295\n",
      "\n",
      "Epoch 00027: loss improved from 0.33399 to 0.32953, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.3188\n",
      "\n",
      "Epoch 00028: loss improved from 0.32953 to 0.31880, saving model to model_weights_saved.hdf5\n",
      "Epoch 29/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2929\n",
      "\n",
      "Epoch 00029: loss improved from 0.31880 to 0.29288, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2943\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.29288\n",
      "Epoch 31/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2797\n",
      "\n",
      "Epoch 00031: loss improved from 0.29288 to 0.27972, saving model to model_weights_saved.hdf5\n",
      "Epoch 32/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2739\n",
      "\n",
      "Epoch 00032: loss improved from 0.27972 to 0.27387, saving model to model_weights_saved.hdf5\n",
      "Epoch 33/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2640\n",
      "\n",
      "Epoch 00033: loss improved from 0.27387 to 0.26402, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2527\n",
      "\n",
      "Epoch 00034: loss improved from 0.26402 to 0.25269, saving model to model_weights_saved.hdf5\n",
      "Epoch 35/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2519\n",
      "\n",
      "Epoch 00035: loss improved from 0.25269 to 0.25195, saving model to model_weights_saved.hdf5\n",
      "Epoch 36/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2395\n",
      "\n",
      "Epoch 00036: loss improved from 0.25195 to 0.23954, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.2222\n",
      "\n",
      "Epoch 00037: loss improved from 0.23954 to 0.22224, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/40\n",
      "5929/5929 [==============================] - 14619s 2s/step - loss: 0.2338\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.22224\n",
      "Epoch 39/40\n",
      "5929/5929 [==============================] - 152s 26ms/step - loss: 0.2230\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.22224\n",
      "Epoch 40/40\n",
      "5929/5929 [==============================] - 153s 26ms/step - loss: 0.2229\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.22224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f02f97668>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.2186\n",
      "\n",
      "Epoch 00001: loss improved from 0.22224 to 0.21859, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.2072\n",
      "\n",
      "Epoch 00002: loss improved from 0.21859 to 0.20716, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1941\n",
      "\n",
      "Epoch 00003: loss improved from 0.20716 to 0.19415, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1915\n",
      "\n",
      "Epoch 00004: loss improved from 0.19415 to 0.19152, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1760\n",
      "\n",
      "Epoch 00005: loss improved from 0.19152 to 0.17600, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1755\n",
      "\n",
      "Epoch 00006: loss improved from 0.17600 to 0.17551, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1813\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.17551\n",
      "Epoch 8/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1717\n",
      "\n",
      "Epoch 00008: loss improved from 0.17551 to 0.17171, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1591\n",
      "\n",
      "Epoch 00009: loss improved from 0.17171 to 0.15912, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1613\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.15912\n",
      "Epoch 11/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1530\n",
      "\n",
      "Epoch 00011: loss improved from 0.15912 to 0.15303, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1523\n",
      "\n",
      "Epoch 00012: loss improved from 0.15303 to 0.15228, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1612\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.15228\n",
      "Epoch 14/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1522\n",
      "\n",
      "Epoch 00014: loss improved from 0.15228 to 0.15219, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1488\n",
      "\n",
      "Epoch 00015: loss improved from 0.15219 to 0.14879, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1396\n",
      "\n",
      "Epoch 00016: loss improved from 0.14879 to 0.13955, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1374\n",
      "\n",
      "Epoch 00017: loss improved from 0.13955 to 0.13741, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1355\n",
      "\n",
      "Epoch 00018: loss improved from 0.13741 to 0.13550, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1236\n",
      "\n",
      "Epoch 00019: loss improved from 0.13550 to 0.12365, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1261\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.12365\n",
      "Epoch 21/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1189\n",
      "\n",
      "Epoch 00021: loss improved from 0.12365 to 0.11894, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1203\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.11894\n",
      "Epoch 23/40\n",
      "5929/5929 [==============================] - 149s 25ms/step - loss: 0.1175\n",
      "\n",
      "Epoch 00023: loss improved from 0.11894 to 0.11748, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1197\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.11748\n",
      "Epoch 25/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1226\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.11748\n",
      "Epoch 26/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1199\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.11748\n",
      "Epoch 27/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1045\n",
      "\n",
      "Epoch 00027: loss improved from 0.11748 to 0.10452, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1073\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.10452\n",
      "Epoch 29/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0982\n",
      "\n",
      "Epoch 00029: loss improved from 0.10452 to 0.09816, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1018\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.09816\n",
      "Epoch 31/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1016\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.09816\n",
      "Epoch 32/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.1007\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.09816\n",
      "Epoch 33/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0953\n",
      "\n",
      "Epoch 00033: loss improved from 0.09816 to 0.09534, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0996\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.09534\n",
      "Epoch 35/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0958\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.09534\n",
      "Epoch 36/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0852\n",
      "\n",
      "Epoch 00036: loss improved from 0.09534 to 0.08516, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/40\n",
      "5929/5929 [==============================] - 147s 25ms/step - loss: 0.0820\n",
      "\n",
      "Epoch 00037: loss improved from 0.08516 to 0.08198, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1000\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.08198\n",
      "Epoch 39/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.1047\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.08198\n",
      "Epoch 40/40\n",
      "5929/5929 [==============================] - 148s 25ms/step - loss: 0.0872\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.08198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f5b6074a8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture loss history, see  \n",
    "* https://stackoverflow.com/questions/38445982/how-to-log-keras-loss-output-to-a-file\n",
    "* https://forums.fast.ai/t/passing-multiple-callbacks-in-keras-early-stopping-modelcheckpoint-lrratescheduler/5477  \n",
    "(Haven't implemented it yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_weights_LSTM_character_Angela_attemp2.hdf5\"\n",
    "model.load_weights(checkpoint_path)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\"  defeat seek peace security support wondered america beacon still burns bright tonight proved true s \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trength nation comes might arms scale wealth enduring power ideals democracy liberty opportunity unyielding hope true genius america america change union perfected already achieved gives us hope must achieve tomorrow election many firsts many stories told generations one mind tonight woman cast ballot atlanta lot like millions others stood line make voice heard election except one thing ann nixon cooper 106 years old born generation past slavery time cars road planes sky someone like vote two reasons woman color skin tonight think seen throughout century america heartache hope struggle progress times told people pressed american creed yes time women voices silenced hopes dismissed lived see stand speak reach ballot yes despair dust bowl depression across land saw nation conquer fear new deal new jobs new sense common purpose yes bombs fell harbor tyranny threatened world witness generation rise greatness democracy saved yes buses montgomery hoses birmingham bridge selma preacher atlant"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**Note**_: Should look into Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
