{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To Chairman Dean and my great friend Dick Durbin; and to all my fellow citizens of this great nation;With profound gratitude and great humility, I accept your nomination for the presidency of the United States.', 'Let me express my thanks to the historic slate of candidates who accompanied me on this journey, and especially the one who traveled the farthest  a champion for working Americans and an inspiration to my daughters and to yours  Hillary Rodham Clinton.', 'To President Clinton, who last night made the case for change as only he can make it; to Ted Kennedy, who embodies the spirit of service; and to the next Vice President of the United States, Joe Biden, I thank you.', 'I am grateful to finish this journey with one of the finest statesmen of our time, a man at ease with everyone from world leaders to the conductors on the Amtrak train he still takes home every night.', 'To the love of my life, our next First Lady, Michelle Obama, and to Sasha and Malia  I love you so much, and I am so proud of all of you.']\n",
      "['We salute you in the United States Senate.', 'I thank you so much.', 'And for the last three years, this administration has changed very little.', 'I look forward to getting back to work next month at the National Archives.', 'Tonight, I am going to sing to you  to my elected officials.']\n"
     ]
    }
   ],
   "source": [
    "original = open(\"all_obama_test_tuned_inputs-Copy1.txt\", \"r\").read()\n",
    "results = open(\"all_obama_test_tuned_results-Copy1.txt\", \"r\").read()\n",
    "print(original)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score.bert_score.score as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 898823/898823 [00:00<00:00, 1912363.05B/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 456318/456318 [00:00<00:00, 1137944.65B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 474/474 [00:00<00:00, 237498.52B/s]\n",
      "100%|██████████████████████████████████████████████████████████████| 1425941629/1425941629 [06:09<00:00, 3860973.02B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4802b077544cdea62888db1da462ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee229212696d4070b68af43df1611ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.54 seconds, 1.10 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "original = original.splitlines()\n",
    "results = results.splitlines()\n",
    "P, R, F1 = score(original, results, lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8291, 0.8049, 0.8318, 0.8415, 0.8444]) tensor([0.9074, 0.8800, 0.8702, 0.8772, 0.8756]) tensor([0.8665, 0.8408, 0.8505, 0.8590, 0.8597])\n"
     ]
    }
   ],
   "source": [
    "print(P,R,F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmt.nmt.scripts.bleu as bleu\n",
    "\"\"\"\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, [0.33948339483394835, 0.0, 0.0, 0.0], 1.0, 54.2, 271, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bleu, precisions, bp, ratio, translation_length, reference_length\n",
    "bleu.compute_bleu(original, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmt.nmt.scripts.rouge as rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05194804850733705, 0.03333333333333333, 0.11764705882352941)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Computes ROUGE-N of two text collections of sentences.\n",
    "  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "  papers/rouge-working-note-v1.3.1.pdf\n",
    "\n",
    "  Args:\n",
    "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
    "    reference_sentences: The sentences from the referene set\n",
    "    n: Size of ngram.  Defaults to 2.\n",
    "\n",
    "  Returns:\n",
    "    A tuple (f1, precision, recall) for ROUGE-N\n",
    "\n",
    "  Raises:\n",
    "    ValueError: raises exception if a param has len <= 0\n",
    "\"\"\"\n",
    "rouge.rouge_n(original, results, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0720686072702675, 0.06842105263157895, 0.25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Computes ROUGE-L (sentence level) of two text collections of sentences.\n",
    "  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n",
    "  rouge-working-note-v1.3.1.pdf\n",
    "\n",
    "  Calculated according to:\n",
    "  R_lcs = LCS(X,Y)/m\n",
    "  P_lcs = LCS(X,Y)/n\n",
    "  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n",
    "\n",
    "  where:\n",
    "  X = reference summary\n",
    "  Y = Candidate summary\n",
    "  m = length of reference summary\n",
    "  n = length of candidate summary\n",
    "\n",
    "  Args:\n",
    "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
    "    reference_sentences: The sentences from the referene set\n",
    "\n",
    "  Returns:\n",
    "    A float: F_lcs\n",
    "\n",
    "  Raises:\n",
    "    ValueError: raises exception if a param has len <= 0\n",
    "\"\"\"\n",
    "rouge.rouge_l_sentence_level(original, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-py3",
   "language": "python",
   "name": "w266-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
