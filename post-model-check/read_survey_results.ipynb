{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "survey_result_path = '../survey'\n",
    "survey_result_file_lst = [f for f in listdir(survey_result_path) if isfile(join(survey_result_path, f)) and f.endswith(\".csv\")]\n",
    "\n",
    "file_of_interest = survey_result_file_lst[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def parse_survey_results(file, verbose = 0):\n",
    "    with open(join(survey_result_path, file)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        question_index_lst = []\n",
    "        \n",
    "        input_dict3_lst = []\n",
    "        actual_score_lst = []\n",
    "        gpt2_tuned_score_lst = []\n",
    "        xlnet_score_lst = []\n",
    "        lstm_attention_score_lst = []\n",
    "        gpt2_untuned_bert_score_lst = []\n",
    "        gpt2_untuned_score_lst = []\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                for (i,item) in enumerate(row):\n",
    "                    if '_' in item:\n",
    "                        question_index_lst.append(i)\n",
    "            elif line_count == 1:\n",
    "                for (i,item) in enumerate(row):\n",
    "                    if i in question_index_lst:\n",
    "                        input_dict3_lst.append(int(item.split('. ')[0]))\n",
    "            elif line_count > 2:\n",
    "                j = 0\n",
    "                for (i,item) in enumerate(row):\n",
    "                    if i in question_index_lst:\n",
    "                        if len(item) == 0:\n",
    "                            score_num = ''\n",
    "                        else:\n",
    "                            if item == '1':\n",
    "                                score_num = 1\n",
    "                            elif item == '2':\n",
    "                                score_num = 2\n",
    "                            elif item == '6':\n",
    "                                score_num = 3\n",
    "                            elif item == '7':\n",
    "                                score_num = 4\n",
    "                            elif item == '8':\n",
    "                                score_num = 5\n",
    "                                \n",
    "                        if j % 6 == 0:\n",
    "                            actual_score_lst.append(score_num)\n",
    "                        elif j % 6 == 1:\n",
    "                            gpt2_tuned_score_lst.append(score_num)\n",
    "                        elif j % 6 == 2:\n",
    "                            xlnet_score_lst.append(score_num)\n",
    "                        elif j % 6 == 3:\n",
    "                            lstm_attention_score_lst.append(score_num)\n",
    "                        elif j % 6 == 4:\n",
    "                            gpt2_untuned_bert_score_lst.append(score_num)\n",
    "                        elif j % 6 == 5:\n",
    "                            gpt2_untuned_score_lst.append(score_num)\n",
    "                        j += 1\n",
    "            line_count += 1\n",
    "            \n",
    "    response_count = line_count - 3    \n",
    "    \n",
    "    # Due to my stupidity, manual part3 - 20 has 119 repeated twice\n",
    "    # The code below won't work for this scenario\n",
    "    # input_dict3_nodup_lst = sorted(set(input_dict3_lst),key=input_dict3_lst.index)\n",
    "    input_dict3_nodup_lst = []\n",
    "    prev = -1\n",
    "    for item in input_dict3_lst:\n",
    "        if item != prev:\n",
    "            input_dict3_nodup_lst.append(item)\n",
    "        prev = item\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(len(input_dict3_nodup_lst))\n",
    "        print(len(actual_score_lst))\n",
    "    \n",
    "    df = pd.DataFrame(list(zip( \n",
    "            input_dict3_nodup_lst*response_count, \n",
    "            actual_score_lst, \n",
    "            gpt2_tuned_score_lst, \n",
    "            xlnet_score_lst, \n",
    "            lstm_attention_score_lst, \n",
    "            gpt2_untuned_bert_score_lst, \n",
    "            gpt2_untuned_score_lst \n",
    "            )), \n",
    "            columns =['Sim_index', 'Actual', 'gpt2_tuned', 'xlnet', 'lstm_attention', 'gpt2_untuned_bert', 'gpt2_untuned']) \n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(df)\n",
    "        \n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(70, 7)\n(50, 7)\n(30, 7)\n(50, 7)\n(200, 7)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df_all = None\n",
    "for file_of_interest in survey_result_file_lst:\n",
    "    df_of_interest = parse_survey_results(file_of_interest)\n",
    "    print(df_of_interest.shape)\n",
    "    if df_all is None:\n",
    "        df_all = df_of_interest\n",
    "    else:\n",
    "        df_all = df_all.append(df_of_interest, ignore_index=True)\n",
    "    \n",
    "print(df_all.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "Actual               3.425\ngpt2_tuned           2.905\nxlnet                3.215\nlstm_attention       1.580\ngpt2_untuned_bert    2.795\ngpt2_untuned         3.135\ndtype: float64"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 63
    }
   ],
   "source": [
    "df_all.drop(columns=['Sim_index']).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "--- Compare Actual against gpt2_tuned ---\nt-Statistics=4.315, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare Actual against xlnet ---\nt-Statistics=1.747, 2-tailed p-value=0.082\nSame distributions (fail to reject H0)\n\n\n--- Compare Actual against lstm_attention ---\nt-Statistics=18.492, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare Actual against gpt2_untuned_bert ---\nt-Statistics=5.974, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare Actual against gpt2_untuned ---\nt-Statistics=2.627, 2-tailed p-value=0.009\nDifferent distributions (reject H0)\n\n\n--- Compare xlnet against gpt2_tuned ---\nt-Statistics=2.764, 2-tailed p-value=0.006\nDifferent distributions (reject H0)\n\n\n--- Compare xlnet against lstm_attention ---\nt-Statistics=16.995, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare xlnet against gpt2_untuned_bert ---\nt-Statistics=3.846, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare xlnet against gpt2_untuned ---\nt-Statistics=0.720, 2-tailed p-value=0.472\nSame distributions (fail to reject H0)\n\n\n--- Compare gpt2_untuned against gpt2_tuned ---\nt-Statistics=2.041, 2-tailed p-value=0.043\nDifferent distributions (reject H0)\n\n\n--- Compare gpt2_untuned against lstm_attention ---\nt-Statistics=15.885, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n--- Compare gpt2_untuned against gpt2_untuned_bert ---\nt-Statistics=4.062, 2-tailed p-value=0.000\nDifferent distributions (reject H0)\n\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# paired sample t-test\n",
    "\n",
    "for method in df_all.drop(columns=['Sim_index', 'Actual']).columns:\n",
    "    print('--- Compare Actual against %s ---' %method)\n",
    "    # compare samples\n",
    "    stat, p = ttest_rel(df_all['Actual'], df_all[method])\n",
    "    print('t-Statistics=%.3f, 2-tailed p-value=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Same distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distributions (reject H0)')\n",
    "    print('\\n')\n",
    "        \n",
    "for method in df_all.drop(columns=['Sim_index', 'Actual', 'xlnet']).columns:\n",
    "    print('--- Compare xlnet against %s ---' %method)\n",
    "    # compare samples\n",
    "    stat, p = ttest_rel(df_all['xlnet'], df_all[method])\n",
    "    print('t-Statistics=%.3f, 2-tailed p-value=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Same distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distributions (reject H0)')\n",
    "    print('\\n')\n",
    "    \n",
    "for method in df_all.drop(columns=['Sim_index', 'Actual', 'xlnet', 'gpt2_untuned']).columns:\n",
    "    print('--- Compare gpt2_untuned against %s ---' %method)\n",
    "    # compare samples\n",
    "    stat, p = ttest_rel(df_all['gpt2_untuned'], df_all[method])\n",
    "    print('t-Statistics=%.3f, 2-tailed p-value=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Same distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distributions (reject H0)')\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}