{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from random import randint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Windows only\n",
    "# gpt2_tuned_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\2.ModelingGPT2andXLNet\\testing_data_run\\combined_data\\test_models_results.txt\"\n",
    "# gpt2_untuned_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\2.ModelingGPT2andXLNet\\testing_data_run\\combined_data\\test_gpt2_results.txt\"\n",
    "# xlnet_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\2.ModelingGPT2andXLNet\\testing_data_run\\combined_data\\test_xlnet-base-cased_results.txt\"\n",
    "# lstm_attention_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\LSTM-word\\textgenrnn2\\LSTM-attention-result-test-fixed-30-epoch.txt\"\n",
    "# gpt2_untuned_bert_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\BERT_HAMMERING\\gpt2_BERT_result-new-dim.txt\"\n",
    "\n",
    "gpt2_tuned_path = r\"../2.ModelingGPT2andXLNet/testing_data_run/combined_data/test_models_results.txt\"\n",
    "gpt2_untuned_path = r\"../2.ModelingGPT2andXLNet/testing_data_run/combined_data/test_gpt2_results.txt\"\n",
    "xlnet_path = r\"../2.ModelingGPT2andXLNet/testing_data_run/combined_data/test_xlnet-base-cased_results.txt\"\n",
    "lstm_attention_path = r\"../LSTM-word/textgenrnn2/LSTM-attention-result-test-fixed-30-epoch.txt\"\n",
    "gpt2_untuned_bert_path = r\"../BERT_HAMMERING/gpt2_BERT_result-new-dim.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "max_line_review = 2700\n",
    "\n",
    "# Windows only\n",
    "# output1_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\survey\\question_prep1.txt\"\n",
    "# output2_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\survey\\question_prep2.txt\"\n",
    "# output3_path = r\"D:\\UCBerkeley\\CourseWork\\201909\\W266\\GitHub\\FinalProject-Collab\\survey\\question_prep3.txt\"\n",
    "\n",
    "output1_path = r\"../survey/question_prep1.txt\"\n",
    "# output2_path = r\"../survey/question_prep2.txt\"\n",
    "# output3_path = r\"../survey/question_prep3.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "gpt2_tuned_file = open(gpt2_tuned_path, 'r', encoding=\"utf8\").read().split('<end_line>')\n",
    "gpt2_untuned_file = open(gpt2_untuned_path, 'r', encoding=\"utf8\").read().split('<end_line>')\n",
    "xlnet_file = open(xlnet_path, 'r', encoding=\"utf8\").read().split('<end_line>')\n",
    "lstm_attention_file = open(lstm_attention_path, 'r', encoding=\"utf8\").read().split('<end_line>')\n",
    "gpt2_untuned_bert_file = open(gpt2_untuned_bert_path, 'r', encoding=\"utf8\").read().split('<end_line>')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def check_repeats(file, verbose=1, max_line=None):\n",
    "    \n",
    "    # flag as repeats if {repeat_threshold}% of test input is in the generated sentence\n",
    "    repeat_threshold = 0.5\n",
    "    \n",
    "    repeat_count = 0\n",
    "    invalid_count = 0\n",
    "    special_character_count = 0\n",
    "    valid_sentence_index = []\n",
    "    input_set = set()\n",
    "    input_duplicate_set = set()\n",
    "    \n",
    "    for i in range(0, len(file)):\n",
    "        \n",
    "        if max_line is not None:\n",
    "            if i == max_line:\n",
    "                break\n",
    "                \n",
    "        line = file[i].split('<entry>')\n",
    "        duplicate_binary = 0\n",
    "        \n",
    "        # Make sure it has input_sentence, actual_sentence, and pred_sentence\n",
    "        if len(line) < 3:\n",
    "            invalid_count += 1\n",
    "            continue\n",
    "            \n",
    "        if line[0][0:10] != '<new_line>':\n",
    "            invalid_count += 1   \n",
    "            continue\n",
    "            \n",
    "        # sentences less than 30 characters are invalid\n",
    "        if min(len(line[0][10:]), len(line[1]), len(line[2])) <= 30:\n",
    "            invalid_count += 1\n",
    "            continue  \n",
    "            \n",
    "        input_sentence = line[0][10:]\n",
    "        # Create a set of duplicate inputs\n",
    "        if input_sentence not in input_set:\n",
    "            input_set.add(input_sentence)\n",
    "        else:\n",
    "            input_duplicate_set.add(input_sentence)\n",
    "            \n",
    "        actual_sentence = line[1]\n",
    "        pred_sentence = line[2]\n",
    "        \n",
    "        # Don't want carriage return, # sign, or excessive quotes etc. in the predicted sentence\n",
    "        if '\\n' in pred_sentence or '#' in pred_sentence or '(\"\")' in pred_sentence or '()' in pred_sentence \\\n",
    "                or '\"' in pred_sentence:\n",
    "            special_character_count += 1\n",
    "            continue  \n",
    "        \n",
    "        input_len = len(input_sentence.split())\n",
    "        check_len = round(input_len*repeat_threshold)\n",
    "    \n",
    "        for start_index in range(0, input_len-check_len):\n",
    "            check_list = input_sentence.split()[start_index: start_index + check_len]\n",
    "            check_phrase = ' '.join(check_list)\n",
    "            if check_phrase in pred_sentence:\n",
    "                if verbose == 1:\n",
    "                    print(f\"-------- line {i} --------\")\n",
    "                    print(input_sentence)\n",
    "                    print(pred_sentence)\n",
    "                repeat_count += 1\n",
    "                duplicate_binary = 1\n",
    "                break\n",
    "        \n",
    "        if duplicate_binary == 0:\n",
    "            valid_sentence_index.append(i)\n",
    "        \n",
    "    print('Number of invalid sentences: ', invalid_count)\n",
    "    print('Number of generated sentences similar to the input sentence: ', repeat_count)\n",
    "    print('Number of input sentences that appear multiple times: ', len(input_duplicate_set))\n",
    "    print('Number of sentences containing carriage return, # signs, or excessive quotes etc.: ', special_character_count)\n",
    "    if max_line is not None:\n",
    "        print('Sanity checks above are based off of the first ', max_line, 'lines. ')\n",
    "    print('Total number of input sentences: ', len(file))\n",
    "    \n",
    "    return valid_sentence_index, input_duplicate_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def check_repeats2(file, verbose=1, max_line=None):\n",
    "    \"\"\"\n",
    "    This function only uses for gpt2_untuned_bert because the data format is different\n",
    "    :param file: \n",
    "    :param verbose: \n",
    "    :param max_line: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # flag as repeats if {repeat_threshold}% of test input is in the generated sentence\n",
    "    repeat_threshold = 0.5\n",
    "\n",
    "    repeat_count = 0\n",
    "    invalid_count = 0\n",
    "    special_character_count = 0\n",
    "    valid_sentence_index = []\n",
    "    input_set = set()\n",
    "    input_duplicate_set = set()\n",
    "    \n",
    "    for i in range(0, len(file)):\n",
    "        \n",
    "        if max_line is not None:\n",
    "            if i == max_line:\n",
    "                break\n",
    "                \n",
    "        line = file[i].split('<entry>')\n",
    "        duplicate_binary = 0\n",
    "        \n",
    "        if len(line) < 7:\n",
    "            invalid_count += 1\n",
    "            continue\n",
    "            \n",
    "        if line[0][0:10] != '<new_line>':\n",
    "            invalid_count += 1   \n",
    "            continue\n",
    "        # sentences less than 30 characters are invalid\n",
    "        if min(len(line[0][10:]), len(line[1]), len(line[3])) <= 30:\n",
    "            invalid_count += 1\n",
    "            continue  \n",
    "            \n",
    "        input_sentence = line[0][10:]\n",
    "        # Create a set of duplicate inputs\n",
    "        if input_sentence not in input_set:\n",
    "            input_set.add(input_sentence)\n",
    "        else:\n",
    "            input_duplicate_set.add(input_sentence)\n",
    "            \n",
    "        actual_sentence = line[1]\n",
    "        pred_sentence = line[3]\n",
    "        \n",
    "        # Don't want carriage return, # sign, or excessive quotes etc. in the predicted sentence\n",
    "        if '\\n' in pred_sentence or '#' in pred_sentence or '(\"\")' in pred_sentence or '()' in pred_sentence \\\n",
    "                or '\"' in pred_sentence:\n",
    "            special_character_count += 1\n",
    "            continue  \n",
    "            \n",
    "        input_len = len(input_sentence.split())\n",
    "        check_len = round(input_len*repeat_threshold)\n",
    "    \n",
    "        for start_index in range(0, input_len-check_len):\n",
    "            check_list = input_sentence.split()[start_index: start_index + check_len]\n",
    "            check_phrase = ' '.join(check_list)\n",
    "            if check_phrase in pred_sentence:\n",
    "                if verbose == 1:\n",
    "                    print(f\"-------- line {i} --------\")\n",
    "                    print(input_sentence)\n",
    "                    print(pred_sentence)\n",
    "                repeat_count += 1\n",
    "                duplicate_binary = 1\n",
    "                break\n",
    "        \n",
    "        if duplicate_binary == 0:\n",
    "            valid_sentence_index.append(i)\n",
    "        \n",
    "    print('Number of invalid sentences: ', invalid_count)\n",
    "    print('Number of generated sentences similar to the input sentence: ', repeat_count)\n",
    "    print('Number of input sentences that appear multiple times: ', len(input_duplicate_set))\n",
    "    print('Number of sentences containing carriage return, # signs, or excessive quotes etc.: ', special_character_count)\n",
    "    if max_line is not None:\n",
    "        print('Sanity checks above are based off of the first ', max_line, 'lines. ')\n",
    "    print('Total number of input sentences: ', len(file))\n",
    "    \n",
    "    return valid_sentence_index, input_duplicate_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "------------- gpt2 tuned -------------\nNumber of invalid sentences:  269\nNumber of generated sentences similar to the input sentence:  1332\nNumber of input sentences that appear multiple times:  0\nNumber of sentences containing carriage return, # signs, or excessive quotes etc.:  0\nSanity checks above are based off of the first  2700 lines. \nTotal number of input sentences:  12501\n------------- gpt2 untuned -------------\nNumber of invalid sentences:  299\nNumber of generated sentences similar to the input sentence:  60\nNumber of input sentences that appear multiple times:  1\nNumber of sentences containing carriage return, # signs, or excessive quotes etc.:  249\nSanity checks above are based off of the first  2700 lines. \nTotal number of input sentences:  4999\n------------- xlnet -------------\n",
      "Number of invalid sentences:  325\nNumber of generated sentences similar to the input sentence:  131\nNumber of input sentences that appear multiple times:  1\nNumber of sentences containing carriage return, # signs, or excessive quotes etc.:  385\nSanity checks above are based off of the first  2700 lines. \nTotal number of input sentences:  2754\n------------- lstm attention -------------\nNumber of invalid sentences:  793\nNumber of generated sentences similar to the input sentence:  0\nNumber of input sentences that appear multiple times:  0\nNumber of sentences containing carriage return, # signs, or excessive quotes etc.:  0\nSanity checks above are based off of the first  2700 lines. \nTotal number of input sentences:  52238\n------------- gpt2 untuned with bert -------------\nNumber of invalid sentences:  307\nNumber of generated sentences similar to the input sentence:  37\nNumber of input sentences that appear multiple times:  1\nNumber of sentences containing carriage return, # signs, or excessive quotes etc.:  624\nSanity checks above are based off of the first  2700 lines. \nTotal number of input sentences:  4990\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('------------- gpt2 tuned -------------')\n",
    "gpt2_tuned_valid_sentence_index, duplicate_input1 = check_repeats(gpt2_tuned_file, verbose=0, max_line=max_line_review)\n",
    "print('------------- gpt2 untuned -------------')\n",
    "gpt2_untuned_valid_sentence_index, duplicate_input2 = check_repeats(gpt2_untuned_file, verbose=0, max_line=max_line_review)\n",
    "print('------------- xlnet -------------')\n",
    "xlnet_valid_sentence_index, duplicate_input3 = check_repeats(xlnet_file, verbose=0, max_line=max_line_review)\n",
    "print('------------- lstm attention -------------')\n",
    "lstm_attention_valid_sentence_index, duplicate_input4 = check_repeats(lstm_attention_file, verbose=0, max_line=max_line_review)\n",
    "print('------------- gpt2 untuned with bert -------------')\n",
    "gpt2_untuned_bert_valid_sentence_index, duplicate_input5 = check_repeats2(gpt2_untuned_bert_file, verbose=0, max_line=max_line_review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "duplicate_input_all = duplicate_input1.union(duplicate_input2, duplicate_input3, duplicate_input4, duplicate_input5)\n",
    "\n",
    "print(len(duplicate_input_all))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "------------- gpt2 tuned -------------\n<new_line>There are cynics , there are skeptics who say it cannot be done.<entry>The American people and all the freedom loving peoples of this earth are now demanding that it must be done.<entry>But I think that the way we do it is to make sure we do it.\n------------- gpt2 untuned -------------\n<new_line>There are cynics , there are skeptics who say it cannot be done.<entry>The American people and all the freedom loving peoples of this earth are now demanding that it must be done.<entry>But the fact is, we are doing this.\n------------- xlnet -------------\n<new_line>There are cynics , there are skeptics who say it cannot be done.<entry>The American people and all the freedom loving peoples of this earth are now demanding that it must be done.<entry>There are \" \", there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are \" \" and there are.\n------------- lstm attention -------------\n<new_line>There are cynics , there are skeptics who say it cannot be done.<entry>The American people and all the freedom loving peoples of this earth are now demanding that it must be done.<entry>Thirty four nations of industrial centers.\n------------- gpt2 untuned with bert -------------\n<new_line>The American people and all the freedom loving peoples of this earth are now demanding that it must be done.<entry>And the will of these people shall prevail.<entry>The United States of America has a long history of supporting freedom of expression, freedom of religion and freedom of the press.<entry> The United States of America has a long history of free freedom of expression , freedom of speech and freedom of the press <entry>11.08237413334106<entry>9.330310191237182<entry>2.622021141420901\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "index =2185\n",
    "print('------------- gpt2 tuned -------------')\n",
    "print(gpt2_tuned_file[index])\n",
    "print('------------- gpt2 untuned -------------')\n",
    "print(gpt2_untuned_file[index])\n",
    "print('------------- xlnet -------------')\n",
    "print(xlnet_file[index])\n",
    "print('------------- lstm attention -------------')\n",
    "print(lstm_attention_file[index])\n",
    "print('------------- gpt2 untuned with bert -------------')\n",
    "print(gpt2_untuned_bert_file[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# test_duplicate_lst = []\n",
    "# for j in range(50):\n",
    "#     test_input = list(duplicate_input4)[j]\n",
    "#     for i, sentences in enumerate(lstm_attention_file):\n",
    "#         test_input_sentence = sentences.split('<entry>')[0][10:]\n",
    "#         if test_input == test_input_sentence:\n",
    "#             test_duplicate_lst.append(i)\n",
    "#             print(i)\n",
    "# print(len(test_duplicate_lst))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Remove gpt2_untuned_bert_valid_sentence_index since it doesn't have the same index as others\n",
    "valid_sentence_index_union = gpt2_tuned_valid_sentence_index \\\n",
    "                            + gpt2_untuned_valid_sentence_index \\\n",
    "                            + xlnet_valid_sentence_index \\\n",
    "                            + lstm_attention_valid_sentence_index\n",
    "\n",
    "valid_sentence_index_dict = dict()\n",
    "for index in valid_sentence_index_union:\n",
    "    valid_sentence_index_dict[index] = valid_sentence_index_dict.get(index, 0) + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "573\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "valid_sentence_index_intersect = [key for key in valid_sentence_index_dict.keys() if valid_sentence_index_dict[key]==4]\n",
    "print(len(valid_sentence_index_intersect))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Create a list of valid input sentences based on gpt2_tuned, xlnet, and lstm_attention\n",
    "input_dict = {sentences.split('<entry>')[0][10:]: i for i, sentences in enumerate(gpt2_tuned_file) if i in valid_sentence_index_intersect}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "440\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def match_by_input(file, valid_sentence_index, in_input):\n",
    "    out_input = dict()\n",
    "    for i, sentences in enumerate(file):\n",
    "        input_sentence = sentences.split('<entry>')[0][10:]\n",
    "        if i in valid_sentence_index and input_sentence in in_input.keys() and input_sentence not in duplicate_input_all:\n",
    "            out_input[input_sentence] = in_input[input_sentence]\n",
    "        \n",
    "    return out_input\n",
    "\n",
    "input_dict3 = match_by_input(file=gpt2_untuned_bert_file, valid_sentence_index=gpt2_untuned_bert_valid_sentence_index, in_input=input_dict)\n",
    "# input_dict3 = match_by_input(file=lstm_attention_file, valid_sentence_index=lstm_attention_valid_sentence_index, in_input=input_dict2)\n",
    "\n",
    "print(len(input_dict3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "440\n440\n440\n440\n440\n440\n440\n440\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Input | actual | gpt2_tuned | xlnet | lstm_attention | gpt2_untuned_bert | gpt2_untuned \n",
    "input_index_lst = [k for k in input_dict3.values()]\n",
    "input_lst = [k for k in input_dict3.keys()]\n",
    "output_lst = [sentences.split('<entry>')[1] for i, sentences in enumerate(gpt2_tuned_file) if i in input_dict3.values()]\n",
    "gpt2_tuned_lst = [sentences.split('<entry>')[2] for i, sentences in enumerate(gpt2_tuned_file) if i in input_dict3.values()]\n",
    "gpt2_untuned_lst = [sentences.split('<entry>')[2] for i, sentences in enumerate(gpt2_untuned_file) if i in input_dict3.values()]\n",
    "xlnet_lst = [sentences.split('<entry>')[2] for i, sentences in enumerate(xlnet_file) if i in input_dict3.values()]\n",
    "lstm_attention_lst = [sentences.split('<entry>')[2] for i, sentences in enumerate(lstm_attention_file) if i in input_dict3.values()]\n",
    "\n",
    "gpt2_untuned_bert_lst = [sentences.split('<entry>')[3] for sentences in gpt2_untuned_bert_file if sentences.split('<entry>')[0][10:] in input_dict3.keys()]\n",
    "\n",
    "# sanity check\n",
    "print(len(input_index_lst))\n",
    "print(len(input_lst))\n",
    "print(len(output_lst))\n",
    "print(len(gpt2_tuned_lst))\n",
    "print(len(xlnet_lst))\n",
    "print(len(lstm_attention_lst))\n",
    "print(len(gpt2_untuned_bert_lst))\n",
    "print(len(gpt2_untuned_lst))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Input | actual | gpt2_tuned | xlnet | lstm_attention | gpt2_untuned_bert | gpt2_untuned \n",
    "all_data = zip(input_index_lst, input_lst, output_lst, gpt2_tuned_lst, xlnet_lst, lstm_attention_lst, gpt2_untuned_bert_lst, gpt2_untuned_lst)\n",
    "\n",
    "test = 0\n",
    "result = open(output1_path, 'w')\n",
    "# result.write(\"input_index|input|output|gpt2_tuned|xlnet|lstm_attention|gpt2_untuned_bert|gpt2_untuned\\n\")\n",
    "result.write(u\"input_index<entry>input<entry>output<entry>gpt2_tuned<entry>xlnet<entry>lstm_attention<entry>gpt2_untuned_bert<entry>gpt2_untuned\\n\")\n",
    "for data in all_data:\n",
    "    data = list(map(str, data))\n",
    "    result.write(u\"<entry>\".join(data) + '\\n')\n",
    "    test += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "manually_discarded_lst = [109, 144, 241, 282, 423, 427, 476, 595, 669, 755, 802, 819, 920, 944, 1022, 1056, 1080, 1116, 1136, 1198, 1329, 1343, 1443, 1497, 1625, 1668, 1760, 1798, 1904, 2036, 2103, 2185, 2257, 2383, 2473, 2522, 2625]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "result2 = open(output2_path, 'w')\n",
    "survey_index_cum_lst = []\n",
    "for i in range(1000):\n",
    "    selected_index = randint(0, len(input_index_lst)-1)\n",
    "    \n",
    "    if input_index_lst[selected_index] in manually_discarded_lst:\n",
    "        continue\n",
    "    elif input_index_lst[selected_index] in survey_index_cum_lst:\n",
    "        continue\n",
    "    else:\n",
    "        survey_index = 50 + selected_index\n",
    "        survey_index_cum_lst.append(survey_index)\n",
    "    \n",
    "    survey_question_p1 = '{\"SurveyID\":\"BL_eCE6nlVo6ilQDCl\",\"Element\":\"SQ\",\"PrimaryAttribute\":\"QID'\n",
    "    survey_question_p2 = str(survey_index)\n",
    "    survey_question_p3 =  '\",\"SecondaryAttribute\":\"'\n",
    "    input_plus_index = str(input_index_lst[selected_index])+'. '+input_lst[selected_index]\n",
    "    if len(input_plus_index)<=97:\n",
    "        input_plus_index97 = input_plus_index\n",
    "    else:\n",
    "        input_plus_index97 = input_plus_index[:97]+'...'\n",
    "    survey_question_p4 = input_plus_index97\n",
    "    survey_question_p5 = '\",\"TertiaryAttribute\":null,\"Payload\":{\"QuestionText\":\"<p>'\n",
    "    survey_question_p6 = input_plus_index\n",
    "    survey_question_p7 = '<\\/p>\",\"DefaultChoices\":false,\"DataExportTag\":\"Q'\n",
    "    survey_question_p8 = str(survey_index)\n",
    "    survey_question_p9 = '\",\"QuestionID\":\"QID'\n",
    "    survey_question_p10 = str(survey_index)\n",
    "    survey_question_p11 = '\",\"QuestionType\":\"Matrix\",\"Selector\":\"Likert\",\"SubSelector\":\"DL\",\"Configuration\":{\"QuestionDescriptionOption\":\"UseText\",\"TextPosition\":\"inline\",\"ChoiceColumnWidth\":25,\"MobileFirst\":false},\"QuestionDescription\":\"'\n",
    "    survey_question_p12 = input_plus_index97\n",
    "    survey_question_p13 = '\",\"Choices\":{\"1\":{\"Display\":\"'\n",
    "    survey_question_p14 = output_lst[selected_index]\n",
    "    survey_question_p15 = '\",\"ExclusiveAnswer\":false},\"2\":{\"Display\":\"'\n",
    "    survey_question_p16 = gpt2_tuned_lst[selected_index]\n",
    "    survey_question_p17 = '\",\"ExclusiveAnswer\":false},\"3\":{\"Display\":\"'\n",
    "    survey_question_p18 = xlnet_lst[selected_index]\n",
    "    survey_question_p19 = '\",\"ExclusiveAnswer\":false},\"12\":{\"Display\":\"'\n",
    "    survey_question_p20 = lstm_attention_lst[selected_index]\n",
    "    survey_question_p21 = '\",\"ExclusiveAnswer\":false},\"13\":{\"Display\":\"'\n",
    "    survey_question_p22 = gpt2_untuned_bert_lst[selected_index]\n",
    "    survey_question_p23 = '\",\"ExclusiveAnswer\":false},\"14\":{\"Display\":\"'\n",
    "    survey_question_p24 = gpt2_untuned_lst[selected_index]\n",
    "    survey_question_p25 = '\",\"ExclusiveAnswer\":false}},\"ChoiceOrder\":[\"1\",\"2\",\"3\",\"12\",\"13\",\"14\"],\"Validation\":{\"Settings\":{\"ForceResponse\":\"ON\",\"ForceResponseType\":\"ON\",\"Type\":\"None\"}},\"GradingData\":[],\"Language\":[],\"NextChoiceId\":15,\"NextAnswerId\":9,\"Answers\":{\"1\":{\"Display\":\"Terrible\"},\"2\":{\"Display\":\"Bad\"},\"6\":{\"Display\":\"OK\"},\"7\":{\"Display\":\"Good\"},\"8\":{\"Display\":\"Great\"}},\"AnswerOrder\":[\"1\",\"2\",\"6\",\"7\",\"8\"],\"ChoiceDataExportTags\":false,\"Randomization\":{\"Advanced\":null,\"Type\":\"All\",\"TotalRandSubset\":\"\"},\"DataVisibility\":{\"Private\":false,\"Hidden\":false},\"AnswerColumns\":5}},'\n",
    "\n",
    "    survey_question_x = \\\n",
    "        survey_question_p1\\\n",
    "        +survey_question_p2\\\n",
    "        +survey_question_p3\\\n",
    "        +survey_question_p4\\\n",
    "        +survey_question_p5\\\n",
    "        +survey_question_p6\\\n",
    "        +survey_question_p7\\\n",
    "        +survey_question_p8\\\n",
    "        +survey_question_p9\\\n",
    "        +survey_question_p10\\\n",
    "        +survey_question_p11\\\n",
    "        +survey_question_p12\\\n",
    "        +survey_question_p13\\\n",
    "        +survey_question_p14\\\n",
    "        +survey_question_p15\\\n",
    "        +survey_question_p16\\\n",
    "        +survey_question_p17\\\n",
    "        +survey_question_p18\\\n",
    "        +survey_question_p19\\\n",
    "        +survey_question_p20\\\n",
    "        +survey_question_p21\\\n",
    "        +survey_question_p22\\\n",
    "        +survey_question_p23\\\n",
    "        +survey_question_p24\\\n",
    "        +survey_question_p25\n",
    "    \n",
    "    result2.write(survey_question_x)\n",
    "    \n",
    "    if len(survey_index_cum_lst) == 30:\n",
    "        break\n",
    "\n",
    "result2.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "result3 = open(output3_path, 'w')\n",
    "\n",
    "for index in survey_index_cum_lst:\n",
    "    survey_question_p30 =' {\"Type\":\"Question\",\"QuestionID\":\"QID'\n",
    "    survey_question_p31 = str(index)\n",
    "    survey_question_p32 = '\"},'\n",
    "    survey_question_y = survey_question_p30+survey_question_p31+survey_question_p32\n",
    "    result3.write(survey_question_y)\n",
    "result3.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "487"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 54
    }
   ],
   "source": [
    "max(survey_index_cum_lst)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}